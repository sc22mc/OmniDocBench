are limited compared to the more substantial enhancements observed in simpler problem categories. After directly adding 5 correct CoT examples into the prompt,GPT-4 BoT $^ +$ CoTdemonstrates a significant performance boost, surpassing GPT-4 BoT by $7 . 7 \%$ and $1 1 . 5 \%$ in Precalculus and Intermediate Algebra domains,respectively.This basic conclusion from these observations is that to guarantee the top performance of BoT in complex mathematical problems,relying on trial-and-error analysis to learnhow toreason is notsuficient;instead,the correctanswers should also be provided in the prompt for LLMs.

While GPT3.5 withBoTmay initiallyfallbehind GPT-4 CoT,leveraging GPT-4as the evaluator and analyzer to generate experience allows GPT-3.5 BoT(GPT-4) to outperform GPT-4 Complex CoT.With the GPT3.5,which has less capacity than GPT4,as the LLM,the solving rate obtained byBoT isatleast $7 . 7 \%$ (on Algebra) lower than GPT4 ComplexCoT. It is evident that when less powerful LLMs produce lower-quality trial-and-error analyses,the BoTis unable to outperform GPT4 ComplexCoT.Thus,after using the GPT4 in the experience generation part while GPT3.5 is only used to generate reasoning steps, GPT3.5 BoT(GPT4) shows a significant improvement in all categories,leading to a solving rate of $5 5 . 8 \%$ ,which outperforms GPT4 ComplexCoTby $5 . 5 \%$ and iseven $1 . 9 \%$ higher than the current state-of-the-art GPT4 $\mathrm { P H P + C }$ omplexCoT.These observations further demonstrate that the accumulation of experience over iterations in the prompt constitutes the primary factor contributingto the success of the BoT framework.

# GREASONING RESULTS OF“GAME OF 24"

First, inTable $\boxed { 5 }$ -Table $\mathscr { G }$ we present the detailed prompts that BoT used during the reasoning process,thus providing a comprehensive understanding of what BoT does within each iteration. Then, startingfrom Table $\bigstar \bigstar$ we show some exact examples containing the whole reasoning process of BoT.Following the basic setings shown in the experiment section,these experiments are obtained using BoT with the GPT-3.5-turbo model.

Table 5: Reasoning steps generated by gpt-3.5-turbo when no experience is included in the input prompt. We firstletthe model generate one step of reasoning five times to check the diversityand then present the final reasoningchainafter finishing the first iteration of BoT.   

<table><tr><td>Prompt ofGameof24without experience</td></tr><tr><td>Ite can only use each number once,and parentheses can be used to change the order of operations.</td></tr><tr><td>Analysifoaeedsiugb</td></tr><tr><td>Thegivenfournumbersare:1146.</td></tr><tr><td>Let’sthinkstepbystep.</td></tr><tr><td>Recall historical reasoning experience (Ignore when experience is empty):</td></tr><tr><td>########################################## Payatention toanalysisandconclusion intheexperience toavoid making similarmistakes by folowing theadvice.</td></tr><tr><td>Belowisisl</td></tr><tr><td></td></tr><tr><td>Based on listed reasoning steps onlywithin the above”- ”(i.e.,Not the ones in the experience block),please make one step of reasoning to generate only one</td></tr><tr><td>subsequentialpossiblereasoningstep.</td></tr><tr><td>Fiveresponsesfromthegpt-3.5-turbo</td></tr><tr><td>StepCreeste</td></tr><tr><td>Step1,Cueectedbsitedri4</td></tr><tr><td>Step1,Cuectedbstedrb46</td></tr><tr><td>Step1,Cureectebsted46 Step1Cete</td></tr><tr><td>Obtained reasoningchain afteraggregation</td></tr></table>

# HREASONING RESULTS OF“GSM8K"

BoT uses similar basic prompts and the specific format as shown in Table5]-Table Only the task prompt will be changed,as shown in Table15 Then,starting from Table16we show some exact examples containing the whole reasoning process of BoT. Following the basic settings shown in the experiment section,these experiments are obtained using BoT with the GPT-3.5-turbo model.