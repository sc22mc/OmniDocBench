GUI recordings from end-users,we propose a more advanced approach to capture the spatial features of touch indicators and the temporal features of touch effects,to achieve better performance on user action identification.

To generate video captions,many works 78l, 84 started using one single unified deep-learning model (one-fit-all). Recent works infused knowledge about objects in the video by using object detectors to generate more informative captions.For example,Zhang et al. [85l adopted an object detector to augment the object feature to yield object-specific video captioning.Different from the natural scenes,generating action-centric descriptions for GUI recording requires a more complex GUI understanding，as there are many aspects to consider,such as the elements in the GUI, their relationships, thesemantics of icons,etc.Therefore,we modeled GUIspecific features by using mature methods,and then proposed a tailored algorithm to automatically generate natural language descriptions for GUI recordings.

# VII. CONCLUSION

The bug recording is trending in bug reports due to its easy creation and rich information. However, watching the bug recordings and understanding the user actions can be timeconsuming.In this paper,we present a lightweight approach CAPdroid to automatically generate semantic descriptions of user actions in the recordings,without requiring additional app instructions,recording tools,or restrictive video requirements. Our approach proposes image-processing and deep-learning models to segment bug recordings,infer user actions,and generate natural language descriptions. The automated evaluation and user study demonstrate the accuracy and usefulness of CAPdroid in boosting developers’productivity.

In the future,we will keep improving our method for better performance in terms of action segmentation and action attribute inference.According to user feedback,we will also improve the understanding of GUI to achieve higher-level semantic descriptions.

# REFERENCES

[1]S.Planning,“The economic impacts of inadequate infrastructure for softwaretesting,National Instituteof StandardsandTechnology,2002.   
[2]J.Anvik,L.Hiew,and G.C.Murphy，“Copingwith an open bug repository,inProceedings of the20o5 OOPSLAworkshopon Eclipse technology eXchange,2005,pp.35-39.   
[3]J.Aranda and G.Venolia,“The secret life of bugs:Going past the errors and omissions insoftware repositories,in 2oo9 IEEE31st International Conference on Software Engineering.IEEE,2009,pp.298-308.   
[4]S.Feng and C.Chen,“Gifdroid: Automated replay of visual bug reports forandroidapps,in2022IEEE/ACM44th International Conferenceon Software Engineering (ICSE). IEEE,2022,pp.1045-1057.   
[5] “Record the screen on your iphone,ipad,or ipod touch,"https://support. apple.com/en-us/HT207935 2022.   
[6] "Take a screenshot or record your screen on your android device,"https: //support.google.com/android/answer/9075928?hl=en2021.   
[7] S.Feng and C. Chen, “Gifdroid: Automated replay of visual bug reports forandroid apps,arXivpreprint arXiv:2112.04128,2021.   
[8] C.Bernal-Cardenas,N.Cooper,K.Moran,O.Chaparro,A.Marcus, andD.Poshyvanyk,“Translating video recordings of mobile app usages intoreplayable scenarios,”in ProceedingsoftheACM/IEEE 42nd International Conference on Software Engineering,2020,pp.309-321.   
[9]M.A.Gernsbacher,“Video captionsbenefit everyone,”Policy insights fromthebehavioral andbrainsciences,vol.2,no.1,pp.195-202,2015.   
[10]T.J. Garza,“Evaluating the use of captioned video materials in advanced foreignlanguage learningForeign LanguageAnnals,vol.24,no.3,pp. 239-258,1991.   
[11]J.Chen,C.Chen,Z. Xing,X.Xu,L. Zhut,G.Li,and J.Wang,“Unblind your apps: Predicting natural-language labels for mobile gui components bydeep learning,in 2020 IEEE/ACM42nd International Conference on Software Engineering (ICSE). IEEE,2020,pp. 322-334.   
[12]K.Moran,A.Yachnes,G.Purnell,J.Mahmud,M.Tufano,C.B. Cardenas,D.Poshyvanyk,and Z.H'Doubler,“Anempirical investigation into the use of image captioning for automated software documentation, in 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE,2022, pp.514-525.   
[13]C.Chen,T.Su,G.Meng,Z.Xing,andY.Liu,“Fromui design image to gui skeleton:a neural machine translator to bootstrap mobile gui implementation,”in Proceedings of the 4Oth International Conference on Software Engineering,2018,pp.665-676.   
[14]S.Feng,S.Ma,J.Yu,C.Chen,T.Zhou,andY.Zhen,“Auto-icon: An automated code generation tool for icon designs asssting in ui development,”in 26th International Conference on Intelligent User Interfaces,2021,pp.59-69.   
[15]S.Feng,M.Jiang,T.Zhou,Y.Zhen,and C.Chen,“Auto-icon+: An automated end-to-end code generation tool for icon designs in ui development”ACM Transactions on Interactive Intelligent Systems, vol.12,no.4,pp.1-26,2022.   
[16] J.D.Cintas and A.Remael,Audiovisual translation: subtitling.Routledge,2014.   
[17]S.Fengand C.Chen,“Gifdroid:an automated light-weight tool for replaying visual bugreports,in ProceedingsoftheACM/IEEE 44th International Conference on Software Engineering: Companion Proceedings, 2022, pp. 95-99.   
[18] “Github,”https:/github.com/2022.   
[19] D.Spencer, Card sorting: Designing usable categories.Rosenfeld Media,2009.   
[20] “Du recorder,https://www.du-recorder.com/2022.   
[21] H. Chen,M.Sun,and E.Steinbach,"Compression of bayer-pattern video sequences using adjusted chroma subsampling,”IEEE transactions on circuitsandsystems forvideo technologyvol.19,no.12,pp.1891- 1896,2009.   
[22]R. Sudhir and L.D.S. S.Baboo,“An efficient cbir technique with yuv color space and texture features, Computer Engineering and Intelligent Systems,vol.2,no.6,pp.78-85,2011.   
[23]M.LivingstoneandD.H.Hubel,Visionandart:Thebologyofseing. HarryN.AbramsNew York,2002,vol.2.   
[24] Z.Wang,A.C.Bovik,H.R. Sheikh,and E.P.Simonceli，“Image qualityassessment:from error visibility to structural similarity,”IEEE transactions onimageprocessing,vol.13,no.4,pp.600-612,2004.   
[25]Y.Du,C.Li,R.Guo,C.Cui,W.Liu,J.Zhou,B.Lu,Y.Yang,Q.Liu, X.Hu et al.,“Pp-ocrv2:Bag of tricks for ultra lightweight ocr system” arXivpreprint arXiv:2109.03144,2021.   
[26]A.Krizhevsky,I.Sutskever,and G.E.Hinton,“Imagenet clasification with deep convolutional neural networks,Advancesin neural information processing systems,vol.25,2012.   
[27]Y.LeCun,L.Bottou,YBengioandPHaffer,“Gradient-basedleaing applied to document recognition,Proceedingsof the IEEE,vol.86, no.11,pp.2278-2324,1998.   
[28] C.Feichtenhofer,“X3d:Expanding architecturesfor effcient video recognition,in Proceedings of the IEEE/CVF Conference on Computer Vision and PatternRecognition,2020,pp.203-213.   
[29]K.He,X.Zhang,S.Ren,and J.Sun,“Deep residual learning for image recognitionin Proceedings ofthe IEEE conference oncomputer vision and pattern recognition,2016,pp.770-778.   
[30]J.Schmidhuber,“Deep learning in neural networks:An overview,” Neural networks,vol.61,pp.85-117,2015.   
[31]S.Ren,K.He,R.Girshick,andJ.Sun,“Faster r-cnn:Towardsreal-time object detection with region proposal networks,Advances in neural information processing systems,vol.28,pp.91-99,2015.   
[32]D.P.Kingma andJ.Ba,“Adam:Amethod for stochastic optimization,” arXiv preprint arXiv:1412.6980,2014.   
[33]K.P.Murphy,Machine learning:a probabilistic perspective.MIT press,2012.   
[34]P.Irani,C.Gutwin,andX.D.Yang,“Improving selection of off-screen targetswith hopping,”in Proceedings of the SIGCHI conferenceon Human Factorsin computing svstems.2006.pp.299-308.