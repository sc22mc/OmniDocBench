the experiment ( $\delta$ and $\epsilon$ )and approximation ( $\delta ^ { \prime }$ and $\epsilon ^ { \prime }$ ）is evaluatedby

$$
\mathrm { e r r o r } : = \frac { 1 } { 4 } \sum _ { i = 1 } ^ { 4 } \sqrt { | \delta _ { i } - \delta _ { i } ^ { \prime } | ^ { 2 } + | \epsilon _ { i } - \epsilon _ { i } ^ { \prime } | ^ { 2 } } .
$$

# 5.2Chaos-Like and Heteroclinic Dynamics

Interestingly,learning dynamics in multi-memory games are complex. Fig.3 shows two learning dynamics between which there is a slight difference in their initial strategies ( $\pmb { x } = \pmb { y } = 0 . 8 \times \pmb { 1 }$ in the solid line,but in thebroken line ( $\mathbf { x } ^ { \prime }$ and $\boldsymbol { y ^ { \prime } }$ ） $x _ { 1 } ^ { \prime } = 0 . 8 0 1$ and others are the same as the solid line).We use Algorithm 2 with $\eta = 1 0 ^ { - 3 }$ and $\gamma = 1 0 ^ { - 6 }$ .These dynamics are similar in the beginning ( $0 \leq t \leq 3 2 0$ ).However,the difference between these dynamics is gradually amplified ( $3 2 0 \leq t \leq 3 6 0$ ), leading to the crucial difference eventually $3 6 0 \leq t \leq 4 2 0$ ).We here introduce the distance between $\mathbf { x } ^ { \prime }$ and $_ x$ as

$$
D ( \pmb { x } ^ { \prime } , \pmb { x } ) : = \frac { 1 } { 4 } \sum _ { i = 1 } ^ { 4 } | L ( \pmb { x } _ { i } ^ { \prime } ) - L ( \pmb { x } _ { i } ) | ,
$$

with $L ( x ) : = \log x - \log ( 1 - x )$ $L ( x )$ is the measure taking into account the weight in replicator dynamics. Furthermore,inorder toanalyze how thediference isamplified,Fig.3alsoshows the maximumeigenvalue in learning dynamics.We can see that the larger the maximum eigenvalue is,the more the difference between the two trajectories is amplifed. We observe that such an amplification typically occurs when strategies are close to the boundary of the simplex.In conclusion,the learning dynamics provide chaos-like sensitivity to the initial condition.

# 5.3Divergence in General Memories and Actions

Although we have focused on the one-memory two-action zero-sum games so far,numerical simulations demonstrate that similar phenomena are seen in games of other numbers of memories and actions. Fig. 4 shows the trajectories of learning dynamics in various multi-memory and multi-action games,where we use Algorithm 2with $\eta = 1 0 ^ { - 2 }$ and $\gamma = 1 0 ^ { - 6 }$ .Note that we consider zero-sum games in all the panels (see Fig. 4-A for the payoff matrices). In Fig. 4-B,each panel shows that strategy variables $x ^ { a | s }$ roughly diverge from the Nash equilibrium and sojourn longer at the edges of the simplex, ie., $x ^ { a | s } = 0$ or $1$ .Furthermore, Kullback-Leibler divergence from the Nash equilibrium averaged over the whole states,i.e.,

$$
D _ { \mathrm { K L } } ( \mathbf { x } ^ { * } \| \mathbf { x } ) : = \frac { 1 } { | \mathcal { S } | } \sum _ { s \in \mathcal { S } } \sum _ { a \in \mathcal { A } } x ^ { * a | s } \log \frac { x ^ { * a | s } } { x ^ { a | s } } .
$$

also increases with time in each panel of the figure.Thus,we confirm that learning reaches heteroclinic cyclesunder various (action,memory）pairs.

# 6Conclusion

This study contributes to an understanding of a cutting-edge model of learning in games in Sections 3 and 4.In practice,several famous algorithms,i.e.,replicator dynamics and gradient ascent,were newly extended to multi-memory games (Algorithms1 and 2).Then，we proved the correspondence between these algorithms (Theorems1-3） in general and the uniqueness of Nash equilibrium in two-action zerosum games (Theorem 4). As a background, multi-agent learning dynamics are generally complicated; thus, many theoretical approaches usually have been taken to grasp such complicated dynamics. In light of this background,our theorems succeeded in capturing the learning dynamics in multi-memory games,which are even more complicated than usual memory-less ones.

This study also experimentally discovered a novel and non-trivial phenomenon that simple learning algorithms such as replicator dynamics and gradient ascent asymptotically reaches a heteroclinic cycle in multi-memory zero-sum games.In other words,the players choose actions in highly skewed proportions throughout learning.Such a phenomenon is specific to multi-memory games:Perhaps this is because the