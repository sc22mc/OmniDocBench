![](images/58a2d1e74f161338601462de928fcad6935f0f45f107dbb907df61a3344cf951.jpg)  
Figure2.SchematicilustratioofteTimewapconditioalfowcitectureescribedinction4LeftAsingeonditioalealP coupling layer.Middle:A single atom transformer module.Right: the multihead kernel self-atention module.

not use a positional encoding. Second, instead of dot productattention, we use a simple kernel self-attention module, which we describe next.

Kernel self-attention We motivate the kernel selfattentionmodulewith theobservation thatphysical forces actingon theatomsin a molecule are local:i.e.,theyact morestronglyonnearbyatoms.Intuitively,forvaluesof $\tau$ that are not too large,the positions at time $t + \tau$ will be more influenced by atoms that are nearby at time $t$ ,compared to atoms that are far away.Thus,we define the attention weight $w _ { i j }$ for atom $i$ attending to atom $j$ as follows:

$$
w _ { i j } = \frac { \exp ( - \| x _ { i } ^ { p } - x _ { j } ^ { p } \| _ { 2 } ^ { 2 } / \ell ^ { 2 } ) } { \sum _ { j ^ { \prime } = 1 } ^ { N } \exp ( - \| x _ { i } ^ { p } - x _ { j ^ { \prime } } ^ { p } \| _ { 2 } ^ { 2 } / \ell ^ { 2 } ) } ,
$$

where $\ell$ is a lengthscale hyperparameter.The output vectors $\{ r _ { \mathrm { o u t } , i } \} _ { i = 1 } ^ { N }$ ï¼Œgiven the input vectors $\{ r _ { \mathrm { i n } , i } \} _ { i = 1 } ^ { N }$ are then:

$$
\begin{array} { r } { \textstyle r _ { \mathrm { o u t } , i } = \sum _ { j = 1 } ^ { N } w _ { i j } V \cdot r _ { \mathrm { i n } , j } , } \end{array}
$$

where $V \in \mathbb { R } ^ { d _ { \mathrm { o u t } } \times d _ { \mathrm { i n } } }$ is a learnable matrix. This kernel self-attention is an instance of the RBFkernel attention investigatedin Tsai etal. (2019). Similarly to Vaswani etal.(2017),we introduceamultihead version ofkernel self-attention, where each head has a different lengthscale. This is illustrated in Figure 2,Right. We found that kernel self-attention was significantly faster to compute than dot product attention,and produced similar or improved performance.

# 4.1.Symmetries

TheMD dynamics respects certain physical symmetries that would be advantageous to incorporate. We now describe how each of these symmetries is incorporated in Timewarp.

Permutation equivariance Let $\sigma$ be a permutation of the $N$ atoms.Since the atoms have no intrinsic ordering,the only effect of a permutation of $x ( t )$ on the future state $x ( t +$ $\tau$ )is to permute the atoms similarly,i.e.,

$$
\begin{array} { r } { \mu ( \sigma x ( t + \tau ) | \sigma x ( t ) ) = \mu ( x ( t + \tau ) | x ( t ) ) . } \end{array}
$$

Our conditional flow satisfies permutation equivariance exactly. To show this,we use the following proposition proved in Appendix A.1,which isan extension ofKohler et al. (2020); Rezende et al. (2019) for conditional flows:

Proposition 4.1. Let o be a symmetry action, and let $f ( \cdot ; \cdot )$ be an equivariant map such that $f ( \sigma z ; \sigma x ) = \sigma f ( z ; x ) f o r$ all $\sigma , z , x$ Further, let thebasedistribution $p ( z )$ satisfy $p ( \sigma z ) = p ( z )$ for all $\sigma , z .$ Then the conditional flow defined by $\sim p ( z ) , x ( t + \tau ) : = f ( z ; x ( t ) )$ satisfies $p ( \sigma x ( t +$ $\tau ) | \sigma x ( t ) ) = p ( x ( t + \tau ) | x ( t ) )$

Our flow satisfies $f _ { \theta } ( \sigma z ; \sigma x ( t ) ) = \sigma f _ { \theta } ( z ; x ( t ) )$ ,since the transformer is permutation equivariant,and permuting $z$ and $x ( t )$ together permutes the inputs. Furthermore,the base distribution $p ( z ) = \mathcal { N } ( 0 , I )$ is permutation invariant. Note that the auxiliary variables allow us to easily construct a