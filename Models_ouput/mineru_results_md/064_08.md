![](images/91fd5a6c5195157ef23602a3ca501590f3d9dcc0779cb30978cde50cba0646f0.jpg)  
Figure 3.We show the visualizations of 5 procedures of QR-CLIP.For each proces,the reader can refer to Fig.2

# 4.5.Limitation and Future Work

Weare still in the early stages of investigating how to best use CLIP and the QR principle to explore open-world knowledge to support location and time reasoning. And the modules and techniques developed are simple but effective. In the future:1） we will investigate more efficient and elegant implementations;2） while addressing the limited computational resources,collect a larger OWK dataset as input candidates;3) using multimodal OWKs to see if images fromInstagram,Twitter,etc.could help with this task.

# 5. Conclusion

We designed a novel QR-CLIP model. It consists of two modules:1） the Quantity module and 2） the Relevance module. Experiments show that it outperforms all previous SOTA on location and time reasoning by a wide margin. To show how our designed components affect the model, we conduct comprehensive ablation studies and verify that open-world knowledge is beneficial for solving our problem. We hope this paper will serve as a technical foundation for this study area and inspire more fascinating research.