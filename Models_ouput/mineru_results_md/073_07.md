![](images/9d93d6de6c29c6b535feb8a2865ece5bed262587fa319627b1572676274af46d.jpg)  
Figure3:Initial statesensitivity in learning dynamics inmulti-memory games.Inthe toppanels,colored lines are time series of $x _ { i }$ (X's strategy). The black line is the distance between the solid (sample of $_ { \textrm { \tiny a v } }$ ）and broken ( $\mathbf { x } ^ { \prime }$ ） lines. In the bottom panels,the black lines indicate the maximum eigenvalue in the learning dynamics of the solid line.

![](images/eeef056eeddba804c283de2793b3f6c72212a67b635339b4634f9bfd3ebf30e8.jpg)  
Figure 4:A.Payoff matrices of three-action (rock-paper-scissors)and four-action (extended rock-paper-scissors) games.B.Ineach panel, colored lines indicate time series of $x ^ { a | s }$ forrandom $a \in { \mathcal { A } }$ and $s \in S$ .The black broken line indicates the Kullback-Leibler divergence averaged over all the states $s \in S$ ,intuitively meaning a distance from the Nash equilibrium.

that in the zero-memory version of the game.This theorem means that in zero-sum games,the region of Nash equilibrium does not expand even if players have memories.Taking into account that having multiple memories expands the region of Nash equilibrium,such as a cooperative equilibrium in prisoner's dilemma games [20], this theoremisnontrivial.

In order to discuss whether our algorithms converge to this unique Nash equilibrium under Assumption 1, we consider the neighbor of Nash equilibrium and define sufficient smalldeviation from the Nash equilibrium, i.e., $\delta : = x - x ^ { \ast } \mathbf { 1 }$ and $\epsilon : = { \pmb y } - { \pmb y } ^ { * } { \pmb 1 }$ .Here,we assume that these deviations have the same scale $O ( \delta ) : =$ $O ( \delta _ { i } ) = O ( \epsilon _ { i } )$ forall $i$ .Then，defining that the superscript ( $k$ ）shows $O ( \delta ^ { k } )$ terms,the dynamics are