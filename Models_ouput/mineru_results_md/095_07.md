Table 6. Component-levellayout detection evaluationon OmniDocBench layout subset: mAPresults byPDF page type.   

<table><tr><td rowspan="2">Model</td><td rowspan="2">Backbone</td><td rowspan="2">Params</td><td rowspan="2">Book</td><td rowspan="2">Slides</td><td rowspan="2">Research</td><td rowspan="2">Textbook</td><td rowspan="2">Paer</td><td rowspan="2">Magazine</td><td rowspan="2">Academe</td><td rowspan="2">Notes</td><td rowspan="2">Newspaper</td><td rowspan="2">Average</td></tr><tr><td></td></tr><tr><td>DiT-L [24]</td><td>ViT-L</td><td>361.6M</td><td>43.44</td><td>13.72</td><td>45.85</td><td>15.45</td><td>3.40</td><td>29.23</td><td>66.13</td><td>0.21</td><td>23.65</td><td>26.90</td></tr><tr><td>LayoutLMv3[17]</td><td>RoBERTa-B</td><td>138.4M</td><td>42.12</td><td>13.63</td><td>43.22</td><td>21.00</td><td>5.48</td><td>31.81</td><td>64.66</td><td>0.80</td><td>30.84</td><td>28.84</td></tr><tr><td>DocLayout-YOLO [53]</td><td>v10m</td><td>19.6M</td><td>43.71</td><td>48.71</td><td>72.83</td><td>42.67</td><td>35.40</td><td>51.44</td><td>64.64</td><td>9.54</td><td>57.54</td><td>47.38</td></tr><tr><td>SwinDocSegmenter [4]</td><td>Swin-L</td><td>223M</td><td>42.91</td><td>28.20</td><td>47.29</td><td>32.44</td><td>20.81</td><td>52.35</td><td>48.54</td><td>12.38</td><td>38.06</td><td>35.89</td></tr><tr><td>GraphKD [5]</td><td>R101</td><td>44.5M</td><td>39.03</td><td>16.18</td><td>39.92</td><td>22.82</td><td>14.31</td><td>37.61</td><td>44.43</td><td>5.71</td><td>23.86</td><td>27.10</td></tr><tr><td>DOCX-Chain [50]</td><td>-</td><td>-</td><td>30.86</td><td>11.71</td><td>39.62</td><td>19.23</td><td>10.67</td><td>23.00</td><td>41.60</td><td>1.80</td><td>16.96</td><td>21.27</td></tr></table>

<table><tr><td rowspan="2">Model Type</td><td rowspan="2">Model</td><td colspan="3"></td><td colspan="4"></td><td colspan="4"></td><td rowspan="2">Overall</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>OCR-based</td><td>PaddleOCR[23]</td><td>76.871.8</td><td></td><td>80.1</td><td>67.9</td><td>74.3</td><td>81.1</td><td>74.5</td><td>70.6/75.2</td><td>71.3/74.1</td><td>72.7/74.0</td><td>23.3/74.6</td><td>73.6</td></tr><tr><td>Models</td><td>RapidTable[37]</td><td></td><td>80.083.2</td><td>91.2</td><td>83.0</td><td>79.7</td><td>83.4</td><td>78.4</td><td>77.1/85.4</td><td>76.7/83.9</td><td>77.6/84.9</td><td>25.2/83.7</td><td>82.5</td></tr><tr><td rowspan="2">Expert VLMs</td><td>SOT-CR[455</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td>72875</td><td>84</td><td>72</td><td>77</td><td>769</td><td>88.7</td><td>6.58.</td><td>69.276</td><td>72.8764</td><td>3.5762</td><td>78</td></tr><tr><td rowspan="2">General VLMs</td><td>wm2YL-7B 441</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>68.876.5</td><td></td><td>71.470.8</td><td></td><td></td></tr><tr><td></td><td></td><td>702 707</td><td>24</td><td>702</td><td>2</td><td>745</td><td></td><td></td><td>63.4726</td><td></td><td>20.0121</td><td>710</td></tr></table>

Table 7. Component-level Table Recognition evaluation on OmniDocBench table subset. $( + / - )$ means with/without special situation

ponents, with tables,images,and ignored components excluded from the final reading order calculation.

# 5.Benchmarks

Based on the distinct characteristics of these algorithms, we categorize document content extraction methods into three main classes:

·Pipeline Tools: These methods integrate layout detection and various content recognition tasks (such as OCR, table recognition,and formula recognition) into a document parsingpipeline forcontent extraction.Prominent examples includeMinerU[42] (v0.9.3),Marker [34] (v1.2.3), andMathpix4.   
·Expert VLMs: These are large multimodal modelsspecifically trained for document parsing tasks. Representative models include GOT-OCR2.0 [45] and Nougat [7].   
·General VLMs: These are general-purpose large multimodal models inherently capable of document parsing. Leading models in this category include GPT-4o [2], Qwen2-VL-72B[44],and InternVL2-76B [8].

# 5.1.End-to-End Evaluation Results

Overall Evaluation Results.As illustrated in Table 2, pipeline tools such as MinerU and Mathpix,demonstrate superior performance across sub-tasks like text recognition, formula recognition,and table recognition. Moreover, the general VisionLanguageModels(VLMs),Qwen2-VL,and GPT4o,also exhibit competitive performance. Almost all algorithms score higher on English than on Chinese pages. Performance Across Diverse Page Types. To gain deeper insights into model performance on diverse document types, we evaluated text recognition tasks across different page types.Intriguingly,as shown in Table 3,pipeline tools perform well for commonly used data, such as academic papers and financial reports.Meanwhile,for more specialized data,such as slides and handwritten notes,general VLMs demonstrate stronger generalization. Notably, most VLMs fail to recognize when dealing with the Newspapers,while pipeline tools achieve significantly better performance.

Performance on Pages with Visual Degradations.In Table 4，we further analyze performance on pages containing common document-specific challenges,including fuzzy scans,watermarks,and colorful backgrounds.VLMs like InternVL2 and Qwen2-VL exhibit higher robustness in these scenarios despite visual noise. Among pipeline tools, MinerU remains competitive due to its strong layout segmentation and preprocessing capabilities.

Performance on DifferentLayout Types. Page layout is a critical factor in document understanding,especially for tasks involving reading order. OmniDocBench annotates layout attributes such as single-column,multi-column,and complex custom formats.Across all models,we observe a clear drop in accuracy on multi-column and complex layouts.MinerU shows the most consistent reading order prediction,though its performance dips on handwritten singlecolumn pages due to recognition noise.

Discussion on End-to-End Results.1） While general VLMs often lag behind specialized pipelines and expert models on standard documents (e.g., academic papers), they generalize better to unconventional formats (e.g.，notes) and perform more robustly under degraded conditions (e.g., fuzzy scans). Thisis largely due to their broader training data,enabling better handling of long-tail scenarios compared to models trained onnarrow domains.2) VLMs,how