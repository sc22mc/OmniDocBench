![](images/91f1b9928abfea20aafd93326103491e93ac28dd648ddf0a699919e4ee31f763.jpg)  
Fig.2．Prediction images obtained by applying SoftPLS-DA and $\mathrm { \bf S }$ -SoftPLSDA models alongside the corresponding RGB images as references.

The application of Soft PLS-DA yielded promising results in both cross-validation and prediction of the external test set.It achieved sensitivity and specificity for the BMSBclass exceeding $9 0 . 0 \%$ [36] (see Table I). Furthermore,employing sparse variable selection through the s-Soft PLS-DA algorithm maintained high classification performance while considering a reduced subset of 6O relevant spectral variables.The selected wavelengths can be grouped into five main spectral regions, corresponding to absorption bands associated with cellulose, hemicellulose，and lignin (1220-1295 and 1420-1480 nm). These are indicative of different background types.In addition,we selected absorption bands related to water,proteins, chitin,and lipids (980-1070,1330-1350,and $1 3 7 0 \mathrm { - } 1 4 0 0 \ \mathrm { n m }$ ）， reflecting the biochemical structure of insects’exoskeleton [36]. Subsequently,we applied both SoftPLS-DA and s-Soft PLS-DA models to the test set images to generate prediction images, facilitating the evaluation of classification performance across the entire images.This assessment allowed an overallunderstanding of the models’effectiveness in accurately classifying BMSB specimens and vegetal backgrounds

Fig.2 presents some representative prediction images alongside their corresponding RGB counterparts.It is evident that, in general, the pixelsare accuratelyclassified into theirrespective classes.This observation highlights the challenge of detecting BMSB specimens against dark brown vegetal backgrounds using only RGB images,whereas the bugs are distinctly identified using SWIR-HSI. The prediction images of tree branches and soilas background reveal that spectral variable selection slightly enhances the classification of background pixels.To enhance BMSB detection capabilities for field applications,we integrated spectral information modeling with CNN algorithms to leverage spatial relationships amongpixels.U-Net,in particular,is well suited forimplementation onUAVs due to itsability to achieve satisfactory model performance with minimal computational resources [39].

To streamline network complexity in the spectral dimension, weeffectively applied U-Net using only the spectral bands identified by s-Soft PLS-DA [36]. Notably, the spectral regions selected by s-Soft PLS-DA can serve as a basis for developing moreaffordable and rapid multispectral sensors,ideal forfield monitoring using UAVs.When coupled with RGB cameras, these sensors can enable the deployment of efficient monitoring systems capable of autonomously detecting BMSB and providing real-time information on pest spread to farmers.

![](images/f3a70bd3600df5878093e70568fb017a3a5ae9c0cf1c6dc517fe78316d86a1ab.jpg)  
Fig.3.Utilization of deep learning object detection models for BMSB detection with the false-color image constructed using manually selected wavebands of theNIR range.

Furthermore,light and cost-efficient hyperspectral cameras covering the SWIR range are increasingly accessible in the market [4O]. This trend enhances the practicality of developing advanced monitoring solutions for agricultural applications, facilitating timely and accurate pest management decisions.

# B.Evaluation ofVis-NIR MSI

We conducted a pilot study employing a snapshot Vis-NIR UAVcamera in the field to detectBMSB.The camera consists of two distinct acquisition devices capturing in the ranges $4 5 7 -$ $5 9 3 \mathrm { n m }$ (16bands)and $6 0 5 { - } 8 4 5 \mathrm { n m }$ (15bands),respectively. For our experiments,we used the latter 15 bands of the images. In one approach (camera I,Fig.3),we performed manual wavelength selection based on the visibility of BMSB to construct the false-color image.We identified the wavelengths 727.24, 811.24,and 608.75 as suitable for BMSBdetection.

Then,we trained two object detection models,namely,Faster RCNN and RetinaNet,on these images [41],[42]. The pretrained RetinaNet yielded the best resultswith an F1 score of $7 8 . 4 5 \%$ ， precision of $7 4 . 6 1 \%$ ,recall of $7 6 . 3 9 \%$ ,andaccuracy of $7 8 . 4 5 \%$ (see Table I). In the other approach,we utilized the entire Vis-NIR hypercube from camera II (see Fig.3) to train the two object detection models.We customized the input layer of these models to accommodate the hyperspectral image with 15channels.RetinaNet trained from scratch exhibited the best performance,achieving an F1 score of $7 0 . 7 4 \%$ ，precision of $8 4 . 2 5 \%$ ,recall of $6 0 . 9 6 \%$ ,andaccuracy of $5 4 . 7 3 \%$

# V.EDGE-BASED SMART STICKY TRAP IMAGING SYSTEM

As part of the HALY.ID project,we developed IoT imaging systems with integrated pheromone enhanced sticky traps to monitor the population of BMSB and to combat the spread of this invasive species.Sticky traps are widely used in orchards to attract and capture insects based on their color or pheromones. Agronomists can use these traps to estimate the insect populations by visiting their orchards regularly with manual trap inspection and insect counting.A preliminary version of the trap using mainly lab-based experiments and artificial insects to