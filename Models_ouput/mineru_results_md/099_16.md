Table 8: Auto-J's generality on unseen scenarios.We train two new variants by removing a set of scenarios from the training data,and compare their performance with the complete version of Auto-Jthat has been trained onall data.We report the agreement rate with human label on the pairwise response comparison task (Eval-P),and the winrate against ChatGPT judged by GPT-4 on the critique generation task (Eval-C).   

<table><tr><td rowspan="2">Model</td><td colspan="2">Eval-P</td><td colspan="2">Eval-C</td></tr><tr><td>Seen</td><td>Unseen /</td><td>Seen</td><td>Unseen</td></tr><tr><td colspan="5">Unseen scenarios: Randomly select one scenario from each group.</td></tr><tr><td>Auto-J (Complete Version)</td><td>54.5</td><td>56.8</td><td>146/200</td><td>25/32</td></tr><tr><td>Auto-J (Training w/o unseen scenarios)</td><td>53.5</td><td>55.7</td><td>143/200</td><td>25/32</td></tr><tr><td colspan="5">Unseen scenarios: Scenarios of NLP Tasks group.</td></tr><tr><td>Auto-J (Complete Version)</td><td>54.2</td><td>57.6</td><td>136/188</td><td>35/44</td></tr><tr><td>Auto-J (Training w/o unseen scenarios)</td><td>54.2</td><td>54.9</td><td>130/188</td><td>38/44</td></tr></table>

Table 9: Scenario criteria as system message in prompt.   

<table><tr><td>You aregiven the criteria to craft good responses for this type of query fromusers:</td><td></td></tr><tr><td>-{scenariodescription} Thecriteria areasfollows:</td><td></td></tr><tr><td>[Criteriastart]</td><td></td></tr><tr><td>{criteriaforthescenario}</td><td></td></tr><tr><td>[Criteriaend]</td><td></td></tr></table>

# CAUTO-J'S GENERALITY ON UNSEEN SCENARIOS

It is quite important to see how Auto-J performs on the scenarios that are not included in its training data. To investigate this research problem,we retrain two variants of Auto-Jby holding out two sets ofunseen scenarios.

1. We randomly select one scenario from each scenario group as the unseen scenarios,and retrain Auto-J with the remaining scenarios. This in total leads to 8 unseen scenarios in testing.   
2.We take the complete“NLP Tasks”group as the unseen scenarios,and retrain Auto-J with the remaining scenarios.This in total leads to 11 unseen scenarios in testing.

Under both seting,we select the complete version of Auto-Jas the baseline to compare with. We report the agreement with human annotation labels on the pairwise response selection task (Eval-P) and the win rate against ChatGPT in the critique generation task (Eval-C) judged by GPT-4. The resultsare shownin Tab.8

Compared with the complete version of Auto-J,the two re-trained variants only show slightly degraded performance on the two evaluated tasks both on the seen and unseen scenarios,which indicates that Auto-J can generalize wellto scenarios unseen during training.

# DPROMPTS

Tab. $\boxed { 9 . 1 6 }$ shows different prompts. Tab.913guide GPT-4 to generate training data $( \ S 3 . 2 )$ Tab.9 and $\boldsymbol { \vert 1 0 \vert }$ provide GPT-4 system messages,where the scenario and the criteria are defined.Tab.1113 show GPT-4 user messages, providing the instance-related information. Tab.1415elaborate the prompts $( \ S \bigstar \bigstar . 2 )$ ,which all baseline models use to generate the testing results. Tab.[16is used for GPT-4 evaluation that conducts a pairwise comparison between our AUTo-J with one baseline.