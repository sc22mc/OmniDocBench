Matthew E.Peters,Mark Neumann,Mohit Iyyer,Matt Gardner, Christopher Clark,Kenton Lee, and Luke Zetlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,Volume1 (LongPapers),pp.2227-2237,NewOrleans,Lousiana June 2018.Association forComputationalLinguistics.doi:10.18653/v1/N18-1202. URLhttps //aclanthology.org/N18-1202   
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.In SC2o: International Conference for High Performance Computing,Networking,Storage and Analysis, pp.1-16. IEEE,2020.   
Jeff Rasley,Samyam Rajbhandari,Olatunji Ruwase,and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the26thACMSIGKDDInternational Conference on KnowledgeDiscovery&Data Mining,p. 3505-3506,2020.   
JieRen,Samyam Rajbhandari,Reza Yazdani Aminabadi, Olatunji Ruwase,Shuangyan Yang,Minjia Zhang,Dong Li,and Yuxiong He. Zero-offload: Democratizing billion-scale model training. In 2021 USENIX Annual Technical Conference (USENIX ATC21),pp.551-564,2021.   
William Saunders,Catherine Yeh, Jeff Wu,Steven Bills,Long Ouyang,Jonathan Ward,and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022.   
Nisan Stiennon,Long Ouyang,Jeffrey Wu,Daniel Ziegler,Ryan Lowe,Chelsea Voss，Alec Radford,Dario Amodei,and Paul F Christiano. Learning to summarize with human feedback．InH.Larochele,M.Ranzato，R.Hadsell，M.F.Balcan，and H.Lin(eds.)，Advances in Neural Information Processing Systems, volume 33, pp. 3008-3021. Curran Associates,Inc.,2020. URL https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1f89885d556929e98d3ef9b86448f951-Paper.pdf   
Rohan Taori, Ishaan Gulrajani,Tianyi Zhang,Yann Dubois,Xuechen Li, Carlos Guestrin,Percy Liang,and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca 2023.   
Hugo Touvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne Lachaux,Timothée Lacroix,Baptiste Roziere,Naman Goyal,Eric Hambro,Faisal Azhar, et al.Llama: Open and efficient foundation language models.arXiv preprint arXiv:2302.13971, 2023a.   
Hugo Touvron,Louis Martin,Kevin tone,Peter Albert,Amjad Almahairi,Yasmine Babaei,Nikolay Bashlykov,Soumya Batra,Prajjwal Bhargava,Shruti Bhosale,etal.Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,2023b.   
Peiyi Wang,LeiLi,Liang Chen,Dawei Zhu,BinghuaiLin,Yunbo Cao,QiLiu,Tianyu Liu,and Zhifang Sui. Large language models are not fair evaluators. arXiv preprint arXiv:23o5.17926, 2023a.   
Tianlu Wang,Ping Yu,Xiaoqing Ellen Tan,Sean O'Brien,Ramakanth Pasunuru,Jane Dwivedi-Yu, Olga Golovneva,Luke Zettlemoyer,Maryam Fazel-Zarandi,and Asli Celikyilmaz. Shepherd:A critic for language model generation. arXiv preprint arXiv:2308.04592,2023b.   
Yidong Wang, Zhuohao Yu,Zhengran Zeng,Linyi Yang,Cunxiang Wang, HaoChen,ChaoyaJiang, Rui Xie, Jindong Wang, Xing Xie, et al. Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization. arXiv preprint arXiv:2306.05087, 2023c.   
YizhongWang,Yeganeh Kordi,SwaroopMishra,AlisaLiu,NoahASmith,Daniel Khashabi,and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.   
Can Xu,Qingfeng Sun,Kai Zheng,Xiubo Geng,Pu Zhao,Jiazhan Feng,Chongyang Tao,and Daxin Jiang.Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244,2023.