we can generate $B$ chains in parallel,allstarting from the same initial state.This batched sampling procedure leads to even further speedups. For all exploration experiments we use a batch size of 1OO,and run $M = 1 0 0 0 0$ exploration steps. The maximum allowed energy change cutoff is set at $\Delta U _ { \mathrm { m a x } } = 3 0 0 \mathrm { k J / m o l }$

# D. Dataset details

We evaluateourmodelontreedierentdatasets,AD,2AA,and4AA,asintroducedinection6.Alldatasetsareulated inimplicit solventusing theopenMMlibrary(Eastmanetal.,2017).ForallMDsimulations weuse theparametersshown inTable2.

Table 2. OpenMM MD simulation parameters   

<table><tr><td>ForceField</td><td>amber-14</td></tr><tr><td>Time step</td><td>0.5fs</td></tr><tr><td>Friction coefficient</td><td>ps 0.3</td></tr><tr><td>Temperature</td><td>310K</td></tr><tr><td>Integrator</td><td>LangevinMiddleIntegrator</td></tr></table>

# E.Hyperparameters

Depending onthedataset,differentTimewarp modelsizes wereused,asshowninTable3.ForalldatasetstheMultihead kernel self-attention layer consists of 6 heads with lengthscales $\ell _ { i } = \{ 0 . 1 , 0 . 2 , 0 . 5 , 0 . 7 , 1 . 0 , 1 . 2 \}$ ,givenin nanometers.

Table 3.Timewarp model hyperparameters   

<table><tr><td>Dataset</td><td>RealNVP layers</td><td>Transformer layers</td><td>Parameters</td><td></td><td>Atom-embedding dim HTransformer feature dimension D</td></tr><tr><td>AD</td><td>12</td><td>6</td><td>1×108</td><td>64</td><td>128</td></tr><tr><td>2AA</td><td>12</td><td>6</td><td>1×108</td><td>64</td><td>128</td></tr><tr><td>4AA</td><td>16</td><td>16</td><td>4×108</td><td>128</td><td>128</td></tr></table>

The $\phi _ { \mathrm { i n } }$ and $\phi _ { \mathrm { o u t } }$ MLPs use SiLUs as activation functions, while the Transformer MLPs use ReLUs. Note the transformer MLPrefers tothe atom-wise MLPshownin Figure2,Middleinside the transformer block.Theshapes ofthese MLPs vary for the different datasets as shown in Table4.

Table4. Timewarp MLP layer sizes   

<table><tr><td>Dataset</td><td>inMLP</td><td>outMLP</td><td>Transformer MLP</td></tr><tr><td>AD</td><td>[70,256,128]</td><td>[128,256,3]</td><td>[128,256,128]</td></tr><tr><td>2AA</td><td>[70,256,128]</td><td>[128,256,3]</td><td>[128,256,128]</td></tr><tr><td>4AA</td><td>[134,2048,128]</td><td>[128,2048,3]</td><td>[128,2048,128]</td></tr></table>

Che first linear layers in the kernel self-attention module always has the shape [128,768] (in Section 4 denoted as $V$ ,and he second (after concatenating the output of head head) has the shape [768,128].

Afterlikelioodtraining,wefine-tune the modelfortheADand2AAdataset withacombinationofallthreelosss discussed in Section5.Wedidnotperformfine tuning forthe modeltrainedonthe4AA dataset.Weuseaweightedsumofthe losses with weights detailedinTable5.Weuse theFusedLamboptimizerandthe DeepSpeedlibrary(Rasleyetal.,2020)forall experiments.Thebatchsizeaswellaste training timesarereported inTable6.Allsimulationsare startedwithalearning rateof $5 \times 1 0 ^ { - 4 }$ ,the learning rate is then consecutively decreased by a factor of 2 upon hiting training loss plateaus.