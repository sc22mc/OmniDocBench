# ACKNOWLEDGEMENTS

We thank Chunpu Xu and Yuqing Yang for supporting the human annotation process,and Yuan Guo for his support in the post-submission maintenance and development of Auto-J. We also thank the anonymous reviewers for their valuable feedback and helpful suggestions. This project is supported by Qingyuan Research Project and Shanghai Artificial Intelligence Laboratory.

REFERENCES   
Yuntao Bai, Andy Jones, Kamal Ndousse,Amanda Askell,Anna Chen,Nova DasSarma, Dawn Drain, Stanislav Fort,Deep Ganguli, Tom Henighan,etal.Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022a.   
Yuntao Bai,Saurav Kadavath,Sandipan Kundu,Amanda Askell,JacksonKernion,Andy Jones,Anna Chen,Anna Goldie,Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073,2022b.   
Manik Bhandari, PranavNarayan Gour,Atabak Ashfaq,Pengfei Liu,and Graham Neubig.Reevaluating evaluation in text summarization. In Proceedings of the 2O2O Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 9347-9359, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.751. URLhttps: //aclanthology.org/2020.emnlp-main.751   
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askel, et al. Language models are few-shot learners. Advances in neural information processing systems,33:1877-19o1,2020.   
Tianqi Chen,BingXu, Chiyuan Zhang,and Carlos Guestrin.Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06l74,2016.   
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,Lianmin Zheng, Siyuan Zhuang,Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with $90 \% *$ chatgpt quality, March 2023. URLhttps: //lmsys.0rg/blog/2023-03-30-vicuna/   
Tri Dao.Flashattention-2:Faster attention with beter parallelism and work partitioning.arXiv preprint arXiv:2307.08691,2023.   
Tri Dao,Dan Fu, Stefano Ermon,Atri Rudra,and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359,2022.   
Jacob Devlin,Ming-Wei Chang,Kenton Lee,and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2O19 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,Volume1(ongndhortapers),pp.4i7-86,inneapolis,innesotaJue 2019.Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https: //aclanthology.org/N19-1423   
Yann Dubois,XuechenLi,Rohan Taori,Tianyi Zhang,IshaanGulrajani,JimmyBa,CarlosGuestrin, PercyLiang,and Tatsunori B Hashimoto.Alpacafarm:A simulation framework for methods that learn from human feedback. arXiv preprint arXiv:2305.14387,2023.   
Kawin Ethayarajh, Yejin Choi, and Swabha Swayamdipta. Understanding dataset diffculty with V-usable information. In International Conference on Machine Learning,pp.5988-6008.PMLR, 2022.   
Jinlan Fu,See-Kiong Ng,Zhengbao Jiang,and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv preprint arXiv:2302.04166,2023.   
Leo Gao, John Schulman,and Jacob Hilton. Scaling laws for reward model overoptimization. In International Conference on Machine Learning, pp.10835-10866.PMLR,2023.