Table 5. Distribution of the number of unique axes $( \% )$   

<table><tr><td rowspan="2">Dataset1</td><td colspan="8">Number of unique axes</td></tr><tr><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td></tr><tr><td>Train</td><td>0.0</td><td>1.3</td><td>13.5</td><td>25.7</td><td>24.8</td><td>18.8</td><td>10.9</td><td>5.1</td></tr><tr><td>Test</td><td>0.0</td><td>1.4</td><td>12.8</td><td>25.7</td><td>25.6</td><td>19.6</td><td>10.2</td><td>4.6</td></tr></table>

1 SL-MH,SL-PB,SP36O,and HoliCity all have the same distribution of thenumber ofunique axes shown in this table

Ignoring back labels.After removing label ambiguity, we ignored back labels because the training and test sets had only $0 . 1 \%$ and $0 . 3 \%$ back labels,respectively. Therefore, theVP estimator detected 13 points,that is,the five VPs (front,left,right,top,and bottom) and eight ADPs in Table 2. If all VP/ADPs are successfully detected, our method can estimate a unique rotation for over $98 \%$ imageswith two ormore unique axes from the VP/ADPsin Table 5.

# 4.3.Parameter settings

Following [53]，we fixed the image sensor height to 24 mm, $d _ { u } = d _ { v }$ ,used the principal points $\left( { { c } _ { u } } , { { c } _ { v } } \right)$ as the image center,and the translation vectors t as the zero vectors．Therefore,in our method,we focused on the estimation of five camera parameters, that is,focal length $f$ distortion coefficient $k _ { 1 }$ ，pan angle $\phi$ ,tilt angle $\theta$ ,and roll angle $\psi$ inaManhattan world.We independently trained the VP estimator and distortion estimator using a minibatch size of 32. We optimized the VP estimator,which was pretrained on ImageNet [42],using the Adam optimizer[17] and Random Erasing augmentation [62] with $( p , s _ { l } , s _ { h } , r _ { 1 } , r _ { 2 } ) \ = \ ( 0 . 5 , 0 . 0 2 , 0 . 3 3 , 0 . 3 , 3 . 3 )$ .The initial learning rate was set to $1 \times 1 0 ^ { - 4 }$ and was multiplied by 0.1 at the1Ooth epoch.We also trained the distortion estimator pretrained on Wakai et al.'s original network [53] using the RAdam optimizer [31] with a learning rate of $1 \times 1 0 ^ { - 5 }$

# 4.4. Experimental results

We implemented the comparison methods according to the corresponding papers but trained them on SL-MH, SL-PB, SP360,and HoliCity.

# 4.4.1Vanishing point estimation

To demonstrate the validity and effectiveness of the proposed VP estimator, we used pose estimation metrics and the distance error between the detected and ground-truth VP/ADPs.Following [7],we used the standard keypoint metrics of the average precision (AP) and average recall (AR)[47] with a 5.6-pixel $s k _ { i }$ object keypoint similarity [36] as well as the percentage of correct keypoints (PCK)[2] with a 5.6-pixel distance threshold,which corresponds to $1 / 1 0 \mathrm { t h }$ of the heatmap height.

Overall,the VP estimator detected the VP/ADPs,although the performance in the cross-domain evaluation decreased,asTable6 reveals.TheAP,AR,andPCK results suggest that VP/ADP estimation is an easier task than humanpose estimation becausethe VP/ADPs havethe implicit constraints of the geometric coordinates shown in Figure 3.Table 6 also reveals that ADP detection is more difficult than VP detection because VPs generally have specifc appearances at infinity. In terms of distance errors,the VP estimator addressed the various directions of VP/ADPs. In addition, considering the qualitative results in Figure 5, the VP estimator stably detected VP/ADPs in the entire image. Therefore, the VP estimator can precisely detect VP/ADPs from a fisheye image.

![](images/f283a9dace577bd8f68ca34e882a3f9f5fa978b6e89c1e6621b17670f0811ebb.jpg)  
Figure 5. Qualitative results of VP/ADP detection using the proposed VP estimator on the SL-MH test set.The VP estimator estimated fiveVPand eightADPheatmaps for eachVP/ADP.

# 4.4.2Parameter and reprojection errors

To validate the accuracy of the camera parameters,we compared our method with conventional methods that estimate both rotation and distortion.Following[53],we evaluated the mean absolute error and reprojection error (REPE). Our method achieved the lowest mean absolute angle error and REPE of the methods listed in Table 7. The pan-angle errors in our method using HRNet-W32 for the VP estimator are substantially smaller than those ofLochman et al.'s method[32] by $2 0 . 1 6 ^ { \circ }$ on the SL-MH test set. Our method achieved $3 . 1 5 ^ { \circ }$ and $3 . 0 0 ^ { \circ }$ errors for the tilt and roll angles, respectively,outperformingtheothermethods.We evaluated our method using HRNet-W48 (a larger backbone) and obtained a slight RMSE improvement of O.16 pixelswith respect to the RMSE obtained using HRNet-W32.

The Pritts et al.'s [41] andLochman et al.'s[32] methods could not perform calibration for some images because of a lack of arcs.In particular,Lochman et al.'s method [32] had a $5 9 . 1 \%$ executable rate, that is, the number of successful executions divided by the number of all images.Note that we calculated errors using only these successful executions. By contrast,our learning-based method can ad