probability distribution over the generic vocabulary $V _ { g }$ at the decoding time step $t$ is computed as,

$$
P _ { v _ { g } } = \operatorname { s o f t m a x } \big ( \operatorname { F C } _ { 1 } ( \mathbf { o } _ { t } ) \big ) ,
$$

where $\mathrm { F C _ { 1 } }$ is a fully connected layer.

To calculate the probability distribution over the history-aware vocabulary $V _ { h }$ ,we adopt a maxpooling layer over the context-updated history memory $\mathbf { C } _ { s }$ ,a fully connected layer,and a softmax function as follows,

$$
P _ { v _ { h } } = \mathrm { s o f t m a x } \big ( \mathrm { F C } _ { 2 } ( \mathrm { m a x - p o o l i n g } ( \mathbf { C } _ { s } ) ) \big ) ,
$$

where $\mathrm { F C _ { 2 } }$ is a fully connected layer.

The final word probability distribution at time step $t$ is computed by using a switching mechanism between $P _ { v _ { g } }$ and $P _ { v _ { h } }$ as follows,

$$
P = \alpha _ { v _ { g } } * P _ { v _ { g } } + \alpha _ { v _ { h } } * P _ { v _ { h } } ,
$$

where $\alpha _ { v _ { g } }$ and $\alpha _ { v _ { h } }$ is the switching probability of generating from generic vocabulary or copying from history conversations. $\alpha _ { v _ { g } }$ and $\alpha _ { v _ { h } }$ is calculated as follows,

$$
\begin{array} { r } { \left[ \alpha _ { v _ { g } } , \alpha _ { v _ { h } } \right] = \qquad } \\ { \mathrm { s o f t m a x } \big ( \mathrm { F C } _ { 3 } ( [ o _ { j } ; \mathrm { m a x - p o o l i n g } ( \mathbf { C } _ { s } ) ] ) \big ) , } \end{array}
$$

where $\mathrm { F C _ { 3 } }$ isa fully connected layer, and $[ ; ]$ isa concatenation operation over the last dimension.

# 3.4Model Training

We train the model to maximize the generation probability of the target response, given the current conversation context and history conversations in an end-to-end manner. The loss function ofHAHT is defined as,

$$
\mathcal { L } = - \sum _ { t = 1 } ^ { n _ { y } } \log \big ( P ( y _ { j } | X , H , y _ { < t } ) \big ) ,
$$

where $X$ denotes the current conversation context, $H$ denotes all history conversations, $y _ { < t }$ denotes tokens before time step $t$ ,and $n _ { y }$ denotes the length of the ground truth response.

# 4Experimental Settings

In this section,we introduce the experimental dataset, evaluationmetrics,baselinemethods,and model settings.

<table><tr><td rowspan="2">Session number</td><td colspan="2">Train</td><td colspan="2">Valid</td><td colspan="2">Test</td></tr><tr><td>Conv.</td><td>Utter.</td><td>Conv.</td><td>Utter.</td><td>Conv.</td><td>Utter.</td></tr><tr><td>1</td><td>8939</td><td>131,438</td><td>1000</td><td>7,801</td><td>1015</td><td>6,634</td></tr><tr><td>2</td><td>4000</td><td>46,420</td><td>500</td><td>5,897</td><td>501</td><td>5,939</td></tr><tr><td>3</td><td>4000</td><td>47,259</td><td>500</td><td>5,890</td><td>501</td><td>5,924</td></tr><tr><td>4</td><td>1001</td><td>11,870</td><td>500</td><td>5,904</td><td>501</td><td>5,940</td></tr><tr><td>5</td><td>-</td><td>-</td><td>500</td><td>5,964</td><td>501</td><td>5,945</td></tr><tr><td>Total</td><td></td><td>236,987</td><td>-</td><td>31,456</td><td>-</td><td>30,382</td></tr></table>

Table1:The statistics ofFacebook Multi-Session Chat (Facebook MSC) Dataset. Session number $i$ indicates there are $i - 1$ history conversation sessions that happen before the last conversation session.\*:Session 1 does not contain history conversation sessions.

# 4.1Experimental Dataset

The experiments are performed on a large dataset, i.e.,FacebookMULTI-SESSIONCHAT(Facebook MSC) (Xu et al., 2022). It is a crowdsourced dataset consisting of multi-session conversations, where the interlocutors learn about each other's interests and discuss the things they have understood from past sessions.The number of history conversationsinFacebook MSC varies from1 to 4. Session number $i$ indicates there are i-1 history conversations happening before the last conversation session. The statistics of theFacebookMSC datasetare summarized in Table 1.As session 1 doesnot have history conversations,we evaluate our model on session 2-5.

# 4.2Evaluation Metrics

We conduct both automatic and human evaluations to demonstrate the effectiveness of the proposed model. For automatic evaluations,we leverage BLEU-2,BLEU-3 (Papineni et al., 2002)，and ROUGE-L (Lin and Och, 2004) to measure word overlaps between the generated response text and ground truth text.

Moreover, we also randomly sample 5O MSCs from the test set to conduct human evaluations. We present all the history conversation sessions, current conversation context,and the generated responses to three well-educated annotators. The annotators will evaluate the quality of the generated responses from the following three aspects:

·Readability: measures whether the generated responsesare natural and fluent. · Context Relevancy: measures whether the generated responses are correlated with the current conversation context. ·History Relevancy: measures whether the generated responses are correlated with history con