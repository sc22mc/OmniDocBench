![](images/ead9d3c9c7c83330e29c1be4336e0c420d48e14fb4ff212d4b9cb214f73d675c.jpg)  
Fig.T

![](images/4946bd8539a4a5ef63fd9a4583db2a0069628356ad1618ec57aed09541f3c1c0.jpg)  
Fig.6TraidfeaLetoeatCcso Howeeel lanes.VectoretoatEsteaieddiaised

![](images/825812595e4a949b545f1d8ad403e3d52a6e980fbfc49d902861903e38380633.jpg)  
Fig.7Traidclta Cidicate

since ACC is a special case of ACC&LK (a straight road). The generalization to LK,however,is poor.Row 4 shows that no generalization from ACC and LK to ACC&LKis achieved.Fig. 6and Fig. 7indicate that these results are due to insufficient variance in the training data.

Row 9 shows that generalization from ACC and LK to ACC&LK is achieved if some data from the scenario ACC&LK are added.The resulting ADE isbetter thanwhat isachieved with the same number of samples of ACC&LK on their own (see row 1O). Hence,existing data can improve predictions for new functional scenarios or reduce the amount of data necessary to achieve a certain predictive performance.

Originally,VectorNetuses211k trainingand 41k validation samples $\boxed { 2 3 }$ p. 5]. Our results show that with $\leq 2  { \mathrm { k } }$ samples, reasonable ADEs are achievable. We expect that adjusting the architecture or hyperparameters would allow working with even less data or increasing predictive performance.

# B.Assessment Based on Predicted Evaluation Metrics

Next,we assess the prediction of evaluation metrics derived from VectorNet’s predictions (see Fig.1. The baselines are an extra-tree (ET） and a Bayesian neural network (BNN) with [17's hyperparameters. All models are trained with $2 \mathrm { k \Omega }$ samples.Table II shows that for different evaluation metrics, different models achieve the best mean average errors (MAEs).

The regression models’high performance can be attributed to them learning directly on the relevant features (5 to 1O scenario inputs).However, they cannot account for the complex spatial and temporal nuances described in the scenario embeddings (although it would be possible to take these nuances into account, this would result in up to 150 features,which regression models can hardly process).VectorNet,on the other hand,can account for the nuances but must first identify the relevant features influencing the Ego's trajectory (for the ACC scenarios,the feature (FT) matrix has 581 entrie.Hence, learning based on vectorized scenario embeddings is more flexible but also more challenging.

$$
\begin{array} { r } { ^ { 1 } ( 2 \ \mathrm { l a n e s } \cdot 2 4 \ \frac { \mathrm { v e c . } } { \mathrm { l a n e } } + \mathrm { C o } \cdot 2 5 \ \frac { \mathrm { v e c . } } { \mathrm { C o } } + \mathrm { E g o } \cdot 1 \ \frac { \mathrm { v e c . } } { \mathrm { E g o } } ) \cdot 7 \ \frac { \mathrm { F T } } { \mathrm { v e c . } } = 5 1 8 \ \mathrm { F T } } \end{array}
$$

TABLEIII MEANAVERAGE ERRORS(MAE) OFPREDICTED EVALUATIONMETRICS   

<table><tr><td>Scenario</td><td>Evaluation Metric</td><td>MAEET</td><td>MAEBNN</td><td>MAE VectorNet</td></tr><tr><td>ACC</td><td>amin</td><td>0.13</td><td>0.07</td><td>0.58</td></tr><tr><td></td><td>dmin</td><td>0.87m</td><td>0.44m</td><td>0.26m</td></tr><tr><td>LK</td><td>Platmax</td><td>0.09m 0.19</td><td>0.08m</td><td>0.38m</td></tr><tr><td rowspan="3">ACC&amp;LK</td><td>amin</td><td></td><td>0.09</td><td>0.74</td></tr><tr><td>dmin</td><td>1.14m</td><td>0.55m</td><td>0.43m</td></tr><tr><td>Platmax</td><td>0.08m</td><td>0.08m</td><td>0.21m</td></tr></table>