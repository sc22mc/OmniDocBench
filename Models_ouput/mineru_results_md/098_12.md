Hugo Touvron,Louisartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,2023b.   
Ashish Vaswani,Noam Shazeer,NikiParmar,Jakob Uszkoreit,Llion Jones,AidanNGomez,Lukasz Kaiser,and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,30,2017.   
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560,2022.   
Zhenhailong Wang, Shaoguang Mao, Wenshan Wu,Tao Ge,Furu Wei,and Heng Ji. Unleashing the emergent cognitive synergyin large language models: A task-solving agent through multi-persona self-collaboration. arXiv preprint arXiv:2307.05300,2023.   
Jason Wei,MaartenBosma,Vincent Y Zhao,Kelvin Guu,Adams WeiYu,BrianLester,NanDu, Andrew MDai,and Quoc VLe.Finetuned language modelsare zero-shot learners.arXiv preprint arXiv:2109.01652,2021.   
Jason Wei, Xuezhi Wang,Dale Schuurmans,Maarten Bosma,Fei Xia,EdChiQuoc VLe,Denny Zhou,et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems,35:24824-24837,2022.   
David Wingate,Mohammad Shoeybi,and Taylor Sorensen. Prompt compresson and contrastive conditioning for controllability and toxicity reduction in language models.arXiv preprint arXiv:2210.03162,2022.   
Yuhuai Wu,Markus NRabe,DeLesley Hutchins,and Christian Szegedy.Memorizing transformers. arXiv preprint arXiv:2203.08913,2022.   
Yadong Zhang,Shaoguang Mao,Tao Ge, Xun Wang,Yan Xia,Man Lan,and Furu Wei. K-level reasoning with large language models. arXiv preprint arXiv:2402.01521,2024.   
Hao Zhao,Maksym Andriushchenko, Francesco Croce,and Nicolas Flammarion. Long is more foralignment: A simple but tough-to-beat baseline for instruction fine-tuning.arXiv preprint arXiv:2402.04833,2024.   
Lin Zheng, Chong Wang,and Lingpeng Kong. Linear complexity randomized self-attention mechanism.In International Conference on Machine Learning,2022.

# AMODEL TRAINING CONFIGURATION

We show how to perform pretraining with the text continuation objective and instruction fine-tuning in Figure7and 8.

We train the ICAE on 8 Nvidia A10o GPUs (80GB). The hyperparameters for pretraining and fine-tuning ICAE are presented in Table 8.We by default train the ICAE with bf16.

Table 8: Hyperparameters for training   

<table><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>learning rate</td><td>1e-4 (pretrain); 5e-5 (fine-tuning)</td></tr><tr><td>batch size</td><td>256</td></tr><tr><td>warmup</td><td>300</td></tr><tr><td>#updates</td><td>200k (pretrain);30k (fine-tuning)</td></tr><tr><td>clip norm</td><td>2.0</td></tr></table>