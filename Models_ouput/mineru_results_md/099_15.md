<table><tr><td>scenario</td><td>train</td><td>test</td><td>scenario</td><td>train</td><td>test</td><td>scenario</td><td>train</td><td>test</td></tr><tr><td>others</td><td>317</td><td></td><td></td><td>40</td><td>11</td><td>classification_identification</td><td>24</td><td>6</td></tr><tr><td>functional_writing</td><td>128</td><td></td><td>explaining_code</td><td>40</td><td>10</td><td>language_polishing</td><td>2222</td><td>4</td></tr><tr><td>brainstorming</td><td>90</td><td></td><td>writing_legal_document</td><td>40</td><td>10</td><td>chitchat</td><td></td><td>7</td></tr><tr><td>seeking_advice</td><td>88</td><td></td><td>asking_how_to_question</td><td>40</td><td>10</td><td>writing_product_description</td><td>20</td><td>5</td></tr><tr><td>open_question</td><td>77</td><td></td><td>writing_presentation_script</td><td>38</td><td>10</td><td>data_analysis</td><td>18</td><td>5</td></tr><tr><td>explaining_general</td><td>66</td><td>17</td><td>writing_social_media_post</td><td>38</td><td>10</td><td>writing_marketing_materials</td><td>17</td><td>5</td></tr><tr><td>instructional_rewriting</td><td>58</td><td>15</td><td>question_generation</td><td>38</td><td>10</td><td>note_summarization</td><td>17</td><td>4</td></tr><tr><td>verifying_fact</td><td>49</td><td>13</td><td>planning</td><td></td><td>10</td><td>paraphrasing</td><td>17</td><td>5</td></tr><tr><td>analyzing_general</td><td>49</td><td>13</td><td>writing_blog_post</td><td>3830</td><td>9</td><td>writing_technical_document</td><td>17</td><td>5</td></tr><tr><td>title_generation</td><td>48</td><td>12</td><td>writing_job_application</td><td>36</td><td>10</td><td>text_simplification</td><td>16</td><td>5</td></tr><tr><td>code_generation</td><td>48</td><td>12</td><td>writing_personal_essay</td><td>36</td><td>10</td><td>information_extraction</td><td>16</td><td>2</td></tr><tr><td>roleplay</td><td>47</td><td>12</td><td>value_judgement</td><td>35</td><td>9</td><td>writing_biography</td><td>16</td><td>4</td></tr><tr><td>rejecting</td><td>45</td><td>12</td><td>code_to_code_translation</td><td>32</td><td>9</td><td>text_correction</td><td>12</td><td>6</td></tr><tr><td>creative_writing</td><td>45</td><td>12</td><td>writing_advertisement</td><td>31</td><td></td><td>reading_comprehension</td><td>12</td><td>3</td></tr><tr><td>exam_question_without_math</td><td>44</td><td>12</td><td>writing_email</td><td>30</td><td></td><td>keywords_extraction</td><td>12</td><td>3</td></tr><tr><td>writing_song_lyrics</td><td>44</td><td>11</td><td>recommendation</td><td>2928</td><td>888</td><td>topic_modeling</td><td>10</td><td>3</td></tr><tr><td>text_to_text_translation</td><td>43</td><td>11</td><td>ranking</td><td></td><td>8</td><td>writing_scientific_paper</td><td>10</td><td>3</td></tr><tr><td>text_summarization</td><td>43</td><td>12</td><td>counterfactual</td><td>26</td><td>7</td><td>peer_review</td><td>7</td><td>2</td></tr><tr><td>code_correction_rewriting</td><td>43</td><td>11</td><td>exam_question_with_math</td><td>24</td><td>4</td><td>code_simplification</td><td>6</td><td>2</td></tr><tr><td>math_reasoning</td><td>41</td><td>12</td><td>writing_news_article</td><td>24</td><td>6</td><td>overll</td><td>2383623</td><td></td></tr></table>

Table 7: The scenario distribution in the training and test set for scenario classifier, note that “rejecting” and "peer_review”are two early-defined scenarios that have been removed by us.

# BTRAINING DETAILS OF SCENARIO CLASSIFIER

In this section we describe in detail the training process of the scenario classifier mentioned in We model the scenario classification task as a generation task.The classifier are required to generate only the scenario name when given the query，with the prompt as "Identifythe scenario forthe user'squery，output'default’ifyou are uncertain.\n\nQuery:\n\n{input}\n\nScenario:"(the "default" scenario in the prompt is the early naming for "others" scenario).

In general, the training involves three steps:

1.We first brainstorm about 10 seed queries for each scenario with the help of ChatGPT,and train a model that can directly output the scenario name when given a query as a conditional generation task on this small synthetic dataset.   
2.Using the trained model,we conducted an initial classification for queries in Chatbot Arena Conversations and ShareGPTas they cover much more scenarios than other datasets.Based on this preliminary classification, we randomly select up to 5O queries from each scenario for a secondary manual validation,involving data cleaning and correcting misclassified labels.   
3.We combine the newly-collected dataset and the small synthetic dataset in step 1,and retrain our final classifier. We divide queries in each scenario in an 8:2 train/test split (Tab. $\textcircled { 7 }$ .The accuracy and F1 of the final classifier on test set are 72.55 and 74.12,respectively.

Our scenario classifier is trained from LLaMA-2-13B (Touvron et al.2023b), and we set the max sequence length as 2,O48,and the max length for query as $\overline { { 2 , 0 4 8 - 5 0 } } \overline { { = 1 , 9 9 8 } }$ both in training and inference.If aquery $Q$ withlength $L$ exceeds that limit,we truncate it from the middle and replace the dropped part with a ".." since the front and end of the sequence usually contain more important information for identifying scenario of the (such as the user's instruction): $Q _ { 1 : L } \ \to$ $[ Q _ { 1 : 9 9 9 } ; . . . ; Q _ { L - 1 0 0 0 : L } ]$ ·

We train the scenario classifier for3 epochs on the training set,and set the batch size as 64.Without warmup steps, we set the initial learning rate to 1e-5and cosine decaying to O by the end of training. The optimizerisAdamWwith $\beta _ { 1 } = 0 . 9 , \beta _ { 2 } = 0 . 9 5$ as in training AUTO-J,and we also use the speedupand GPU memory saving techniques like DeepSpeed Zero 3,BF16,TF32,and gradientcheckpointing.The loss isonly calculated on the output end as well.