Algorithm 2 (Discretized MMGA） takes not only its learning rate $\eta$ but a small value $\gamma$ in measuring an approximate gradient as inputs.In each time step,each player measures the gradients of its payof for each variable ofits strategy (lines 2-6).Then,the player updates its strategy by the gradients (lines 7-10). Here,note that the strategy update is weighted by the probability $x ^ { a | s }$ (line 8） in order to correspond to Algorithm 1.Here,each of lines 3-5 and line 8can beupdated in parallel with respect to $a$ and $s$

# 4Theoretical Analysis

# 4.1Continuous-Time Equivalence of Algorithms

The following theorems provide a unified understanding of diferent algorithms.Theorem 1 and 2 are concerned with continualizationof the twodiscrete algorithms.Surprisingly，Theorem 3 proves thecorrespondence between these different continualized algorithms by Theorem 1 and 2.

Theorem 1 (Coutinualized MMRD). Let $\pmb { p } ^ { a | s }$ be the expected distribution when $X$ chooses a under state s;

$$
p _ { i ^ { \prime } } ^ { a | s } : = \left\{ \begin{array} { l l } { y ^ { b | s } } & { ( s _ { i ^ { \prime } } = a b s ^ { - } ) } \\ { 0 } & { ( \mathrm { o t h e r w i s e } ) } \end{array} \right. .
$$

In the limit of $\eta  0$ ，Algorithm $\mathit { 1 }$ is continualized as dynamics

$$
\begin{array} { l } { { \displaystyle { \dot { x } } ^ { a | s _ { i } } ( { \bf x , y } ) = p _ { i } ^ { \mathrm { s t } } x ^ { a | s _ { i } } \left( \pi ( p ^ { a | s _ { i } } , { \bf x , y } ) - \bar { \pi } ^ { s _ { i } } ( { \bf x , y } ) \right) , } } \\ { { \displaystyle { \bar { \pi } } ^ { s _ { i } } ( { \bf x , y } ) = \sum _ { a } x ^ { a | s _ { i } } \pi ( p ^ { a | s _ { i } } , { \bf x , y } ) , } } \end{array}
$$

for all $a \in { \mathcal { A } }$ and $s \in S$ Here, $\bar { \pi } ^ { s _ { i } }$ is the expected payoff under state $s _ { i }$

Theorem 2 (Continualized MMGA). In the limit of $\gamma  0$ and $\eta  0$ ，Algorithm $\mathcal { L }$ is continualized as dynamics

$$
{ \dot { x } } ^ { a | s } ( \mathbf { x } , \mathbf { y } ) = x ^ { a | s } { \frac { \partial } { \partial x ^ { a | s } } } u ^ { \mathrm { s t } } ( \mathrm { N o r m } ( \mathbf { x } ) , \mathbf { y } ) ,
$$

forall $a \in { \mathcal { A } }$ and $s \in S$

See Appendix A.1 and A.2 for the proof of Theorems 1 and 2.

Theorem 3 (Equivalence between the algorithms). The dynamics Eqs. (8) and (10) are equivalent.

Proof Sketch.Let $\mathbf { x } ^ { \prime }$ be the strategy given by $x ^ { a | s }  x ^ { a | s } + \gamma$ in $\mathbf { x }$ for $a \in { \mathcal { A } }$ and $s \in S$ . Then, we consider the changes of the Markov transition matrix $\mathrm { d } M : = M ( \mathrm { N o r m } ( \mathbf { x } ^ { \prime } ) , \mathbf { y } ) - M ( \mathbf { x } , \mathbf { y } )$ and the stationary distribution $\mathrm { d } p ^ { \mathrm { s t } } : = p ^ { \mathrm { s t } } ( \mathrm { N o r m } ( \mathbf { x } ^ { \prime } ) , \mathbf { y } ) - p ^ { \mathrm { s t } } ( \mathbf { x } , \mathbf { y } )$ .By considering this changes in the stationary condition $p ^ { \mathrm { s t } } = M p ^ { \mathrm { s t } }$ ， we get $\mathrm { d } p ^ { \mathrm { s t } } = ( { \cal E } - { \cal M } ) ^ { - 1 } \mathrm { d } { \cal M } p ^ { \mathrm { s t } }$ in $O ( \gamma )$ . The right-hand (resp. left-hand) side of this equation corresponds to the continualized MMRD (resp. MMGA). See Appendix A.3 for the full proof. □

For games with a general number of actions,the study [7] has proposed a gradient ascent algorithm in relation toreplicator dynamics.In light of this study,Theorem 3extends the relation to the multi-memory games.This extension is neither simple nor trivial. The relation between replicator dynamics and gradient ascent has been proved by directly calculating $u ^ { \mathrm { s t } } = p ^ { \mathrm { s t } } \cdot u$ [17]. In multi-memory games, however, $u ^ { \mathrm { s t } } = p ^ { \mathrm { s t } } { \cdot } u$ is too hard to calculate.Thus,as seen in the proof sketch,we proved the relation by consideringa slight change in the stationary condition $p ^ { \mathrm { s t } } = M p ^ { \mathrm { s t } }$ ,technically avoiding such a hard direct calculation.

# 4.2Learning Dynamics Near Nash Equilibrium

Below,let us discuss the learning dynamics in multi-memory games,especially divergence from Nash equilibrium in zero-sum payoff matrices.In order to obtaina phenomenological insight into the learning dynamics simply,we assume one-memory two-action zero-sum games in Assumption 1.