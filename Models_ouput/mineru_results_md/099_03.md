![](images/06f272eeed0569b6c067e9ac8a0f1c0fb7ce63e90ca5e1637ffed3e7dcb9e8f9.jpg)  
Figure 1: An overview for our data construction pipeline in three steps.

Using strong LLMs (usually closed-source ones, e.g.,GPT-4, Claude, ChatGPT) as an automated proxy for assessing LLMs has become a natural choice (Zhou et al.2023). With appropriate prompt design, the quality of evaluation and agreement to human judgment can be promising Dubois et al. 2023Zheng et al.2023 Zhang et al.2023Wang et al.2023a). However, the cost concernstill exists when calling the APIs of these proprietary models, especially when there is a frequent need for model validation on large-scaledata.Moreover,closed-source evaluation leads to low reproducibility due to potential changes in models behind the API.Some recent works have started to make attempts for open-source alternatives. SelFee $\{ \mathrm { Y e ~ e t ~ a l . } \} [ 2 0 2 3 ]$ collects generations, feedback,and revised generations from ChatGPTand fine-tunes LLaMA models to build a critique model. Shepherd (Wang $\boxed { \mathrm { e t ~ a l . } } \boxed { 2 0 2 3 6 }$ trains a model that can output critiques for single-response with the data of feedback from online communities and human annotation. PandaLM (Wang et al.2023c) trains a model to conduct pairwise comparison for LLM Instruction Tuning Optimization, and Zheng et al.(2023 also fine-tune Vicuna $\mathbb { ( C h i a n g e t a l . ] } \mathbb { 2 0 2 3 } \mathbb { ) }$ on a 20K pairwise comparison dataset to explore the potential of open-source models as a more cost-friendly proxy.

# 2.2META-EVALUATION TESTBED FOR LLMEVALUATORS

Besides the evaluators themselves,there is also a practical need to construct a comprehensive testbed to meta-evaluate them (i.e.,assessing the quality of their evaluation). $\mathrm { I n } [ \mathrm { Z h e n g ~ e t ~ a l . } ] ( \mathbb { Z } 0 2 3 )$ MTBench and Chatbot Arena Conversations are proposed.The former has only 80 human-crafted queries,each with several LLMs' responses and expert-level human annotation on pairwise comparison; the latter is alarge collection of crowdsourced data,with more than 3OK queries from real-world users and their vote on pairs of responses from different LLMs. FairEval $\mathrm { \textregistered { ‰} }$ isbased on the 80 queries from VicunaBench $\| \mathrm { C h i a n g ~ e t ~ a l . } \| 2 0 2 3 \|$ with human annotated labels between ChatGPT and Vicuna responses. PandaLM (Wang et al.|2023c constructs a test set comprising 999 pairwise samples, with queries from 252 user-oriented instructions inWang et al.(2022).LLMEval2 Zhang et al. $\boxed { 2 0 2 3 }$ is much larger than the previous two, with 2,553 samples compiled from multiple data sources with human-annotated preferences. Shepherd $\mathrm { ( W a n g ~ e t ~ a l . } ) \tilde { \left[ 2 0 2 3 \mathrm { b } \right] }$ collects 352 samples from multiple sources for its critique model as a test set to evaluate the quality of the critiques.

# 3DATA CONSTRUCTION

We constructdata from massive real-world scenarios with high-quality evaluation judgments for both training and testing.The data construction pipeline involves three main steps: (1) defining evaluation scenario and criteria,(2) collecting real-world queries and responses from different models for these scenarios and (3) generating desired evaluation judgments for different evaluation protocols.An overview of our data construction pipeline is shown in Fig.

# 3.1SCENARIO AND CRITERIA DEFINITION

ScenarioWe define 58 scenarios (including one “others” scenario), categorized into eight major groups: Summarization, Exam Questions, Code,Creative Writing,Functional Writing,Rewriting,