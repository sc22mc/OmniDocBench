# 3Algorithm

In the following,we define multi-memory versions oftwo major learning algorithms,i.e.,replicator dynamics and gradient ascent.Although we consider the learning of player X,that of player Ycan be formulated in the same manner.

Definition 1 (expected future payoff).We define the expected future payoff from the distribution $\mathbfcal { p }$ as

$$
\pi ( \pmb { p } , \mathbf { x } , \mathbf { y } ) : = \sum _ { t = 0 } ^ { \infty } M ^ { t } ( \pmb { p } - \pmb { p } ^ { \mathrm { s t } } ) \cdot \pmb { u } ,
$$

which is the total payoff player $X$ obtains from the present round to the future.

In thisdefinition,the stationary payoff $\pmb { p } ^ { \mathrm { s t } } \cdot \pmb { u } = u ^ { \mathrm { s t } }$ is the offset term every round,and thus $\pi ( p ^ { \mathrm { s t } } , { \bf x } , { \bf y } ) =$ 0.

Definition 2 (normalization). We define the normalization function N $\begin{array} { r } { \mathrm { \mathrm { ~ \ u r m } : \prod } _ { s \in { \mathcal { S } } } \mathbb { R } _ { + } ^ { m } \mapsto \prod _ { s \in { \mathcal { S } } } \operatorname { i n t } ( \Delta ^ { m - 1 } ) } \end{array}$ as

$$
\mathrm { N o r m } ( { \bf x } ) = \left\{ \frac { x ^ { a \vert s } } { \sum _ { a ^ { \prime } } x ^ { a ^ { \prime } \vert s } } \right\} _ { a , s } ,
$$

In this definition, $\operatorname { N o r m } ( \mathbf { x } )$ satisfies the condition of probability variables for all $s$ Based on these definitions,we formulate discretized MMRD and MMGA as Algorithm 1and 2.

# Algorithm1Discretized MMRD

Input: $\eta$

1:for $t = 0 , 1 , 2 , \cdots$ do   
2: X chooses $a$ with probability $x ^ { a | s _ { i } }$   
3: (Y chooses $b$ with probability $y ^ { b | s _ { i } }$ ）   
4: $s _ { i ^ { \prime } }  a b s _ { i } ^ { - }$   
5: $x ^ { a | s _ { i } } \gets x ^ { a | s _ { i } } + \eta \pi ( e _ { i ^ { \prime } } , \mathbf { x } , \mathbf { y } )$   
6: $\mathbf { x } \gets \mathrm { N o r m } ( \mathbf { x } )$   
7: $s _ { i } \gets s _ { i ^ { \prime } }$   
8:end for

Algorithm 1 (Discretized MMRD) takes its learning rate $\eta$ asan input. In each time step,the players choose their actions folowing their strategies (lines 2 and 3)，while the state is updated by their chosen actions (lines 4and 7).Then,each player reinforces its strategy by how much payoffthe chosen action bringsup to the future.Here,note that for simplicity,this payoffis given byan expected payoff(line 5).

# Algorithm2Discretized MMGA