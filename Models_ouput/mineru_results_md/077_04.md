<table><tr><td>HhnrTFhewarpMremreWitnoutenedproposur Require: Initial state Xo =(X,X)Ôºå chain length A proposalbatchsizeB. m‚Üê0 whilem&lt;Mdo SampleX1,...,XB~pe(¬∑|XùëöÔºâ {Batch sample}</td></tr><tr><td>forb=1,...,Bdo ‚àà~N(O,I) {Resample auxiliary variables} Xb‚Üê(Xm,eÔºâ SampleI~Bernouli(Œ±(Xb,Xb)) end for</td></tr><tr><td>ifSÔºö={b:Ib=1,1‚â§b‚â§B}‚â†„ÅÆthen a=min(S) {First accepted sample} ÔºàXm Xùëö+a-1)‚Üê(Xùëö.,Xùëö)</td></tr><tr><td>m+1 Xm+a‚ÜêX m‚Üêm+a else Xùëö+B)‚Üê(XùëöÔºå.,Xm) m+1Ôºå¬∑ m‚Üêm+B end if</td></tr></table>

# 3.5.Fast exploration of the state space without MH

Although the MH correction ensures that Timewarp provides asymptotically unbiased samples, it can lead to much slower exploration of the state space due to the rejected proposals.For some of the peptides we consider, the acceptance probabilities are too lowto apply Algorithm1 effectively. Instead,we can apply Timewarp in a simple exploration algorithm,where we ignore the MH correction and accept all proposals with an energy change lower than some cutoff. This allows much faster exploration of the state space,and in Section 6 we show that the algorithm,although technically biased,often leads to qualitatively accurate free energy estimates. It also succeeds in discoveringall metastable states of a peptide orders of magnitude faster than Algorithm 1 and standard MD.Timewarp applied in exploration mode canbeused to efficiently find the metastable states of a new molecule,which could beused,e.g.,to provide initialisation states for a subsequent MSMmethod, although we do not pursue this here.We provide pseudocode for the exploration algorithmin Algorithm2 in Appendix C.

# 4.Model architecture

Wenow describe the architecture of our conditional normalising flow $f _ { \theta } ( z ^ { p } , z ^ { v } ; x ^ { p } ( t ) )$ ,which is shown in Figure2.

RealNVP coupling flowOur architecture isbased on RealNVP(Dinh et al.,2017),which consists of a stack of coupling layers which affinely transform subsets of the dimensions of the latent variable based on the other dimensions.Specifically,we transform the position variables based on the auxiliary variables,and vice versa.In the $\ell$ th coupling layer of the flow, the following transformations are implemented:

$$
\begin{array} { r l } & { z _ { \ell + 1 } ^ { p } = s _ { \ell , \theta } ^ { p } ( z _ { \ell } ^ { v } ; x ^ { p } ( t ) ) \odot z _ { \ell } ^ { p } + t _ { \ell , \theta } ^ { p } ( z _ { \ell } ^ { v } ; x ^ { p } ( t ) ) , } \\ & { z _ { \ell + 1 } ^ { v } = s _ { \ell , \theta } ^ { v } ( z _ { \ell + 1 } ^ { p } ; x ^ { p } ( t ) ) \odot z _ { \ell } ^ { v } + t _ { \ell , \theta } ^ { v } ( z _ { \ell + 1 } ^ { p } ; x ^ { p } ( t ) ) . } \end{array}
$$

Going forward, we suppress the coupling layer index $\ell$ Here $\odot$ is the element-wise product, and $s _ { \theta } ^ { \bar { p } } : \bar { \mathbb { R } } ^ { 3 N } \to \mathbb { R } ^ { 3 N }$ is ouratom transformer,a neural network based on the transformer architectureÔºàVaswani etal., 2O17) that takes the auxiliary latent variables $z ^ { v }$ and the conditioning state $x ( t )$ and outputs scaling factors for the position latent variables $z ^ { p }$ .The function $t _ { \theta } ^ { p } : \mathbb { R } ^ { 3 N } \to \mathbb { R } ^ { 3 N }$ is implemented as another atom transformer,which uses $z ^ { v }$ and $x ( t )$ to output a translation of the position latent variables $z ^ { p }$ .The affine transformations of the position variables (in Equation (9)) areinterleaved with similaraffine transformations for the auxiliary variables (in Equation (1O)). Since the scale and translation factors for the positions depend only on the auxiliary variables,and vice versa,the Jacobian of the transformation is lower triangular,allowing for effcient computation of the density. The full fow $f _ { \theta }$ consists of $N _ { \mathrm { c o u p l i n g } }$ stacked coupling layers,beginning from $z \sim \mathcal { N } ( 0 , I )$ and ending with a sample from $p _ { \theta } ( x ( t + \tau ) | x ( t ) )$ . This is depicted in Figure 2,Left. Note that there is a skip connection such that the output of the flow predicts the change $x ( t + \tau ) - x ( t )$ rather than outputting $x ( t + \tau )$ directly.

Atom transformer We now describe the atom transformer network. Let $x _ { i } ^ { p } ( t ) , z _ { i } ^ { p } , z _ { i } ^ { v }$ ,all elements of $\mathbb { R } ^ { 3 }$ ,denote respectively the position of atom $i$ in the conditioning state,the position latent variable for atom $i$ ,and theauxiliary latent variable for atom $i$ .To implement an atom transformer which takes $z ^ { v }$ as input (such as $s _ { \theta } ^ { p } ( z ^ { v } , x ^ { p } ( t ) )$ and $t _ { \theta } ^ { p } ( z ^ { v } , x ^ { p } ( t ) )$ in Equation (9)), we first concatenate the variables associated with atom $i$ ÔºéThis leadsto a vector $a _ { i } : = [ x _ { i } ^ { p } ( t ) , h _ { i } , z _ { i } ^ { v } ] ^ { \mathsf { T } } \ \in \ \mathbb { R } ^ { H + 6 }$ Ôºåwhere $z _ { i } ^ { p }$ has been excluded since $s _ { \theta } ^ { p } , t _ { \theta } ^ { p }$ are not allowed to depend on $z ^ { p }$ Here $\boldsymbol { h } _ { i } \in \mathbb { R } ^ { H }$ is a learned embedding vector which depends only on the atom type. The vectors $a _ { i }$ are fed into an MLP $\phi _ { \mathrm { i n } } : \mathbb { R } ^ { H + 6 }  \mathbb { R } ^ { \bar { D } }$ ,where $D$ is the feature dimension of the transformer. The vectors $\phi _ { \mathrm { i n } } ( a _ { 1 } ) , \ldots , \phi _ { \mathrm { i n } } ( a _ { N } )$ are then fed into $N _ { \mathrm { t r a n s f o r m e r } }$ stacked transformer layers. After the transformer layers,they are passed through another atomwise MLP, $\phi _ { \mathrm { o u t } } : \mathbb { R } ^ { D } \to \mathbb { R } ^ { 3 }$ .The final output is in $\mathbb { R } ^ { 3 N }$ as required. Thisis depicted inFigure2,Middle.When implementing $s _ { \theta } ^ { v }$ and $t _ { \theta } ^ { v }$ from Equation (1O),a similar procedure is performed on the vector $[ x _ { i } ^ { p } ( t ) , h _ { i } , z _ { i } ^ { p } ] ^ { \mathsf { T } }$ , but now including $z _ { i } ^ { p }$ and excluding $z _ { i } ^ { v }$ .There are two key differences between the atom transformer and the architecture in Vaswani et al. (2017). First, to maintain permutation equivariance, we do