Table 5. Timewarp loss weighting factors   

<table><tr><td>Dataset</td><td>Llik(0）</td><td>Lace(0)</td><td>Lent（0)</td></tr><tr><td>AD</td><td>0.99</td><td>0.01</td><td>0.1</td></tr><tr><td>2AA</td><td>0.9</td><td>0.1</td><td>0.1</td></tr><tr><td>4AA</td><td>1</td><td>0</td><td>0</td></tr></table>

Table 6. Timewarp training parameters   

<table><tr><td>Dataset+ training method</td><td>Batch size</td><td>No.ofA-100s</td><td>Training time</td></tr><tr><td>AD—likelihood</td><td>256</td><td>1</td><td>1 week</td></tr><tr><td>AD—acceptance</td><td>64</td><td>1</td><td>2 days</td></tr><tr><td>2AA—likelihood</td><td>256</td><td>4</td><td>2 weeks</td></tr><tr><td>2AA—acceptance</td><td>256</td><td>4</td><td>4days</td></tr><tr><td>4AA— likelihood</td><td>256</td><td>4</td><td>3weeks</td></tr></table>

# F. Computing infrastructure

The training was performedon4NVIDIA A-100 GPUs for the2AA and4AA datasets and onasingle NVIDIA A-100 GPU fortheADdataset. Inferencewith the modelas wellas allMDsimulations were conductedonsingleNVIDIA V-100 GPUs forADand2AA,and on singleNVIDIA A-100 GPUs for4AA.