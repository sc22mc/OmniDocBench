![](images/b2be3354402fcdf71d9f6504da3f27147df1745164cc34ee40a425d39f6760df.jpg)  
Figure 3: Win-rate of AUTo-J against baselines on single-response critique generation task, judged byGPT-4and Human.L2Chat refers to LLaMA-2-Chat-13B.

options to further optimize training and effciency.The model is trained for5 epochs (675 parameter update steps in total) and we save checkpoints for every 50 steps.We use AdamW (Loshchilov & $\boxed { \mathrm { H u t t e r } } \boxed { 2 0 1 7 }$ asour optimizer with $\beta _ { 1 } = 0 . 9 , \beta _ { 2 } = 0 . 9 5$ and weight decay of O.1. We use a peak Tearning rate1e-5with $3 \%$ warmup steps and cosine learning rate decay to O,and set the batch size to 64 and maximum sequence length to 4,O96. The loss is only calculated on the output end.

# 5EVALUATION SETTING

# 5.1TASKANDTESTSET

Task I: Pairwise Response Comparison (Eval-P）In this task,the evaluators will see a pair of generated responses fora given query and decide which is beter or is tied.From each scenario defined in $\ S \boxed { 3 . 1 }$ we randomly sample 24 pairwise comparison samples from the data we collected in $\ S 3 . 2$ and skip those that have been used as training data.For some scenarios,the number of paired samples with pre-existed human annotation is smaller than 24,so we extract queries from either ShareGPT or the brainstormed seed data for training scenario classifier in $\mathfrak { s } \underline { { \mathbf { B } } }$ Samples from these two sources have no annotated pairwise labels,so we only use the query for each sample, generate a new pair of responses from two random selected LLMs²and manuallyannotate them. In total, we have $5 8 \times 2 4 = 1 , 3 9 2$ testing samples, each with two responses generated by different LLMs and a human-annotated preference label.We refer to this test set as Eval-P,with the distribution on Win/Tie/Lose being520/373/499.

Task II: Critique Generation for Single Response (Eval-C）In this task,we evaluate the quality of the generated critiques for single-response evaluation.The evaluators are required to write critiques for a response to pinpoint its shortcomings in addressing the query.We apply both GPT-4 and human evaluation to compare critiques generated by different models. In GPT-4 evaluation, we randomly shuffle the order of two critiques to mitigate the positional bias,and use the instruction in Tab.16 In human evaluation, we recruit four expert-level annotators (graduate students)and guide them with the same instruction for GPT-4.We build the test set for this task on the basis of Eval-Pby sampling 4 out of 24 queries for each scenario and pick the less preferred response for each query (if tie, we randomly pick one). We refer to this test set as Eval-C, with $5 8 \times 4 = 2 3 2$ query-response pairs.

Task III: Overall Rating for SingleResponse (Eval-R)In this task, we evaluate the usefulness of the final rating for single-response evaluation in two ways: (1) The first is to use the ratings as verbal “rewards”to help improve the base policy models through the Best-of- $N$ selection Lightman $\boxed { \mathrm { e t ~ a l . } } \boxed { 2 0 2 3 } \boxed { \mathrm { G a o ~ e t ~ a l . } } \boxed { 2 0 2 3 }$ ,i.e., selecting the best response among the first $N$ candidates with the assigned rewards,and use GPT-4 to grade the selected response. Generally,a more reliable model will select a better response with a higher GPT-4 rating more often. (2) The second is to calculate the