response-level correlations between model-generated ratings and GPT-4 ratings.To save cost, we only collect the GPT-4 ratings on the previous “best-of- $N ^ { \ast }$ responses. The test set for this task is built on the basis of Eval-Cby sampling2 out of 4 queries for each scenario.Weask two different base LLMs(LLaMA-2-chat-7B and Vicuna-7B-v1.5) to generate 32 responses for each query through uniform sampling (temperature set as 1.O).We refer to this test set as Eval-R,with $5 8 \times 2 = 1 1 6$ queries and $1 1 6 \times 3 2 = 3 , 7 1 2$ query-response pairs for each base LLM.

# 5.2BASELINES

General-purpose models: We use LLaMA-2-Chat-13B(Touvron et al.2023b), Vicuna-13B-v1.5 Chiang et al.2023) WizardLM-13B-v1.2 (Xu et al.[2023),and ChatGPT(GPT-3.5-turbo-0613). We also use GPT-4 (GPT-4-0613) in the pairwise comparison and critique generation,and Claude-2 and LLaMA-2-Chat-7OB in pairwise comparison.These models are used with corresponding prompt for each task: pairwise comparison prompt in Tab.14 critique generation prompt in Tab. $\boxed { 1 8 }$ (the same input format for AUTo-J's single-response evaluation),and rating prompt in Tab.[15Evaluationspecific models:We use SelFee $\left( \mathrm { Y e ~ e t ~ a l . } \right) \left[ 2 0 2 3 \right]$ in critique generation, SteamSHP Ethayarajh et al. $\boxed { 2 0 2 2 }$ in pairwise comparison and overal rating, Open-Assistant's reward model (Kopf et al.|2023) in overall rating, and PandaLM $\mathrm { \textregistered N a n g ~ e t ~ a l . } ] \mathrm { \textbar { 2 0 2 3 c } } )$ in pairwise comparison.

# 6EXPERIMENTS

# 6.1PAIRWISE RESPONSE COMPARISON

A common problem in pairwise response comparison is positional bias $\mathrm { ( W a n g ~ e t ~ a l . ) } \big [ 2 0 2 3 \mathrm { a } \big ]$ ,whereanLLMmay tend to favor specific positions, causing inconsistency in comparison results when response orders are swapped. To pursue stable and reliable results,we conduct two comparisonsfor each sampleby swappingthe orderof the two responses in the prompt. We consider a model's judgment to agree with human only when the two comparison results are consistent and align with the human judgment.

The agreement rates for AUTO-J and the baselines on Eval-P are in Tab.AUTO-Jachieves a significantly higher agreement rate than all baselines except GPT-4 on every scenario group.We also plot the prediction consistency for each modelin Fig.4AUTo-Jhas a similar consistency rate to GPT-4 and is far more consistent than all other baselines,which makes it a more reliableand robust judge for pairwise comparison

![](images/4cf44a963cb6a23b30d1d7f3a50db2a853fab089daf8826c0418b99e35f6268c.jpg)  
Figure 4: Consistency of prediction when swapping the response order.

# 6.2CRITIQUE GENERATION FOR SINGLE-RESPONSE

The comparison results on Eval-C given by GPT-4 and human are in Fig. $\textcircled { 3 }$ and the complete comparison results for different scenario groups are in Tab. $2 3$ In both evaluation settings, AUTO-J performs significantly than all baselines,including GPT-4,reflecting the strong ability to criticize other LLMs’outputs.We also observe that GPT-4 tends to provide judgments with very few ties, whereas humans often give tie judgments in comparisons, sometimes even exceeding $30 \%$ One possible explanation is that the critique from AUTO-Jexhibit a clearer structureand readability, which leads GPT-4 to pay less atention to the content when making comparisons, while humans are able to read more attentively and discern subtle differences between two critiques.

# 6.3OVERALL RATING FOR SINGLE-RESPONSE

We conduct experiments on Eval-R with the $N$ inBest-of- $N$ selection set as 8,16,and 32.In practice,if two responses share a common model rating,we choose the one with a higher output probability. Results in Tab. $2$ show that responses selected by AUTO-J generally get higher GPT-4 ratings than those selected by baselines on different $N$