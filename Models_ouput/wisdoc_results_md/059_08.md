encoding the history conversations can help the model reserve more history memory to generate more human-like responses. Moreover, HAHT achieves better performance than HAHTHIST. This observation indicates that removing the history en- coder causes the most decline in all metrics. This result confirms the necessity to leverage history conversations to understand the current conversa- tion and generate the response. In addition, the performance degradation caused by removing the switching mechanism shows that directly copying words from the history conversation can help the model generate more history-aware responses.

5.5 Case Study

Table 6 shows a case study of the multi-session conversations generated by different models. Com- pared to baseline models, the proposed HAHT model can better leverage history conversations to understand the current conversation context and generate more history-aware responses. When the user discusses preparing sandwiches and lemonade with the agent, "I can make sandwiches for us! I also have a very good recipe for homemade lemon- ade! Do you like lemonade?", HAHT can remem- ber information mentioned in the history conversa- tions, such as the user likes reading and outdoor ac- tivities and it has adopted a book-lover persona be- fore. HAHT can leverage these historical contexts and generate more human-like, context-relevant, and history-aware responses: “I love lemonade! I’m sure we can find a lot of good recipes for sand- wiches too. Sandwiches and lemonade are perfect for going outdoors and reading books.”.

6 Conclusion

In this work, we propose the History-Aware Hi- erarchical Transformer (HAHT) model for multi- session open-domain dialogue systems. The pro- posed HAHT model maintains a history memory by hierarchically encoding the history conversation sessions. After that, HAHT uses attention-based en- coding to encode the current conversation context together with the history memory and updates the history memory with the current context. In order to explicitly leverage historical information in the responses, HAHT is designed with a history-aware response generator which can switch between a generic vocabulary and a history-aware vocabulary. Experimental results obtained under both normal and session opening MSC settings demonstrate that

History Conversation Sessions

[Session 1]

...

User:

... My dad is a truck driver.

Agent:

... I am a big reader myself.

User:

... I like working outdoors.

...

[Session 2]

User:

What book are you currently

reading?

Agent:

I’m reading a Stephen King

novel. ...

...

User:

I used to sit in the truck on

long tripswith my dad, and had

nothing to do other than read.

Agent:

Ah, that makes sense. I started

reading when my parents were

at work. ...

Current Conversations

...

Agent:

... What kinds of picnic foods

do you like?

User:

I can make sandwiches for us! I

also have a very good recipe for

homemade lemonade! Do you

like lemonade?

Responses from Different Dialogue Systems

BlenderBot: I love lemonades ! I’m not sure

if I could make a picnic for you

though. I’d have to make a lot

more than just a picnic.

BlenderBotmsc: I love lemonade! I’m not a

huge fan of lemonade, but I love

lemonade.

FID-RAG: I love lemonade! I’m sure we

can find a good picnic for you.

HAHT

(ours):

I love lemonade! I’m sure we

can find a lot of good recipes for

sandwiches too. Sandwiches

and lemonade are perfect for

going outdoors and reading

books.

Table 6: A case study of an MSC with two history con- versations. Only important utterances in the history and current conversations are presented. Complete conver- sations sessions are provided in Appendix A.1