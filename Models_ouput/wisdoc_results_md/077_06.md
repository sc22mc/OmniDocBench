Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics

Figure 3. Alanine dipeptide experiments. (a) Ramachandran plots for MD and Timewarp samples generated according to Algorithm 1. (b) Free energy comparison for the two dihedral angles ϕ and ψ. (c) Ramachandran plots for the conditional distribution of MD compared with the Timewarp model. Red cross denotes initial state. (d) Time dependence of the ϕ dihedral angle of MD and the Markov chain generated with the Timewarp model.

Table 1. Dataset details

\begin{tabular}{lccc} \multicolumn{4}{c}{Table 1: Dataset details} \\ Dataset name & AD & 2AA & 4AA \\[5pt] Training set simulation time & $100$~ns & $50$~ns & $50$~ns \\ Test set simulation time & $100$~ns & $1$~$\mu$s & $1$~$\mu$s \\ MD integration step $\Delta t$ & $0.5$~fs & $0.5$~fs & $0.5$~fs \\ Timewarp prediction time $\tau$ & $0.5 \times 10^6$fs & $0.5 \times 10^6$fs & $0.5 \times 10^5$fs \\ No. of training peptides & $1$ & $200$ & $1400$ \\ No. of training pairs per peptide & $2 \times 10^5$ & $1 \times 10^4$ & $1 \times 10^4$ \\ No. of test peptides & $1$ & $100$ & $30$ \\ \end{tabular}

permutation equivariant coupling layer. If the flow only took zp as input without zv, then to maintain permutation equivariance, each coupling layer would have to unnaturally split the Cartesian components of zpi into two disjoint sets.

Translation and rotation equivariance Consider a trans- formation T = (R, a) that acts on xp as follows:

$$
\begin{array}{r}{Tx_{i}^{p}=Rx_{i}^{p}+a,\quad1\leq i\leq N,}\end{array}
$$

(14)

where R is a 3 × 3 rotation matrix, and a ∈ R3 is a translation vector. We would like the model to satisfy

We achieve pθ(Tx(t + τ)|Tx(t)) = pθ(x(t + τ)|x(t)). translation equivariance by subtracting the average position of the atoms in the initial molecular state (Appendix A.2). Rotation equivariance is not encoded in the architecture but is handled by data augmentation: each training pair (x(t), x(t+ τ)) from D is acted upon by a random rotation matrix R to form (Rx(t), Rx(t+ τ)) in each iteration.

5. Training objective

The model is trained in two stages. During likelihood train- ing, the model is trained via maximum likelihood on pairs of states from the trajectories in the dataset. During acceptance training, the model is fine-tuned to maximise the probability of MH acceptance. Let k index training pairs, such that {(x(k)(t), x(k)(t+ τ))}Kk=1 represents all pairs of states at ∑K

times τ apart in D. During likelihood training, we optimise:

$$
\begin{array}{r}{\mathcal{L}_{\mathrm{lik}}(\theta):=\frac{1}{K}\sum_{k=1}^{K}\log p_{\theta}(x^{(k)}(t+\tau)|x^{(k)}(t)).}\end{array}
$$

(15)

Once likelihood training is complete, we add a fine-tuning stage to optimise the MH acceptance probability. Let x(k)(t) be sampled uniformly from D. Then, we use the model to sample x̃(k)θ (t + τ) ∼ pθ( · |x(k)(t)) using Equation (3). Note that the sample value depends on θ through fθ. We use this to optimise the acceptance probability in Equation (7) with respect to θ. Let rθ(X, X̃) denote the model-dependent term in the acceptance ratio in Equation (7):

$$
r_{\theta}(X,{\tilde{X}}):={\frac{\mu_{\mathrm{aug}}({\tilde{X}})p_{\theta}(X\mid{\tilde{X}}^{p})}{\mu_{\mathrm{aug}}(X)p_{\theta}({\tilde{X}}\mid X^{p})}}.
$$

(16)

∑K The acceptance objective is given by:

$$
\begin{array}{r}{\mathcal{L}_{\mathrm{acc}}(\theta):=\frac{1}{K}\sum_{k=1}^{K}\log r_{\theta}(x^{(k)}(t),\tilde{x}_{\theta}^{(k)}(t+\tau)).}\end{array}
$$

(17)

Training to maximise the acceptance probability can lead to the model proposing changes that are too small: if (k) (k)(t), then all proposals will be accepted. x̃ (t+ τ) = x θ To mitigate this, during acceptance training, we use an ob- jective which is a weighted average of Lacc(θ), Llik(θ) and a Monte Carlo estimate of the average differential entropy, ∑K

$$
\begin{array}{r}{\mathcal{L}_{\mathrm{ent}}(\theta):=-\frac{1}{K}\sum_{k=1}^{K}\log p_{\theta}(\tilde{x}_{\theta}^{(k)}(t+\tau)|x^{(k)}(t))}\end{array}
$$

(18)

The weighting factors for each term are hyperparameters.

6. Experiments

We evaluate Timewarp on small peptide systems. To com- pare with MD, we focus on the slowest transitions between metastable states, as these are the most difficult to traverse. To find these, we use time-lagged independent component analysis (TICA) (Pérez-Hernández et al., 2013), a linear dimensionality reduction technique that maximises the au- tocorrelation of the transformed coordinates. The slowest components, TIC 0 and TIC 1, are of particular interest. To measure the speed-up achieved by Timewarp, we com- pute the effective sample size per second of wall-clock time (ESS/s) for the TICA components. The ESS/s is given by: ∑∞

$$
\textrm{ESS/s}=\frac{M_{\textrm{eff}}}{t_{\textrm{sampling}}}=\frac{M}{t_{\textrm{sampling}}\left(1+2\sum_{\tau=1}^{\infty}\rho_{\tau}\right)},
$$

(19)

whereM is the chain length,Meff is the effective number of samples, tsampling is the sampling wall-clock time, and ρτ is the autocorrelation for the lag time τ (Neal, 1993). The speed-up factor is defined as the ESS/s achieved by Time- warp divided by the ESS/s achieved by MD. Additional