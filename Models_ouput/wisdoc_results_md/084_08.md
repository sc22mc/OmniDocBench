by congestion and losses due to channel errors in wireless networks [35, 36] or losses caused by medium contention in optical burst switching networks [37]. While we expect that our approach would benefit from such solutions, as discussed above, the final goal of MARLIN is to train an agent that can take on the tasks of CC algorithms efficiently in a number of different network scenarios.

More interesting studies seek to apply ML models and RL agents directly to CC, by learning policies that can optimally adjust the sending rate [38], the CWND [39], or other param- eters that tune the CC algorithm [40]. Comprehensive surveys of applications of ML to CC are given in [9, 10]. Common problems that emerged from the reviewed studies include parameter selection, computational complexity, high memory consumption, low training efficiency, and compatibility and fairness against existing CC heuristics.

The possibility of training a RL agent in a completely autonomous manner makes the case for applying RL to CC optimization very compelling. The blocking nature of widely used RL libraries such as OpenAI Gym, designed to solve RL problems such as the ATARI games [41], which can be blocked while waiting for the next action, constitutes one of the main challenges. Researchers have circumvented this obstacle by training agents in simulated environments and achieved very promising results, such as with DRL-CC [42] and Aurora [43]. However, reproducing those results in real-world networks has proved to be a challenge. Solutions that rely on blocking while the agent computes the next action introduce significant delays, which are detrimental to the overall communication performance, especially in fast networks. In contrast, we designed MARLIN to integrate asynchronously with Mockets, which avoids blocking the transport protocol waiting for the agent to take the next action.

Other researchers propose the usage of non-blocking agents to optimize CC. MVFST-RL [16] proposed a non-blocking agent based on IMPALA [44], a C++ implementation of the QUIC transport protocol, and Pantheon [39] for network emulation. Communication between the agent and the system work in a similar fashion to Park [17], a platform for experi- menting with RL agents on computer system problems based on RPCs. MARLIN follows a similar philosophy to avoid the drawbacks of blocking systems. However, to the best of our knowledge, MARLIN, is the only work that uses strictly negative rewards and investigates the use of an off-policy and entropy-regularized RL algorithm, such as SAC, with a continuous action space, trained on a real network in which real background traffic flows compete for bandwidth access. Additionally, we trained MARLIN on a infinite-horizon setting and evaluated the model on a common real-world problem such as transferring a file over a shared link.

VI. CONCLUSION

This work has shown how effective policies can be ob- tained by training a RL agent based on a off-policy, entropy- regularized algorithm such as SAC. MARLIN shapes the CC problem as a strictly negative rewarded task actuating on

continuous-actions in a real network with competing dynamic background traffic.

We have also presented future research directions that we plan to pursue. These include training in more heterogeneous environments, exploring MARL settings, investigating more expressive reward functions, and designing an agent able to autonomously decide when to take the next action.

(a) Agent trained on a single traffic pattern.

(b) Agent trained on a single traffic pattern with RTT penalties.

Figure 5: Evaluation of each model on 100 testing experiments.

(c) Agent trained on every permutation of the traffic flows.