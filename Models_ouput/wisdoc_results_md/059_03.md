History Conversation 𝐻 Current Conversation Context 𝑋

Figure 2: The overall structure of the proposed HAHT model, which contains 1) hierarchical history conversa- tion encoder, 2) history-aware context encoder, and 3) history-aware response generator. The details of each component are shown in Figure 3, 4, 5, respectively.

their methods need to retrieve a very large portion of history conversations to achieve better results than the standard Transformer. In addition, these models still need to concatenate the retrieved raw history conversation text with the current conversa- tion context, yielding concatenations that are still much longer than the 128 token truncation lengths. Therefore, the incorporation of historical contexts in these methods is still limited by the short token truncation lengths of pre-trained models.

3 The Proposed Method

In general, a Multi-Session Conversation (MSC) consists of a current conversation session and sev- eral history conversation sessions that happen be- fore the current one, all between the same two in- terlocutors. A multi-session open-domain dialogue system aims to generate natural, well-informed, and context-relevant responses to the user’s utter- ances based on all history conversation sessions and the current conversation context.

Formally, we denote the MSC dataset D by a list of N conversations in the format of (H,X, y). Here, X {x1, x2, · · · , xnx} denotes nx con- text utterances of the current conversation session. H = {H1, H2, · · · , HM} denotesM history con- versation sessions, where H = {hi1, hi2, · · · , hini} denotes ni chronologically ordered utterances of the i-th history conversation session. y is the ground truth response to X under the background ofH . The MSC task can be formulated as learning a function f(H,X) to predict the next utterance xnx+1 based on H and X .

Figure 3: The structure of the hierarchical history con- versation encoder in HAHT.

In this work, we propose a novel model, namely HAHT, for the MSC task. Figure 2 shows the over- all structure of HAHT, which consists of three main components: 1) hierarchical history conversation encoder, 2) history-aware context encoder, and 3) history-aware response generator. We present the details of each component of HAHT as follows.

3.1 Hierarchical History Conversation Encoder

The main challenge in encoding history conversa- tion sessions is the limited maximum input length imposed by pre-trained dialogue systems. If all his- tory conversations are simply concatenated and fed into the pre-trained dialogue system, the length of the concatenation will exceed the maximum input length. Thus, most parts of the input will be trun- cated. To preserve more information in the history conversation, we encode each history conversation session separately in a hierarchical fashion.

Specifically, for a history conversation session H = {hi1, hi2, · · · , hini}, we first prepend a special token “User:” or “Assistant:” to each utterance hij in H depending on the role of the utterance speaker, and then pad all utterances to the same length lutter. For each utterance hij , we apply an embedding layer Em, nenc Transformer encoder layers, and a Max-pooling layer to obtain its dense representation as follows,

uij = Max-pooling Transformernenc(Em(h j)) ,

where uij ∈ Rd. Moreover, we denote all the ut- terance representations in the history conversation H by Ui = {ui1,ui2, · · · ,uini} ∈ R ni×d , where ni is the turn number of H i. Next, we apply a

(1)