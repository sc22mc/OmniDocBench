3 Algorithm

In the following, we define multi-memory versions of two major learning algorithms, i.e., replicator dynamics and gradient ascent. Although we consider the learning of player X, that of player Y can be formulated in the same manner.

Definition 1 (expected future payoff). We define the expected future payoff from the distribution p as

$$
\pi(\boldsymbol{p},\mathbf{x},\mathbf{y}):=\sum_{t=0}^{\infty}\boldsymbol{M}^{t}(\boldsymbol{p}-\boldsymbol{p}^{\mathrm{st}})\cdot\boldsymbol{u},
$$

(5)

which is the total payoff player X obtains from the present round to the future.

In this definition, the stationary payoff pst ·u = ust is the offset term every round, and thus π(pst,x,y) = 0. ∏ ∏ m m−1) Definition 2 (normalization). We define the normalization function Norm : 7→ int(∆ R + s∈S s∈S as

$$
\mathrm{Norm}(\mathbf{x})=\left\{\frac{x^{a|s}}{\sum_{a^{\prime}}x^{a^{\prime}|s}}\right\}_{a,s},
$$

(6)

In this definition, Norm(x) satisfies the condition of probability variables for all s.

Based on these definitions, we formulate discretized MMRD and MMGA as Algorithm 1 and 2.

\begin{tabular}{l} \toprule[1pt] \specialrule{0em}{1pt}{1pt} \textbf{Algorithm 1} Discretized MMRD \\ \specialrule{0em}{1pt}{1pt} \textbf{Input}: $\bar{\eta}$ \\ \specialrule{0em}{1pt}{1pt} 1: \textbf{for} $t = 0, 1, 2, \cdots$ \textbf{d}o \\ \specialrule{0em}{1pt}{1pt} 2: \quad X chooses a with probability $x^{a| s_i}$ \\ \specialrule{0em}{1pt}{1pt} 3: \quad $|$Y$ \choose b$ with probability $y^{b \: s_i}$ \\ \specialrule{0em}{1pt}{1pt} 4: \quad $s_{i'} \leftarrow a\bm{\mathbf{s}}_i^-$ \\ \specialrule{0em}{1pt}{1pt} 5: \quad $x^{a| s_i} \leftarrow x^{a| s_i} + \eta\pi(\bm{\mathbf{e}}_{i'} \cdot \bm{\mathbf{x}}, \bm{\mathbf{y}})$ \\ \specialrule{0em}{1pt}{1pt} 6: \quad $\bm{\mathbf{x}} \leftarrow \operatorname{Nor}\mbox{arn}(\bm{\mathbf{x}})$ \\ \specialrule{0em}{1pt}{1pt} 7: \quad $s_{i} \leftarrow s_{i'}$ \\ \specialrule{0em}{1pt}{1pt} 8: \textbf{eud for} \\ \bottomrule[1pt] \end{tabular}

Algorithm 1 (Discretized MMRD) takes its learning rate η as an input. In each time step, the players choose their actions following their strategies (lines 2 and 3), while the state is updated by their chosen actions (lines 4 and 7). Then, each player reinforces its strategy by how much payoff the chosen action brings up to the future. Here, note that for simplicity, this payoff is given by an expected payoff (line 5).

$$
\begin{array}{rl}&{\mathbf{Algorithm~2}~\mathrm{Discretized~MMGA}}\\ &{\underline{{\mathbf{Input}}}:~\eta,~\gamma}\\ &{~1:~\mathbf{for}~t=0,1,2,\cdots~\mathbf{do}}\\ &{~2:~~~\mathbf{for}~a\in\mathcal{A},~s\in\mathcal{S}~\mathbf{do}}\\ &{~3:~~~~~{\mathbf{x}}^{\prime}\leftarrow{\mathbf{x}}}\\ &{~4:~~~~~{x}^{\prime a|s}\leftarrow{x}^{\prime a|s}+\gamma}\\ &{~5:~~~~~\Delta^{a|s}\leftarrow\frac{u^{\mathrm{st}}(\mathrm{Norm}({\mathbf{x}}^{\prime}),{\mathbf{y}})-u^{\mathrm{st}}({\mathbf{x}},{\mathbf{y}})}{\gamma}}\\ &{~6:~~~~~\mathbf{end~for}}\\ &{~7:~~~~~\mathbf{for}~a\in\mathcal{A},~s\in\mathcal{S}~\mathbf{do}}\\ &{~8:~~~~~{x}^{a|s}\leftarrow{x}^{a|s}(1+\eta\Delta^{a|s})}\\ &{~9:~~~~~\mathbf{end~for}}\\ &{~10:~~~\mathbf{x}\leftarrow\mathrm{Norm}(\mathbf{x})}\\ &{~11:~\mathbf{end~for}}\end{array}
$$