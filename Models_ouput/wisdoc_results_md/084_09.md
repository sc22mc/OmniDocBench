REFERENCES

[12] Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. “Time Limits in Reinforcement Learning”.

[1] Josip Lorincz, Zvonimir Klarin, and Julije Ože- In: (2017). 10.48550/ARXIV.1712.00378.

DOI:

URL: gović. “A Comprehensive Overview of TCP Con- https://arxiv.org/abs/1712.00378. gestion Control in 5G Networks: Research Chal- [13] Martin L. Puterman. Markov Decision Processes: Dis- lenges and Future Perspectives”. In: 21.13

Sensors crete Stochastic Dynamic Programming. 1st. USA: John (2021). 1424-8220. 10.3390/s21134510.

ISSN:

DOI:

Wiley & Sons, Inc., 1994. ISBN: 0471619779. URL: https://www.mdpi.com/1424-8220/21/13/4510.

[14] Richard S. Sutton and Andrew G. Barto. Reinforcement

[2] Shiyao Ma, Jingjie Jiang, Wei Wang, and Bo-chen

Learning: An Introduction. Cambridge, MA, USA: A Li. “Fairness of Congestion-Based Congestion Control:

Bradford Book, 2018. ISBN: 0262039249. Experimental Evaluation and Analysis”. In: arXiv: Net- [15] Lei Zhang et al. “Reinforcement Learning Based Con- working and Internet Architecture (2017). gestion Control in a Real Environment”. In: 2020

[3] J. Widmer, R. Denda, and M. Mauve. “A survey on 29th International Conference on Computer Communi- TCP-friendly congestion control”. In: IEEE Network 2020, pp. 1–9. cations and Networks (ICCCN).

DOI: 15.3 (2001), pp. 28–37. DOI: 10.1109/65.923938. 10.1109/ICCCN49398.2020.9209750.

[4] Raffaele Galliera and Niranjan Suri. “Object Detection

[16] Viswanath Sivakumar et al. “MVFST-RL: An at the Edge: Off-the-shelf Deep Learning Capable

Asynchronous RL Framework for Congestion Devices and Accelerators”. In: Procedia Computer

Control with Delayed Actions”. In: (Oct. 2019). 205 (2022). 2022 International Conference Science

URL: http://arxiv.org/abs/1910.04054. on Military Communication and Information Systems

[17] Hongzi Mao et al. “Park: An Open Platform (ICMCIS), pp. 239–248. 1877-0509.

ISSN:

DOI: for Learning-Augmented Computer Systems”. https://doi.org/10.1016/j.procs.2022.09.025. URL:

In: Advances in Neural Information Pro- https://www.sciencedirect.com/science/article/pii/S1877050922008900.

Ed. by H. Wallach et al. cessing Systems.

[5] Nikita Rudin, David Hoeller, Philipp Reist, and

Vol. 32. Curran Associates, Inc., 2019. URL: Marco Hutter. Learning to Walk in Minutes Us- ing Massively Parallel Deep Reinforcement Learn- [18] Ubiquiti

EdgeOS. URL: 2021. 10.48550/ARXIV.2109.11978. ing. DOI:

URL: https://dl.ubnt.com/guides/edgemax/EdgeOS_UG.pdf. https://arxiv.org/abs/2109.11978.

[19] Naval Research Laboratory (NRL) PROTocol Engi- [6] OpenAI: Marcin Andrychowicz et al. “Learning neering Advanced Networking (PROTEAN) Research dexterous in-hand manipulation”. In: The International

Group. Multi-Generator (MGEN) Network Test Tool. 39.1 (2020), Journal of Robotics Research pp. 3–20. 10.1177/0278364919887447. eprint:

DOI: 2021. https://doi.org/10.1177/0278364919887447. URL:

[20] Antonin Raffin. RL Baselines3 Zoo. https://doi.org/10.1177/0278364919887447. https://github.com/DLR-RM/rl-baselines3-zoo. 2020.

[7] Tuomas Haarnoja et al. Soft Actor-Critic Algorithms and

[21] Antonin Raffin et al. “Stable-Baselines3: Reliable Rein- Applications. 2018. DOI: 10.48550/ARXIV.1812.05905. forcement Learning Implementations”. In: Journal of URL: https://arxiv.org/abs/1812.05905. 22.268 (2021), pp. 1–8. Machine Learning Research

[8] Tobias Jacob., Raffaele Galliera., Muddasar Ali., and

URL: http://jmlr.org/papers/v22/20-1364.html. Sikha Bagui. “Marine Vessel Tracking using a Monoc- [22] Adam Paszke et al. “PyTorch: An Imperative Style, ular Camera”. In: Proceedings of the 2nd International

High-Performance Deep Learning Library”. In: Pro- Conference on Deep Learning Theory and Applications ceedings of the 33rd International Conference on Neu- - DeLTA, INSTICC. SciTePress, 2021, pp. 17–28. ISBN:

Red Hook, NY, ral Information Processing Systems. 978-989-758-526-5. DOI: 10.5220/0010516000170028.

USA: Curran Associates Inc., 2019.

[9] Wenting Wei, Huaxi Gu, and Baochun Li. “Congestion

[23] Greg Brockman et al. 2016. eprint:

OpenAI Gym. Control: A Renaissance with Machine Learning”. In: arXiv:1606.01540. 35 (4 July 2021), pp. 262–269. ISSN: IEEE Network

[24] Nathan Jay, Noga Rotman, Brighten Godfrey, Michael 1558156X. DOI: 10.1109/MNET.011.2000603.

Schapira, and Aviv Tamar. “A Deep Reinforcement

[10] Huiling Jiang et al. When machine learning meets

Learning Perspective on Internet Congestion Control”. June congestion control: A survey and comparison.

In: Proceedings of the 36th International Conference 2021. DOI: 10.1016/j.comnet.2021.108033. on Machine Learning. Ed. by Kamalika Chaudhuri and

[11] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Ruslan Salakhutdinov. Vol. 97. Proceedings of Machine and Sergey Levine. “Soft Actor-Critic: Off- Learning Research. PMLR, Sept. 2019, pp. 3050–3059. Policy Maximum Entropy Deep Reinforcement

URL: https://proceedings.mlr.press/v97/jay19a.html. Learning with a Stochastic Actor”. In: CoRR

[25] Martin Thomson Jana Iyengar. QUIC: A abs/1801.01290 (2018). arXiv: 1801.01290. URL:

UDP-Based Multiplexed and Secure Trans- http://arxiv.org/abs/1801.01290. port. RFC 9000. IETF, Feb. 2022. URL: https://datatracker.ietf.org/doc/rfc9000/.