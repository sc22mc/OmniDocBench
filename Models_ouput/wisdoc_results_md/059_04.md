Figure 4: The structure of the history-aware context encoder in HAHT.

conversation aggregator Fc to aggregate all utter- ance representationsUi into the condensed history memory ci,

$$
\mathbf{c}^{i}=F_{c}(\mathbf{U}^{i}).
$$

(2)

The conversation aggregator is developed based on the following self-attentive mechanism (Lin et al., 2017),

$$
\alpha=\textrm{softmax}\big(\mathbf{W}_{k}\textrm{tanh}(\mathbf{W}_{q}\mathbf{U}^{i\top})\big),
$$

$$
F_{c}(\mathbf{U}^{i})=\alpha\mathbf{U}^{i},
$$

,

(3)

whereWq andWk are learnable parameters. α ∈ Rni is the importance vector of the history conver- sation utterances in H i.

After applying previous steps to all history con- versations H , we will finally obtain a history mem- ory matrix C ∈ RM×d containing a history mem- ory for each history conversation, whereM is the number of history conversation sessions.

3.2 History-aware Context Encoder

History conversation sessions usually contain the background stories (e.g., interlocutors’ profiles or previous discussions between them) that bring out the current conversation session. Leveraging the history conversations will help the model to better understand the current conversation context and respond properly. On the other hand, the current conversation context can help the model update the history memories. Thus, we encode the history memory C together with the current conversation context by adopting the transformer attention be- tween them.

For the current conversation context X , we also prepend a special token “User:” or “Assistant:” to each utterance depending on the role of the utter- ance speaker and concatenate all utterances into a single sentence. Then, we adopt the embedding

Figure 5: The structure of the history-aware response generator in HAHT.

layer Em to obtain a sequence of context token em- beddings S = {s1, s2, · · · , snx}, where nx is the length of the context sequence. Next, we concate- nate the history memory matrix C ∈ RM×d with S ∈ Rnx×d over the first dimension and apply nenc Transformer encoder layers.

By employing attention in the transformer en- coder layers, our model can understand the con- versation context by attending to all context token embeddings and history conversation memories. We denote this history-aware context encoding by Sc ∈ Rnx×d. After context encoding, history con- versation memories are updated based on the latest information from the current conversation context. We denote this context-updated history memory as Cs ∈ RM×d. The concatenation of Cs and Sc over the first dimension will become the input of the response generator.

3.3 History-aware Response Generator

Inspired by CopyNet (Gu et al., 2016), we con- struct two vocabularies, i.e., generic vocabulary Vg and history-aware vocabulary Vh, to better generate history-aware responses. The generic vocabulary Vg contains the words that appear in all the training dataset, and the history-aware vocabulary Vh only contain the words that appear in the history con- versations H . To generate a word of the response, the response generator will choose to generate a generic word from Vg or directly copy a word from Vh based on the switching mechanism (Gulcehre et al., 2016).

Specifically, at each decoding time step t, we feed Cs , Sc and the ground truth word sequence before t into ndec Transformer decoder layers and obtain a hidden representation vector ot ∈ Rd. The