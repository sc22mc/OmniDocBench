Read It, Don’t Watch It: Captioning Bug

Recordings Automatically

Sidong Feng†, Mulong Xie‡, Yinxing Xue§, Chunyang Chen†∗ † Monash University ‡ Australian National University § University of Science and Technology of China †{sidong.feng,chunyang.chen}@monash.edu, ‡mulong.xie@anu.edu.au, §yxxue@ustc.edu.cn Email:

Abstract—Screen recordings of mobile applications are easy to capture and include a wealth of information, making them a popular mechanism for users to inform developers of the problems encountered in the bug reports. However, watching the bug recordings and efficiently understanding the semantics of user actions can be time-consuming and tedious for developers. Inspired by the conception of the video subtitle in movie industry, we present a lightweight approach CAPdroid to caption bug recordings automatically. CAPdroid is a purely image-based and non-intrusive approach by using image processing and convolutional deep learning models to segment bug recordings, infer user action attributes, and generate subtitle descriptions. The automated experiments demonstrate the good performance of CAPdroid in inferring user actions from the recordings, and a user study confirms the usefulness of our generated step descriptions in assisting developers with bug replay.

Index Terms—bug recording, video captioning, android app

I. INTRODUCTION

Software maintenance activities are known to be generally expensive, and challenging [1] and one of the most important maintenance tasks is to handle bug reports [2]. A good bug report is detailed with clear information about what happened and the steps to reproduce the bug. However, writing such clear and concise bug reports takes time, especially for non- developer or non-tester users who do not have that expertise and are not willing to spend that much effort [3], [4]. The emergence of screen recording significantly lowers the bar for bug documenting. First, it is easy to record the screen as there are many tools available, some of which are even embedded in the operating system by default, like iOS [5] and Android [6]. Second, video recording can include more detail and context such as configurations, and parameters, bridging the understanding gap between users and developers.

Unfortunately, in many cases, watching the bug recordings and understanding the user behaviors can be time-consuming and tedious for developers [7], [8]. First, the recording may play too fast to watch, and the developers have to pause the recording, or even replay it multiple times to recognize the bug. Second, the watching experience can be further deteriorated by blurred video resolution, poor video quality, etc. Third, the recording usually contains a visual indicator (in

∗

Corresponding author

Fig. 1) to help developers identify the user actions performed on the screen. However, those indicators sometimes are too small to be conspicuously realized, and developers have to the recording back and forth to guess each action to repeat it in their testing environment.

Besides bug recordings, those issues also apply to general videos (e.g., movies, drama, etc). To address those issues in normal video watching, captions or subtitles are provided to add clarity of details, better engage users, maintain con- centration for longer periods, and translate the different lan- guages [9], [10]. Inspired by the conception of video subtitles in the movie industry, we intend to generate the caption of an app recording to add analogous benefits to developers. Given a caption accompanying the recording, developers, especially novices can more easily identify the user behaviors in the recording and shift their focus toward bug fixing. Specifically, we segment recordings into clips to characterize the “scenes” in the movie and add action descriptions of each clip to guide developers.

Existing work has investigated methods to generate a tex- tual description for GUI screenshot [11], [12], [13], [14], [15], which has been shown useful for various downstream tasks such as GUI retrieval, accessibility enhancement, code indexing, etc. Chen et al. [11] propose an image captioning model to apply semantic labels to GUI elements to improve the accessibility of mobile apps. Clarity [12] further consider multi-modal GUI sources to generate high-level descriptions for the entire GUI screen. However, none of them can generate descriptions for video recording, which is a more challenging task, translating spatial and temporal information into a se- mantic natural language.

To create good video subtitles, there are several stan- dards [16], including caption synchronization with the videos, accurate content comprehension, compact and consistent word usage, etc. Similarity, we propose an image-based approach CAPdroid in this paper to non-intrusively caption each action step for a bug recording, including three phases: 1) action segmentation, 2) action attribute inference, and 3) description generation. Inspired by the previous work GIFdroid [4], [17] to localize keyframes in bug recording, we first develop a heuristic method to segment the recording into a sequence of (a) Default (b) Cursor (c) Custom

arXiv:2302.00886v1 [cs.SE] 2 Feb 2023