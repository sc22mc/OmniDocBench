Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics

coupling layer. Middle: A single atom transformer module. Right: the multihead kernel self-attention module.

Figure 2. Schematic illustration of the Timewarp conditional flow architecture, described in Section 4. Left: A single conditional RealNVP

not use a positional encoding. Second, instead of dot prod- uct attention, we use a simple kernel self-attention module, which we describe next.

Kernel self-attention We motivate the kernel self- attention module with the observation that physical forces acting on the atoms in a molecule are local: i.e., they act more strongly on nearby atoms. Intuitively, for values of τ that are not too large, the positions at time t+τ will be more influenced by atoms that are nearby at time t, compared to atoms that are far away. Thus, we define the attention weight wij for atom i attending to atom j as follows:

$$
w_{ij}=\frac{\exp(-\|x_{i}^{p}-x_{j}^{p}\|_{2}^{2}/\ell^{2})}{\sum_{j^{\prime}=1}^{N}\exp(-\|x_{i}^{p}-x_{j^{\prime}}^{p}\|_{2}^{2}/\ell^{2})},
$$

(11)

where ` is a lengthscale hyperparameter. The output vectors ∑N {rout,i}Ni=1, given the input vectors {rin,i}Ni=1, are then:

$$
\begin{array}{r}{r_{\mathrm{out},i}=\sum_{j=1}^{N}w_{ij}V\cdot r_{\mathrm{in},j},}\end{array}
$$

(12)

where V ∈ Rdout×din is a learnable matrix. This kernel self-attention is an instance of the RBF kernel attention investigated in Tsai et al. (2019). Similarly to Vaswani et al. (2017), we introduce a multihead version of kernel self-attention, where each head has a different lengthscale. This is illustrated in Figure 2, Right. We found that ker- nel self-attention was significantly faster to compute than dot product attention, and produced similar or improved performance.

4.1. Symmetries

The MD dynamics respects certain physical symmetries that would be advantageous to incorporate. We now describe how each of these symmetries is incorporated in Timewarp.

Permutation equivariance Let σ be a permutation of the N atoms. Since the atoms have no intrinsic ordering, the only effect of a permutation of x(t) on the future state x(t+ τ) is to permute the atoms similarly, i.e.,

$$
\begin{array}{r}{\mu(\sigma x(t+\tau)|\sigma x(t))=\mu(x(t+\tau)|x(t)).}\end{array}
$$

(13)

Our conditional flow satisfies permutation equivariance ex- actly. To show this, we use the following proposition proved in Appendix A.1, which is an extension of Köhler et al. (2020); Rezende et al. (2019) for conditional flows:

Proposition 4.1. Let σ be a symmetry action, and let f(·; ·) be an equivariant map such that f(σz;σx) = σf(z;x) for all σ, z, x. Further, let the base distribution p(z) satisfy p(σz) = p(z) for all σ, z. Then the conditional flow defined by z ∼ p(z), x(t + τ) := f(z;x(t)) satisfies p(σx(t + τ)|σx(t)) = p(x(t+ τ)|x(t)).

Our flow satisfies fθ(σz;σx(t)) = σfθ(z;x(t)), since the transformer is permutation equivariant, and permuting z and x(t) together permutes the inputs. Furthermore, the base distribution p(z) = N (0, I) is permutation invariant. Note that the auxiliary variables allow us to easily construct a

scale scale