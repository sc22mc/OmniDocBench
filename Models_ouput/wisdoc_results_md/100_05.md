Published as a conference paper at ICLR 2024

Simple Prompt. For any task, in iteration we create a simple initial prompt t = 0, I0

, where S represents task-agnostic descriptions while the terms X and Q S,X,Q,F0, {Gi}

respectively denote the task information and the question. The experience part of the prompt is denoted as F0, which should be empty at the beginning. {Gi} is a placeholder that is waiting to be filled during building thought structures. In other words, when generating the next thought zi, {Gi} will be substituted with the preceding chain of thoughts z1..,i−1.

Thought Structures Generation. After collecting experience Ft−1, the prompt in the iteration t can be It

. Based on this prompt, BoT generates M thought structures S,X,Q,F1,...,t−1, {Gi}

in parallel. BoT is inherently capable of embracing any thought structure, such as the chain Wei et al. (2022) or tree Yao et al. (2024) structure. Considering the exploration of reasoning steps and experimental results, we investigate the tree thought structure. However, BoT introduces two novel modifications to make it better suited for the boosting framework.

• Weighted Binary Tree. With a simple prompt in each round, BoT builds the weak thoughts structured in low complexity as they can be further revised in the boosting mechanism.

Thus, each thought structure of BoT is a shallow weighted binary tree. For simplicity, we retain the notation z1...i−1 to represent the thoughts from the root to the parent of node i. In addition to providing each node i with one thought zi and its thought evaluation score Vi ∼ pθ (z1...i, Ia, X,Q), we incorporate the edge score Vi−1,i ∼ pθ (zi−1, zi, Ie, X,Q) between a child node and its parent node, where Ia and Ie refer to the instructional descriptions for thought and edge evaluations. Vi−1,i represents the LLMs’ confidence level in generating this reasoning step. Thus, the next thought generation of BoT in this tree structure is formalized as pθ (zi| (Vi−1,i, Vi, It, X,Q)).

• Tree Heterogeneity. Unlike ToT Yao et al. (2024), which seeks to search for a solution in one large and complex tree, BoT aims to build highly heterogeneous tree thought structures.

As a result, complete reasoning chains with various logical in trees of BoT are subsequently assessed as experience. Therefore, to increase heterogeneity, thought structure generation embraces different tree growth strategies, such as level-wise growth and leaf-wise growth. The former emphasizes exploration but less exploitation Chen & Guestrin (2016), while the latter does the opposite Ke et al. (2017). Thus, the leaf-wise strategy tends to continue reasoning from the current best thought to reach a better final thought as compared to level-wise growth, but it also tends to get monotonous reasoning chains. Besides, different temperature and Top p settings of LLMs are applied. Finally, we use a small max depth value in BoT and label a node as a leaf when its Vi−1,i and Vi values are outside the specified range [0.3, 0.8].

Thought Structures Aggregation. Upon obtainingM thought structures, BoT aggregates them into ∑n one thought chain denoted as z1...n. To achieve this, for each thought structure with indexm, BoT first selects the chain with the highest evaluation score as zm1...nm := argmaxz1...n∈Zm Vi + Vi−1,i i=1 where Zm denotes the set of all thought chains of m-th tree. Subsequently, two strategies exist to obtain z1...n.

∑n

• Best-First Aggregation. BoT relies on argmaxz1...n∈{Zm}Mm=1 Vi+Vi−1,i to choose i=1

the best one as z1...n fromM thought structures. This algorithm is fast but may lead to an unreasonable chain that is hard to guide the following refinement.

• Greedy Aggregation. BoT is allowed to perform a greedy search on {Zm}Mm=1 to assemble a new thought chain that may not exist and is globally optimal. Starting from the initial thought, generally the root node of the tree, BoT obtains z1 = argmaxzj∈{zm1 Vj + }Mm=1 Vj−1,j . Subsequently, to obtain zi for zi−1, BoT searches all thoughts where the previous step is zi−1 in {Zm}Mm=1.

Thought Chain Analysis. To gain insights into what should be adjusted to enhance the prompt to generate better thoughts, BoT utilizes the self-evaluation ability Weng et al. (2023) of LLMs to assess z1...n. Specifically, with the prompt Itf (z1...n, X,Q) as input, LLM outputs a feedback paragraph containing issues report of this thought chain z1...n and detailed advice. This feedback will be added to F1,...,t−1 as a new experience in thought generation, resulting F1,...,t.