Fig. 1: Examples of touch indicators.

action clips (i.e., TAP, SCROLL, INPUT). Then, we adopt image-processing and deep-learning methods to model the spatial and temporal features across frames in the clips to infer action attributes, such as touch location, moving offset, and input text. A simple description based on the action

attribute, e.g. tap on (x,y) coordinate, cannot express the action intuitively. Therefore, we first utilize off-the-shelf GUI models to non-intrusively gather the elements information in the GUI. As the GUI elements of interest may not have enough context to be uniquely identified, we propose a novel algorithm using global information of GUI elements to generate high-level semantic descriptions.

We first evaluate the performance of the CAPdroid in obtaining user actions by action segmentation and action attribute inference, through an automated method. We collect 439 Android apps from Google Play and leverage an auto- mated app explore tool to simulate user actions on the screen, meanwhile capturing a 10-min screen recording for each app. Results show that our tool achieves the best performance (0.84 Video F1-score and 0.93 accuracy) in action segmenta- tion from the recordings compared with five commonly-used baselines. CAPdroid also achieves on average 91.46% in inferring action attributes, outperforming two state-of-the-art baselines. We further carry out a user study to evaluate the use- fulness of description generation of CAPdroid in assisting bug replay, with 10 real-world bug recordings from GitHub. Results show that participants save 59.8% time reproducing the bug with the help of the steps we described, compared with the steps written by users. Through questionnaires with participants, they also confirm the clearness, conciseness, and usefulness of our generated action descriptions.

The contributions of this paper are as follows:

This is the first work to generate the caption of bug

• recordings to support developers in reproducing bugs. The first systematic approach CAPdroid, to non- • intrusively segment recordings into clips, infer fine- grained user actions, and create action descriptions as subtitles, with examples in online appendix1.

A comprehensive evaluation including automated exper- •

iments and a user study to demonstrate the accuracy and usefulness of our approach.

II. CAPDROID APPROACH

Given an input GUI recording, we propose an automated approach to segment the recording into a sequence of clips

1https://github.com/sidongfeng/CAPdroid

based on user actions and subsequently localize the action po- sitions to generate natural language descriptions. The overview of our approach is shown in Fig. 2, which is divided into three main phases: (i) the Action Segmentation phase, which segments user actions from GUI recording into a sequence of clips, (ii) the Action Attribute Inference phase that infers touch location, moving offset, and input text from action clips, and (iii) the Description Generation phase that utilizes the off-the-shelf GUI understanding models to generate high-level semantic descriptions. Before discussing each phase in detail, we discuss some preliminary understanding of user actions in GUI recording.

A. Preliminary Study

To understand the recordings from the end-users, we con- ducted a small pilot study of the GUI recordings from GitHub [18]. In detail, we built a crawler to automatically crawl the bug reports from GitHub issue repositories that contain GUI recordings with suffix names like .gif, .mp4, etc. To study more recent GUI recordings, we obtained the record- ings from 2021. Overall, we obtained 5,231 GUI recordings from 1,274 apps. We randomly sampled 1,000 (11.5%) GUI recordings, and we recruited two annotators online to manually check the user actions from the recordings.

Two students were recruited by the university’s internal slack channel and they were compensated with $12 USD per hour. They have annotating experience on GUI-related (e.g., GUI element bounding box) and video-related (e.g., video classification) datasets. To ensure accurate annotations, the process started with initial training. First, we gave them an introduction to our study and also an example set of annotated screen recordings where the labels have been annotated by the authors. Then, we asked them to pass an assessment test. Two annotators were assigned the experimental set of screen recordings to label the user actions independently without any discussion. After the initial labeling, the annotators met and sanity corrected the subtle discrepancies. Any disagreement was handed over to the first author for the final decision.

We observed that 89% of the recordings included a touch indicator, indicating it as a mechanism for the end-user to depict their actions on the screen. We further classified those touch indicators into three categories, following the Card Sorting [19] method:

default (68%). As shown in Fig. 1(a), the touch indicator

• renders a small semi-transparent circle, that gives visual feedback when the user presses his finger on the device screen. This is the default touch indicator on Android. cursor (27%). As shown in Fig. 1(b), users/developers

•

may test the apps in the emulator and directly record the desktop, so that the user actions are captured by the desktop cursor.

custom (5%). As shown in Fig. 1(c), the touch indicator

• is customized by third-party screen recorders, such as DU Recorder [20], etc.

Those findings motivated us to develop a tailored approach, exploiting touch indicators to capture end-user intent, so to