\begin{tabular}{c|l|rrrr|rrrr|rrrr} \toprule \multirow{2}{*}{\textbf{Model Type}} & \multirow{2}{*}{\textbf{Model}} & \multicolumn{4}{c|}{\textbf{Language}} & \multicolumn{4}{c|}{\textbf{Test background}} & \multicolumn{4}{c}{\textbf{Text Rotate}} \\ & & \textit{EV} & \textit{ZI} & \textit{Mixed} & \textit{White} & \textit{Single} & \textit{Multi} & \textit{Normal} & \textit{Rotate90} & \textit{Rotate270} & \textit{Horizontal} & \textit{Vertical} \\ \midrule \multirow{6}{*}{\shortstack{ \textbf{Expert Vision} \\ \textbf{Models} }} & PatilieOCR{\tiny{[\textcolor{gray}{23}]}} & 0.071 & \textbf{0.055} & \textbf{0.118} & \textbf{0.066} & \textbf{0.038} & \textbf{0.085} & \textbf{0.060} & \textbf{0.015} & \underline{0.726} & \textbf{0.021} & \textbf{0.176} \\ & TcscryptOCR{\tiny{[\textcolor{gray}{33}]}} & 0.179 & 0.553 & 0.553 & 0.453 & 0.453 & 0.354 & 0.448 & 0.369 & 0.979 & 0.982 & 0.82 \\ & Sarya{\tiny{[\textcolor{gray}{37}]}} & 0.057 & 0.123 & 0.164 & 0.095 & 0.186 & 0.235 & 0.104 & 0.634 & 0.767 & 0.255 & 0.391 \\ & GOT-OCR{\tiny{[\textcolor{gray}{45}]}} & 0.064 & \underline{0.112} & 0.135 & 0.052 & \underline{0.057} & 0.155 & \underline{0.651} & 0.563 & \textbf{0.966} & 0.973 & 0.852 \\ & Mahep{\tiny{[\textcolor{gray}{44}]}} & 0.033 & 0.240 & 0.261 & 0.185 & 0.121 & 0.166 & 0.18C & 0.038 & \textbf{0.185} & 0.638 & 0.74 \\ \midrule \multirow{4}{*}{\shortstack{ \textbf{Vision Language} \\ \textbf{Models} }} & Qwen2 VL 72B{\tiny{[\textcolor{gray}{14}]}} & 0.072 & 0.27\textcolor{gray}{$^{\pm}$} & 0.286 & 0.224 & 0.155 & \underline{0.148} & 0.223 & 0.273 & 0.721 & \underline{0.067} & 0.251 \\ & InterML{\tiny{[\textcolor{gray}{21}]}} & 0.074 & 0.155 & 0.242 & 0.113 & 0.352 & 0.269 & 0.132 & 0.161 & 0.107 & 0.107 & 0.193 \\ & GIFT4{\tiny{[\textcolor{gray}{2}]}} & \textbf{0.020} & 0.22\textcolor{gray}{$^{\pm}$} & \underline{0.125} & 0.167 & 0.140 & 0.220 & 0.168 & 0.115 & 0.718 & 0.132 & 0.257 \\ \midrule \end{tabular}

Table 9. Component-level formula recognition evaluation on Om- niDocBench formula subset.

\begin{tabular}{l|cccc} \textbf{Models} & \textbf{CDM} & \textbf{ExpRate$@$CDM} & \textbf{BLEU} & \textbf{Norm Edit} \\ [0.5ex] \hline \\[-0.8em] GOT-OCR$[45]$ & 74.1 & 28.0 & 55.07 & 0.290 \\ Mathpix $^{i}$ & \underline{86.6} & 2.8 & \textbf{66.56} & 0.322 \\ Pix2Texe$^{j}$ & 73.9 & 39.5 & 46.00 & 0.337 \\ UniMERNet-B $[40]$ & 85.0 & \underline{60.2} & \underline{60.84} & \textbf{0.238} \\ \hline \\[-0.8em] GPT4o $[2]$ & \textbf{86.8} & \textbf{65.5} & 45.17 & \underline{0.282}\\ IntervVL2-76B $[8]$ & 67.4 & 54.5 & 47.63 & 0.308\\ Qwen2-VL-72B $[44]$ & 83.8 & 55.4 & 53.71 & 0.285 \\ \end{tabular}

ever, struggle with high-density documents like newspapers due to limitations in input resolution and token length. In contrast, pipeline tools leverage layout-based segmentation to process components individually, maintaining accuracy in complex layouts. Enhancing VLMs with layout-aware designs and domain-specific fine-tuning offers a promising path forward. OmniDocBench facilitates this by provid- ing detailed annotations for layout, text, formulas, and ta- bles, enabling comprehensive benchmarking and modular tool development for diverse document parsing tasks.

5.2. Single Task Evaluation Results

Layout Detection Results. Layout detection is the first step in document parsing using pipeline tools. A robust layout detection algorithm should perform well across a variety of document types. Table 6 presents an evaluation of leading layout detection models. The DocLayout-YOLO method, which is pre-trained on diverse synthetic document data, significantly outperforms other approaches. This superior- ity is a key factor in MinerUâ€™s integration of DocLayout- YOLO, contributing to its outstanding overall performance. Other methods perform well on books and academic litera- ture but struggle with more diverse formats due to limited training data.

Table Recognition Results. In Table 7, We evaluate ta- ble recognition models across three dimensions on our Om- niDocBench table subset: language diversity, table frame types, and special situations. Among all models, OCR- based models demonstrate superior overall performance, with RapidTable achieving the highest scores in language diversity and maintaining stable performance across differ- ent frame types. Expert VLMs show competitive results in specific scenarios, with StructEqTable [55] excelling in no- frame tables and showing better rotation robustness. Gen- eral VLMs (Qwen2-VL-7B and InternVL2-8B) exhibit rel- atively lower but consistent performance, suggesting that while general-purpose VLMs have made progress in table understanding, they still lag behind specialized solutions. Text Recognition Results. Table 8 compares OCR tools across languages, backgrounds, and rotations using Edit Distance. PaddleOCR outperforms all competitors, fol- lowed by GOT-OCR and Mathpix. General VLMs struggle to handle text rotation or mixed-language scenarios.

Formula Recognition Results. Table 9 presents results on formula parsing, using CDM, BLEU, and normalized Edit Distance. GPT-4o, Mathpix, and UniMERNet achieve results of 86.8%, 86.6%, and 85.0%, respectively. No- tably, GPT-4o excels with a recall rate of 65.5% under strict conditions requiring perfect character accuracy. Although Mathpix shows high character-level precision, it occasion- ally omits punctuation, such as commas, leading to a lower overall correctness rate. Nonetheless, all three models are strong candidates for formula recognition tasks.

6. Conclusion

This paper addresses the lack of diverse and realistic bench- marks in document parsing research by introducing Om- niDocBench, a dataset featuring a variety of page types with comprehensive annotations, along with a flexible and reli- able evaluation framework. OmniDocBench enables sys- tematic and fair assessments of document parsing meth- ods, providing crucial insights for advancing the field. Its task-specific and attribute-level evaluations facilitate tar- geted model optimization, promoting more robust and ef- fective parsing solutions.

7. Acknowledgments

This project was supported by National Key R&D Program of China (NO.2022ZD0160102) and Shanghai Artificial In- telligence Laboratory.

24845