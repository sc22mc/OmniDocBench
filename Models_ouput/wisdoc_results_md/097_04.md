between an event stack (Eq. (4)) and the two consecutive coded-aperture images (Eq. (1)) as

$$
E_{x,y}^{(n,n+1)}\approx\frac{\log\left(I_{x,y}^{(n+1)}\right)-\log\left(I_{x,y}^{(n)}\right)}{\tau}.
$$

(6)

Assuming that equality holds for Eq. (6) and combining it with Eq. (3), we can obtain I(1), . . . , I(N) as ∑

$$
I_{x,y}^{(n)}=I_{x,y}^{(1)}\exp\left(\tau\sum_{2\leq k\leq n}E_{x,y}^{(k-1,k)}\right)(n\geq2)
$$

(7)

) .

$$
I_{x,y}^{(1)}=\frac{I_{x,y}}{1+\sum_{2\le n\le N}\exp\Big(\tau\sum_{2\le k\le n}E_{x,y}^{(k-1,k)}\Big)}.
$$

(8)

This means that under the equality assumption for Eq. (6), N coded-aperture images (I(1), . . . , I(N)) can be derived analytically from the data observed with our imaging method (Ī and E(1,2), . . . , E(N−1,N)). Therefore, we can state that our imaging method is quasi-equivalent to the baseline coded-aperture imaging method.

Interestingly, this type of quasi-equivalence does not hold for joint aperture-exposure coding. By using Eq. (1), the imaging process of Eq. (2) is rewritten as ∑

$$
I_{x,y}=\sum_{n}p_{x,y}^{(n)}\,I_{x,y}^{(n)}.
$$

(9)

Obviously, it is impossible to analytically obtain N coded- aperture images (I(1), . . . , I(N)) from a single observed im- age I alone. Therefore, our imaging method has a theoret- ical advantage over joint aperture-exposure coding. How- ever, this theory alone is insufficient to ensure the practi- cality of our method. The actual event data are very noisy and harshly quantized (by the contrast threshold τ ), which breaks the equality assumption for Eq. (6).

3.3. Algorithm

We developed an end-to-end trainable algorithm on the ba- sis of deep-optics [13, 14, 21, 28, 33, 48, 52, 54], in which the camera-side optical-coding patterns and the light-field reconstruction algorithm were jointly optimized in a deep- learning-based framework. We carefully designed each part of our algorithm to ensure the compatibility with real cam- era hardware. Although we specifically mention the hard- ware setup that is available to us, the ideas behind our de- sign would be useful for other possible hardware setups. We set N = 4 unless otherwise mentioned.

Our algorithm consists of two parts: AcqNet and Rec- Net. AcqNet describes the data-acquisition process using a coded aperture and an event camera as

$$
\bar{I},E^{(1,2)},E^{(2,3)},E^{(3,4)}=\textup{AcqNet}(L).
$$

(10)

The trainable parameters of AcqNet are related to the aper- ture’s coding patterns. RecNet receives the observed data as

Algorithm 1 Pseudo-code for AcqNet

\begin{tabular}{l} 1: \textnormal{trainable tensors:} $\alpha, \beta \in \mathcal{R}^{8 \times 8}$ \\ 2: \textnormal{forward($L$)}: \\ 3: \quad set $s$ \\ 4: \quad $a^{(1)}$, $a^{(3)}=\textnormal{sigmoid}(s\alpha), \textnormal{sigmoid}(s\beta)$ \\ 5: \quad $a^{(2)}$, $a^{(4)} = 1 - a^{(1)}$, $1 - a^{(3)}$\\ 6: \quad \textbf{for} $n$ \textnormal{in} {[}1, 2, 3, 4{]}: \\ 7: \quad \quad \textnormal{compute} $I^{(n)}$ \textnormal{by Eq.~(\textbf{1})} \\ 8: \quad \quad \textbf{if} $n > 1$: \\ 9: \quad \quad \quad \textnormal{compute} $E^{(n - 1, n)}$ \textnormal{by Eq.~(\textbf{12})}\\ 10: \quad \quad \textbf{end} \\ 11: \quad \textbf{end} \\ 12: \quad \textnormal{compute} $\bar{I}$ \textnormal{by Eq.~(\textbf{3})} \\ 13: \quad \textnormal{return} $\bar{I}, E^{(1, 2)}, E^{(2, 3)}, E^{(3, 4)}$ \end{tabular}

the input and reconstructs the original light field as

$$
\hat{L}=\textrm{RecNet}(\bar{I},E^{(1,2)},E^{(2,3)},E^{(3,4)}).
$$

(11)

AcqNet and RecNet are jointly trained to minimize the re- construction (MSE) loss between L and L̂. Once the train- ing is finished, AcqNet is replaced with the physical imag- ing process of the camera hardware, in which the coding patterns are adjusted to the learned parameters of AcqNet. The data acquired from the camera are fed to RecNet to re- construct the light field of a real 3-D scene.

Hardware-driven constraints for coded aperture. Similar to some previous studies [14, 26, 30], we used a liquid-crystal-on-silicon (LCoS) display (Forth Dimen- sion Displays, SXGA-3DM) to implement a coded aperture. This display can output a sequence of semi-transparent cod- ing patterns repeatedly. We need to consider the following two constraints. Binary constraint. Although our LCoS display can support both binary and grayscale patterns, a grayscale pattern is actually represented as a temporal se- ries of multiple binary patterns; a grayscale transmittance is represented as the ratio of 0/1 periods. To avoid unintended 0/1 flips, we choose to use only binary patterns for aperture coding. Complementary constraint. Our LCoS display requires a “DC balance”; a certain pattern a and its comple- ment a∗ = 1− a should be included in the sequence. Since the events are recorded continuously over time, we use both a and a∗ as the coding patterns.

AcqNet. A pseudo-code of AcqNet is presented in Al- gorithm 1. In lines 4 and 5, we make the coding patterns compatible with the binary and complementary constraints. More specifically, we prepare two sets of trainable tensors, each with 8 × 8 elements, denoted as α and β. They are multiplied by the scale parameter s then fed to the sigmoid function to produce the coding patterns a(1) and a(3). As the training proceeds, s gradually increases, which forces a(1) and a(3) to gradually converge to binary patterns. More- over, a(2) and a(4) are made to be complementary to a(1)

24926