Figure 6. Visual result tested on Origami light field. Top: reconstructed top-left views with epipolar plane images along blue/green lines.

Bottom: view-by-view reconstruction quality in PSNR.

Figure 7. Experiment with our prototype camera. Coding patterns and experimental setup (left), image frame and event stacks obtained from camera (center), and reconstructed light field view (at top-left viewpoint) with epipolar plane images along blue/green lines (right).

4.2. Real-World Experiment

To capture real 3-D scenes, we used our prototype camera and flexible-Ï„ model. The aperture-coding patterns were set to the corresponding parameters in the learned AcqNet model. Figure 7 shows the coding patterns and experimen- tal setup (left), an image frame and event stacks obtained from the camera (center), and the light field reconstructed from them (right). Our method performed well with real 3-D scenes; the parallax information was successfully em- bedded into the events, and a light field was reconstructed with visually convincing quality. Please refer to the supple- mentary video for more results with better visualization.

5. Conclusion

We proposed a new light-field acquisition method that takes advantage of coded-aperture imaging and an event camera. Although the measurement on the camera can be completed in a single exposure, our method is theoretically quasi- equivalent to the baseline coded-aperture imaging method.

We carefully designed our algorithm pipeline to be end-to- end trainable and compatible with real camera hardware, which enabled both accurate light-field reconstruction and successful development of our hardware prototype.

For our future work, we will explore the design space

with respect to, e.g., the number of coding patterns dur- ing an exposure, while considering the hardware restric- tions. Seeking better hardware implementation to achieve RGB color reconstruction and higher spatial resolutions is also an important avenue. We will also extend our method to moving scenes (light field videos), which would ben- efit from the time-efficiency of our method. Finally, our method would be extended to a more event-centered direc- tion, where event streams over the continuous time (instead of discretized event stacks) could be fully utilized and light fields could be reconstructed from events alone.

Acknowledgement: This work was supported by JSPS Grant-in-Aid for Scientific Research (B) 22H03611 and NICT contract research JPJ012368C06801.

24930