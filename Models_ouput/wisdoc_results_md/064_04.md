QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning

To simplify the illustration and better represent our model in the following sections:

$$
[\scriptstyle{\mathcal{CLS}}]_{l}^{\prime}\leftarrow{\textrm{Enc}}_{l}([\scriptstyle{\hat{\mathcal{CLS}}}]_{l}^{\prime})\;{\textrm{and}}\;[\scriptstyle{\mathcal{CLS}}]^{l}\leftarrow{\textrm{Enc}}_{l}([\scriptstyle{\hat{\mathcal{CLS}}}]^{l}),
$$

(2)

t), by default, when talking about [CLS] ([CLS]vi and [CLS] they are the CLIP’s output embeddings rather than the input ˆ[CLS]), token ( the same as shown in Fig. 2.

However, the output embedding [CLS] cannot adequately represent an image as the single embedding provides lim- ited cues to the location and time reasoning. Therefore, we consider enlarging the representations for an image. It is evi- dent that in real life, we would get different ideas about what an image means from different individuals. Following in this vein, we propose a simple yet effective methods: in our technical implementation, we are inspired by MVR (Zhang et al., 2022) and introduce additional [CLS]vi (i= 1, ...,n) to replace the original single [CLS] representation. Through the observation of ablations, we finally use 6 different ˆ

[CLS]vi at the beginning of the image patch token embed- patch ˆ ˆ dings (Î = Îpatch1 ), like ( , ..., Î [CLS]v1...

[CLS]v6 Î), and

after going through the encoder Encv, we get a list of embed- dings ([CLS]v1...[CLS] I). Using this design, the pre-trained model can investigate an image from multiple perspectives and dimensions. It follows the Q-principle and increases the quantity of information from multiple perspectives.

Since the text contains explicit semantic information and most language inputs carry clear messages, we only use the original [CLS]t at the beginning of the text token embedding (T), like ([CLS]t T ). Hence, we search for corresponding information through the image from the CLIP model by conducting:

$$
([\tt CLS]^{\tt I})^{\tt I}\cdot([\tt CLS]^{\tt I}).
$$

(3)

Then, the global loss further constrains the correspondence between image features and location/time features, and the calculation method is as follows:

$$
\textit{L}_{global}=-\textup{log}\frac{\boldsymbol{e}^{f_{max}(q_{l},k_{l+})}}{\boldsymbol{\sum}_{l}^{\prime\prime}[\boldsymbol{e}^{f_{max}(q_{l},k_{l+})}+\boldsymbol{e}^{f_{max}(q_{l},k_{l-})}]},
$$

(5)

{ fi(qv,kt)}, max {} represents the where fmax(qv,kt) =max

maximum value. The entire training loss is defined as a linear combination of two losses as Ltotal = Llocal+Lglobal .

Open-World Knowledge Search. After fine-tuning, each

[CLS]vi output by CLIP-V can represent image location/time information from various perspectives. We use these dif- ferent representations to retrieve more valuable open-world knowledge from the OWK dataset (Sec 4.1) to increase the quantity of knowledge.

Given an image I and the corresponding Open-World Knowl- owk1 owk owk edge (O = T ,T , ...,T ,k = 122,408), the process of searching follows Eq. 3: each [CLS]vi calculates the sim- ilarity with 122,408 candidate Wikipedia corpus (OWK). Here, we select the candidate Wikipedia with the top-1 sim- ilarity for each [CLS]vi , yielding a total of 6 OWKs. After that, the Quantity module (sec 3.2) finished its job by col- lecting a list of highly-related OWKs items that the next Relevance module (sec 3.3) would use as input for CLIP-T.

3.3. Relevance Module

Scoring Mechanism. The amount of useful information varies for each [CLS]vi of an image and the corresponding embeddings of open-world knowledge. As a result, it is critical to weigh the importance of different features dynam- ically. Based on the above motivation, we propose a scoring mechanism to further highlight the relevant features.

In the following fine-tuning or searching for open-world knowledge, each [CLS]vi of the Encv calculates the similarity with [CLS]t of the candidate information by inner-product.

Location/Time Fine-tune. We first initialize and position- encode each [CLS]vi individually, aiming to extend the dis- tance between each [CLS]vi . Then, we fine-tune CLIP with local and global losses (He et al., 2020; Zhang et al., 2022) to ensure each [CLS]vi is aligned with the location and time linguistic features [CLS]t .

For the local loss, the correspondence between each [CLS]vi and [CLS]t is achieved by a contrastive learning loss:

$$
\begin{array}{r}{L_{local}=-\frac{1}{i+1}\sum_{0}^{i}\mathop{\textrm{log}}\frac{\boldsymbol{e}^{f_{l}(q_{l},k_{l+})}}{\sum_{l}^{\prime}[\boldsymbol{e}^{f_{l}(q_{l},k_{l+})}+\boldsymbol{e}^{f_{l}(q_{l},k_{l-})}]},}\end{array}
$$

(4)

here, qv denote the query image embedding ([CLS]vi ); kt+ and kt− are the positive and negative key text embeddings (a

We adopt two layers of MLP (MLP2−layer) as our relevance scoring component and find it helpful:

$$
\scriptstyle W^{\mathrm{v}}=\mathbf{MLP}_{2-layer}\left([\mathbf{CLS}]_{1}^{x}\right),
$$

Here, [CLS]xi is the input embedding, and W is the cal- culated weight. We use contrastive learning to optimize the model. To facilitate implementation, we directly adopt the loss functions from the first step of the Quantity mod- ule (sec 3.2). In this case, we keep the CLIP-T and CLIP-V frozen and only update the parameters of the relevance scor- ing component.

In the local loss, the information of two features is integrated tto optimize the scoring mechanism jointly:

(6)

$$
\begin{array}{r}{\textbf{\textit{$f_{i} (q,k_{+})=(W_{l}^{owk}\times[{\CLS}]_{l}^{owk}+W_{l}^{\mathrm{vv}}\times[{\CLS}]_{l}^{\mathrm{vv}})$},}\end{array}
$$

(7)