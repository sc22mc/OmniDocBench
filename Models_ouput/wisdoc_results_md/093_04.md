ALMSTEDT et al.: BEYOND THE NAKED EYE: COMPUTER VISION FOR DETECTING BROWN MARMORATED STINK BUG AND ITS PUNCTURES

TABLE I

PERFORMANCE INDICATORS USED

\begin{tabular}{lll} Indicator & Formula & Section \\[0.1cm] Precision ($P$) & $\frac{\textrm{TP}}{\textrm{TP}+\textrm{FP}}$ & III,IV,V \\[0.15cm] Recall--sensitivity ($R$) & $\frac{\textrm{TP}}{\textrm{TP}+\textrm{FN}}$ & III,IV,V \\[0.15cm] Specificity & $\frac{\textrm{TN}}{\textrm{TN}+\textrm{FP}}$ & IV \\[0.15cm] Accuracy & $\frac{\textrm{TP}+\textrm{TN}}{\textrm{TP}+\textrm{TN}+\textrm{FP}+\textrm{FN}}$ & IV,VII \\[0.15cm] F1 score & $\frac{2\times P \times R}{P + R}$ & IV \\[0.15cm] Mean average precision ($m$AP) & $\frac{1}{N} \sum_{k=1}^{N} \textrm{AP}_{k}$ & III \\[0.15cm] Intersection over union (IoU) & $\frac{|A \cap B|}{|A \cup B|}$ & III \\[0.15cm] \end{tabular}

of objects in images and is more accurate than many other detectors [28], [31], it tends to lose specimens (i.e., high value of FN) and also to have a not negligible number of FP.When testing SSD on ALL and ONLY, its precision score remains stable. The number of FP slightly increases in the ALL experiment, but not dramatically.

The DETR leverages the transformer architecture, originally designed for natural language processing tasks. It has emerged as a novel NN for object detection, comprising four main com- ponents: backbone, encoder, decoder, and prediction heads [32]. In our experiments, we employed ResNet-50 as the backbone, using a CNN to learn a 2-D representation of the input image. The model flattens this representation and supplements it with positional encoding before feeding it into a transformer encoder. Subsequently, the encoded image data pass through an encoder– decoder structure and is then directed to the prediction heads. These prediction heads, based on feed-forward networks, predict either a detection class and bounding box, or a NOOBJECT class (i.e., background). We utilized the PyTorch implementation of DETR in our application, and the testing results are presented in Table II. As indicated, the recall is poor, resulting in a high number of FN. However, since it achieves the same P for ALL and ONLY, it follows model robust against empty patches.

YOLOV5 stands out for its lightweight and fast computation capabilities, demanding less computational power compared with other current state-of-the-art algorithms while maintaining comparable performance [16]. Although YOLOV5 is offered in several model sizes, in the following, we rely solely on the eXtra-largemodel (X) since it is themost powerful configuration to find fine-grain objects inside a frame due to its architecture. YOLOV5-X has been trained on our custom dataset by fine- tuning pretrained weights [33] on an NVIDIA RTX 3060 OC. The YOLOV5-X results are depicted in Table II. In principle, its precision on ONLY is considerably high, while it balances precision and recall on ALL.

YOLOV9 [34] introduces some innovation with respect to its predecessor. Specifically, to improve accuracy, it introduces programmable gradient information (PGI) and the general- ized efficient layer aggregation network (GELAN). PGI pre- vents data loss and ensures accurate gradient updates, whereas GELAN optimizes lightweight models with gradient path plan- ning. We trained YOLOV9-T and YOLOV9-E using transfer

TABLE II

RESULTS FOR TESTING SSD, DETR, YOLOV5, YOLOV9, AND YOLOV10, WITH IOU = 0.5

\begin{tabular}{cccccl} & model & & P & R & mAP$_{0.5}$ & mAP$_{0.5}^{0.95}$ \\ \\[-0.9em] \hline \\[-0.9em] & \multirow{2}{*}{SSD} & \textbf{ALL} & 65.5\% & 26.8\% & 57.4\% & 25.1\% \\ & & \textbf{ONLY} & 73.1\% & 26.8\% & 59.8\% & 26.3\% \\ & \multirow{2}{*}{DETR} & \textbf{ALL} & 81.8\% & 12.7\% & 50.9\% & 24.1\% \\ & & \textbf{ONLY} & 81.8\% & 12.7\% & 54.4\% & 25.8\% \\ \multirow{7}{*}{\rotatebox{90}{\tiny C = 0.33}} & \multirow{2}{*}{YOLOV5-X} & \textbf{ALL} & 53.5\% & 54.8\% & 44.4\% & 29.0\% \\ & & \textbf{ONLY} & 95.1\% & 54.8\% & 52.0\% & 34.3\% \\ & \multirow{2}{*}{YOLOV9-T} & \textbf{ALL} & 71.0\% & 61.0\% & 67.0\% & 37.0\% \\ & & \textbf{ONLY} & 95.0\% & 61.0\% & 78.0\% & 44.0\% \\ & \multirow{2}{*}{YOLOv9-E} & \textbf{ALL} & 57.50\% & 52.0\% & 51.0\% & 27.0\% \\ & & \textbf{ONLY} & 89.0\% & 52.0\% & 69.0\% & 37.0\% \\ & \multirow{2}{*}{YOLOv10-N} & \textbf{ALL} & 74.0\% & 60.0\% & 50.0\% & 23.0\% \\ & & \textbf{ONLY} & 90.0\% & 60.0\% & 71.0\% & 33.0\% \\ & \multirow{2}{*}{YOLOv10-X} & \textbf{ALL} & 50.0\% & 64.0\% & 41.0\% & 19.0\% \\ & & \textbf{ONLY} & 79.0\% & 64.0\% & 72.0\% & 32.0\% \\ \\[-0.9em] \hline \\[-0.9em] \multirow{13}{*}{\rotatebox{90}{\tiny C = 0.11}} & \multirow{2}{*}{SSD} & \textbf{ALL} & 62.5\% & 28.2\% & 56.7\% & 24.0\% \\ & & \textbf{ONLY} & 71.4\% & 28.2\% & 60.6\% & 25.9\% \\ & \multirow{2}{*}{DETR} & \textbf{ALL} & 73.3\% & 14.1\% & 54.4\% & 25.8\% \\ & & \textbf{ONLY} & 73.3\% & 14.1\% & 57.2\% & 26.8\% \\ & \multirow{2}{*}{YOLOv5-X} & \textbf{ALL} & 39.5\% & 62.0\% & 48.7\% & 31.0\% \\ & & \textbf{ONLY} & 92.4\% & 62.0\% & 60.6\% & 39.9\% \\ & \multirow{2}{*}{YOLOv9-T} & \textbf{ALL} & 71.0\% & 66.0\% & 67.0\% & 36.0\% \\ & & \textbf{ONLY} & 94.0\% & 66.0\% & 81.0\% & 45.0\% \\ & \multirow{2}{*}{YOLOv5-X} & \textbf{ALL} & 57.0\% & 62.0\% & 50.0\% & 26.0\% \\ & & \textbf{ONLY} & 88.0\% & 62.0\% & 74.0\% & 38.0\% \\ & \multirow{2}{*}{YOLOv10-N} & \textbf{ALL} & 74.0\% & 60.0\% & 50.0\% & 23.0\% \\ & & \textbf{ONLY} & 90.0\% & 60.0\% & 71.0\% & 33.0\% \\ & \multirow{2}{*}{YOLOv10-X} & \textbf{ALL} & 50.0\% & 64.0\% & 41.0\% & 19.0\% \\ & & \textbf{ONLY} & 79.0\% & 64.0\% & 72.0\% & 32.0\% \\ \\[-0.9em] \hline \end{tabular}

learning. Table II gives that both models achieve notable per- formance. YOLOV9-T demonstrates a solid robustness against background-only patches, overcoming YOLOV9-E in all the metrics. Despite the previous observation, YOLOV9-E depicts significant detection ability. Notably, YOLOV9-T showcases simultaneously a high mAP0.5 and mAP0.950.5 , setting interesting confidences and IoU values, respectively.

We extend our analysis to the latest release of YOLO family, namely, YOLOV10 [35]. The newest architecture offers a range of model scales, but we decided to rely on YOLOV10-N and YOLOV10-X.One of themain improvements is the introduction of consistent dual assignments, which replaces nonmaximum suppression. Moreover, the developers introduced partial self- attention to boost the detection ability without any burdening on inference speed. As reported in Table II, YOLOV10 confirms its ability to recognize objects also in challenging environments showcasing interesting performance for both the models. Once again, despite YOLOV10-N is the lightest version, it outper- forms YOLOV10-X demonstrating also a fair robustness against background-onlypatches.On theother hand,YOLOV10-Xgains limited knowledge on BMSB detection suggesting potential overfitting on training data.

Comparing all the NNs, SSD and DETR have performed almost the same obtaining a higher precision and a lower recall. SDD tends to miss fewer BMSB inside the frame than DETR, detecting at least the≈ 27% of the total occurrences in contrast with the poor ≈ 13% reached by DETR. Both SSD and DETR revealed a significant robustness against background-only pixels because their performance is almost the same in the two sets, i.e., ALL and ONLY. Moreover, DETR raised less FP than SSD, characterizing a more precise detection overall. Conversely, the YOLOV5-X model was able to detect the largest number of BMSB reaching the highest recall. Despite achieving partic- ularly notable metrics scores in the ONLY set, the model has depicted the least robustness against background-only patches, namely, in the ALL set. So, YOLOV5-X was prone to recognize ≥55% of BMSB instances with a particularly promising pre- cision when a BMSB appears inside the frame. However, this latter capability deteriorates dramatically (P = 53.5%) when images without BMSB are considered. It is noteworthy that each network tends to achieve a larger P thanR. This phenomenon is strictly related to the poses experienced during the acquisition, compounded by the noise surrounding the BMSB pixels, such as blur or overexposure. Despite the dataset comprising a diverse range of BMSB samples, certain poses and lighting/blurriness conditions are more frequent than others. Also, since the dataset consists of images captured by a drone operating according to an autonomous protocol, characterizing the entire range of possi- bilities is extremely challenging. As a result, the networks were able to detect BMSB instances that presented limited variation in pose and color appearance, thereby minimizing FP. How- ever, instances significantly affected by noise were occasionally missed. Both YOLOV9 and YOLOV10-X achieve interesting performance representing a true step ahead of the predeces- sor, YOLOV5. Although YOLOV10-X obtained the largest R, YOLOV9-T achieves the best performance overall. Specifically, it reaches a satisfactorily R (≈ 60%) and almost the largest P with both the thresholds, 95.0% and 94.0%, respectively.