Published as a conference paper at ICLR 2024

\begin{tabular}{l|rl|l|rl|l|rl} \toprule scenario & train & test & scenario & train & test & scenario & train & test \\ \midrule others & 317 & 79 & writing\_cooking\_recipe & 40 & 11 & classification\_identification & 24 & 6 \\ functional\_writing & 128 & 32 & explaining\_code & 40 & 10 & language\_polishing & 22 & 4 \\Brainstorming & 90 & 24 & writing\_legal\_document & 40 & 10 & chitchat & 22 & 7 \\ seeking\_advice & 88 & 25 & asking\_how\_to & 40 & 10 & writing\_product\_description & 20 & 5 \\ open\_question & 77 & 20 & writing\_presentation\_script & 38 & 10 & data\_analysis & 18 & 5 \\ explaining\_general & 66 & 17 & writing\_social\_media\_post & 38 & 10 & writing\_marketing\_materials & 17 & 5 \\ instructive\_rewriting & 58 & 15 & question\_generation & 38 & 10 & note\_summarization & 17 & 4 \\ verifying\_fact & 49 & 13 & planning & 38 & 10 & paraphrasing & 17 & 5 \\ analyzing\_general & 49 & 13 & writing\_blog\_post & 36 & 9 & writing\_technical\_document & 17 & 5 \\ title\_generation & 48 & 12 & writing\_job\_application & 36 & 10 & text\_simplification & 16 & 5 \\ code\_generation & 48 & 12 & writing\_personal\_essay & 36 & 10 & information\_extraction & 16 & 2 \\ roleplay & 47 & 12 & value\_judgement & 35 & 9 & writing\_biography & 16 & 4 \\ rejecting & 45 & 12 & code\_to\_code\_translation & 32 & 9 & text\_correction & 12 & 6 \\ creative\_writing & 45 & 12 & writing\_advertisement & 31 & 8 & reading\_comprehension & 12 & 3 \\ exam\_question\_without\_math & 44 & 12 & writing\_email & 30 & 8 & keywords\_extraction & 12 & 3 \\ writing\_song\_lyrics & 44 & 11 & recommendation & 29 & 8 & topic\_modeling & 10 & 3 \\ text\_to\_text\_translation & 43 & 11 & ranking & 28 & 8 & writing\_scientific\_paper & 10 & 3 \\ text\_summarization & 43 & 12 & counterfactual & 26 & 7 & peer\_review & 7 & 2 \\ code\_correction\_rewriting & 43 & 11 & exam\_question\_with\_math & 24 & 4 & code\_simplification & 6 & 2 \\ math\_reasoning & 41 & 12 & writing\_news\_article & 24 & 6 & overall & 2383 & 623 \\ \bottomrule \end{tabular}

Table 7: The scenario distribution in the training and test set for scenario classifier, note that “rejecting” and “peer_review” are two early-defined scenarios that have been removed by us.

B TRAINING DETAILS OF SCENARIO CLASSIFIER

In this section we describe in detail the training process of the scenario classifier mentioned in §3.2.

We model the scenario classification task as a generation task. The classifier are re- quired to generate only the scenario name when given the query, with the prompt as "Identify the scenario for the user’s query, output ’default’ if you are uncertain.\n\nQuery:\n\n{input}\n\nScenario:" (the "default" scenario in the prompt is the early naming for "others" scenario).

In general, the training involves three steps:

1. We first brainstorm about 10 seed queries for each scenario with the help of ChatGPT, and train a model that can directly output the scenario name when given a query as a conditional generation task on this small synthetic dataset.

2. Using the trained model, we conducted an initial classification for queries in Chatbot Arena Conversations and ShareGPT as they cover much more scenarios than other datasets. Based on this preliminary classification, we randomly select up to 50 queries from each scenario for a secondary manual validation, involving data cleaning and correcting misclassified labels.

3. We combine the newly-collected dataset and the small synthetic dataset in step 1, and retrain our final classifier. We divide queries in each scenario in an 8:2 train/test split (Tab. 7). The accuracy and F1 of the final classifier on test set are 72.55 and 74.12, respectively.

Our scenario classifier is trained from LLaMA-2-13B (Touvron et al., 2023b), and we set the max sequence length as 2,048, and the max length for query as 2,048-50=1,998 both in training and inference. If a query Q with length L exceeds that limit, we truncate it from the middle and replace the dropped part with a "..." since the front and end of the sequence usually contain more → important information for identifying scenario of the (such as the user’s instruction): Q1:L [Q1:999; ...;QL−1000:L].

We train the scenario classifier for 3 epochs on the training set, and set the batch size as 64. Without warmup steps, we set the initial learning rate to 1e-5 and cosine decaying to 0 by the end of training. The optimizer is AdamW with β1 = 0.9, β2 = 0.95 as in training AUTO-J, and we also use the speedup and GPU memory saving techniques like DeepSpeed Zero 3, BF16, TF32, and gradient- checkpointing. The loss is only calculated on the output end as well.

4This dataset is collected from https://sharegpt.com/, containing shared conversations with ChatGPT or GPT-4. We use a public available subset of it.