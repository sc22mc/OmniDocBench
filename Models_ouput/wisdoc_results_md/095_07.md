\begin{tabular}{l|cc|cc|cc|cc|cc|c} \textbf{Model} & \textbf{Backbone} & \textbf{Params} & \textbf{Book} & \textbf{Slides} & \textbf{Research} & \textbf{Textbook} & \textbf{Paper} & \textbf{Magazine} & \textbf{Academic} & \textbf{Literature} & \textbf{Notes} & \textbf{Newspaper} & \textbf{Average} \\ & & & & & \textbf{Report} & \textbf{Textbook} & \textbf{Paper} & \textbf{Magazine} & \textbf{Literature} & \textbf{Notes} & \textbf{Newspaper} & \\ \hline \multirow{1}{*}{DT:T{1}~[24]} & ViT-L & 361.6M & \underline{43.44} & 13.72 & 45.85 & 15.45 & 3.40 & 29.23 & \textbf{68.13} & 0.21 & 23.55 & 26.50 \\ \multirow{1}{*}{\parbox{1.6cm}{\vspace{1pt}Layered-MLV{27}~[177]}} & Retsina-DT{398}: DR-10 & 43.71 & 42.11 & 13.63 & 42.10 & 21.00 & 5.48 & 31.81 & \underline{64.66} & 0.80 & 30.34 & 28.84 \\ \multirow{1}{*}{\parbox{1.6cm}{\vspace{1pt}Doc-Layer-YOLO~[53]}} & ViT-O & 19.63 & \textbf{43.71} & \textbf{48.71} & \textbf{72.83} & \textbf{42.67} & \textbf{35.40} & \underline{51.14} & \underline{64.61} & \underline{9.54} & \textbf{57.54} & \textbf{47.38} \\ \multirow{1}{*}{\parbox{1.6cm}{\vspace{1pt}SwiftDocSegmenter~[41]}} & Swift-L & 223M & 42.91 & \underline{23.82} & \underline{47.29} & 22.44 & \underline{20.81} & \textbf{52.35} & 48.54 & \textbf{12.38} & \underline{38.06} & \underline{35.89} \\ \multirow{1}{*}{\parbox{1.6cm}{\vspace{1pt}Gzip-ASCII~[57]}} & R101 & 44.5M & 39.03 & 16.98 & 39.92 & 22.82 & 14.31 & 37.61 & 44.43 & 5.71 & 23.86 & 27.10 \\ \multirow{1}{*}{\parbox{1.6cm}{\vspace{1pt}DocCX-Chain~[59]}} & R101 & 44.5M & 39.03 & 16.98 & 39.62 & 19.23 & 10.67 & 23.00 & 41.60 & 1.60 & 16.96 & 21.27 \\ \end{tabular}

Table 7. Component-level Table Recognition evaluation on OmniDocBench table subset. (+/-) means with/without special situation.

\begin{tabular}{ccccccccc} \toprule \multirow{2}{*}{\textbf{Maid Type}} & \multirow{2}{*}{\textbf{Maid}} & \multicolumn{4}{c|}{\textbf{Language}} & \multicolumn{3}{c}{\textbf{Table Frame Type}} \\ & & \textit{DL} & \textit{D1} & \textit{MD} & \textit{Full} & \textit{Onscreen} & \textit{Three Zero} & \textit{Merge Cell(+)}. & \textit{OCR(+)*/1.0} & \textit{OCR(+)*/2.0} \\ \midrule \textbf{OCR-labeled} & PaarwiseOCR[\texttt{237}] & 76.8 & 71.8 & 80.1 & 67.9 & 74.3 & 81.1 & 74.5 & \textcolor{gray}{70/6/2.5} & \textcolor{gray}{1/1/0.7} & \textcolor{gray}{23.8/16.1/1.0} \\ \textbf{Mentions} & RatioTop[\texttt{37}] & \textbf{80.0} & \textbf{83.2} & \textbf{91.2} & \textbf{83.0} & \textbf{77.9} & \textbf{83.4} & \textcolor{gray}{74.2} & \textcolor{gray}{\textbf{73.8/3.3}} & \textcolor{gray}{\textbf{85.2/9.3/3.8}} \\ \midrule \textbf{Expert VLMs} & \texttt{SimCite-Estimate}\texttt{[55]} & \textcolor{gray}{72.8} & \textcolor{gray}{75.9} & \textcolor{gray}{83.4} & \textcolor{gray}{7.9} & \textcolor{gray}{\underline{6.6}} & \textcolor{gray}{7.9} & \textcolor{gray}{\textbf{6.5/0.8/0.5}} & \textcolor{gray}{\textbf{33.3/16.9/1.0}} & \textcolor{gray}{\textbf{40.2/47.8/4.3}} \\ & \texttt{COF-OCR} [\texttt{45}] & \textcolor{gray}{72.2} & \textcolor{gray}{75.5} & \textcolor{gray}{85.4} & \textcolor{gray}{\underline{73.1}} & \textcolor{gray}{7.2} & \textcolor{gray}{7.9} & \textcolor{gray}{32.6/16.9/1.0} & \textcolor{gray}{\textbf{43.4/49.2/4.9}} & \textcolor{gray}{\textbf{44.9/51.2/4.8}} \\ \midrule \textbf{General VLMs} & Quer2-ML-7 \texttt{[61]} & \textcolor{gray}{7.0} & \textcolor{gray}{70.7} & \textcolor{gray}{82.4} & \textcolor{gray}{7.2} & \textcolor{gray}{1.8} & \textcolor{gray}{7.6} & \textcolor{gray}{26.2/16.1/1.0} & \textcolor{gray}{13.6/17.9/1.0} & \textcolor{gray}{\underline{22.8/20.9/1.0}} \\ & \texttt{InferSent-ML} [\texttt{37}] & \textcolor{gray}{70.9} & \textcolor{gray}{71.5} & \textcolor{gray}{67.9} & \textcolor{gray}{6.2} & \textcolor{gray}{7.8} & \textcolor{gray}{16.1} & \textcolor{gray}{32.6/17.9/1.0} & \textcolor{gray}{21.3/25.6/2.3} & \textcolor{gray}{\underline{29.9/25.3/2.9}} \\ \midrule \end{tabular}

ponents, with tables, images, and ignored components ex- cluded from the final reading order calculation.

5. Benchmarks

Based on the distinct characteristics of these algorithms, we categorize document content extraction methods into three main classes:

• Pipeline Tools: These methods integrate layout detection and various content recognition tasks (such as OCR, ta- ble recognition, and formula recognition) into a document parsing pipeline for content extraction. Prominent exam- ples include MinerU [42] (v0.9.3), Marker [34] (v1.2.3), and Mathpix4.

• Expert VLMs: These are large multimodal mod- els specifically trained for document parsing tasks.

Representative models include GOT-OCR2.0 [45] and Nougat [7].

• General VLMs: These are general-purpose large mul- timodal models inherently capable of document pars- ing. Leading models in this category include GPT-4o [2], Qwen2-VL-72B [44], and InternVL2-76B [8].

5.1. End-to-End Evaluation Results

Overall Evaluation Results. As illustrated in Table 2, pipeline tools such as MinerU and Mathpix, demonstrate superior performance across sub-tasks like text recognition, formula recognition, and table recognition. Moreover, the general Vision Language Models (VLMs), Qwen2-VL, and GPT4o, also exhibit competitive performance. Almost all algorithms score higher on English than on Chinese pages. Performance Across Diverse Page Types. To gain deeper insights into model performance on diverse document types, we evaluated text recognition tasks across different page types. Intriguingly, as shown in Table 3, pipeline tools per- form well for commonly used data, such as academic pa- pers and financial reports. Meanwhile, for more specialized data, such as slides and handwritten notes, general VLMs demonstrate stronger generalization. Notably, most VLMs fail to recognize when dealing with the Newspapers, while pipeline tools achieve significantly better performance. Performance on Pages with Visual Degradations. In Table 4, we further analyze performance on pages con- taining common document-specific challenges, including fuzzy scans, watermarks, and colorful backgrounds. VLMs like InternVL2 and Qwen2-VL exhibit higher robustness in these scenarios despite visual noise. Among pipeline tools, MinerU remains competitive due to its strong layout seg- mentation and preprocessing capabilities.

Performance on Different Layout Types. Page layout is a critical factor in document understanding, especially for tasks involving reading order. OmniDocBench annotates layout attributes such as single-column, multi-column, and complex custom formats. Across all models, we observe a clear drop in accuracy on multi-column and complex lay- outs. MinerU shows the most consistent reading order pre- diction, though its performance dips on handwritten single- column pages due to recognition noise.

Discussion on End-to-End Results. 1) While general VLMs often lag behind specialized pipelines and expert models on standard documents (e.g., academic papers), they generalize better to unconventional formats (e.g., notes) and perform more robustly under degraded conditions (e.g., fuzzy scans). This is largely due to their broader training data, enabling better handling of long-tail scenarios com- pared to models trained on narrow domains. 2) VLMs, how- Table 8. Component-level evaluation on OmniDocBench OCR subset: results grouped by text attributes using the edit distance metric.

24844