Table 6. Results of the cross-domain evaluation for our VP estimator using HRNet-W32

\begin{tabular}{cccccccccccccccc} \hline & & \multicolumn{6}{c}{Keypoint metric $\uparrow$} & & \multicolumn{6}{c}{Mean distance error [pixel] $\downarrow$} \\ \cmidrule{1-1} \cmidrule{3-8} \cmidrule{10-15} {\bf Train} & {\bf Test} & {\bf AP} & {\bf AF$^{50}$} & {\bf AF$^{75}$} & {\bf AR} & {\bf AR$^{10}$} & {\bf AR$^{75}$} & {\bf PCK} & {\bf front} & {\bf left} & {\bf right} & {\bf top} & {\bf bottom} & {\bf YI$^{3}$} & {\bf ADP$^{1}$} & {\bf AI$^{1}$} \\ \hline \\[-1.8ex] \multirow{4}{*}{\bf SL-MH} & SL-MH & 0.99 & 0.99 & 0.99 & 0.97 & 0.98 & 0.98 & 0.99 & 2.67 & 2.50 & 2.52 & 1.90 & 1.72 & 2.39 & 3.64 & 3.10 \\ & SL-PB & 0.98 & 0.99 & 0.99 & 0.96 & 0.97 & 0.97 & 0.98 & 3.51 & 3.50 & 3.11 & 2.34 & 2.02 & 2.97 & 4.52 & 3.85 \\ & SP360 & 0.85 & 0.94 & 0.90 & 0.79 & 0.87 & 0.83 & 0.65 & 7.55 & 7.42 & 18.18 & 5.34 & 11.77 & 7.41 & 14.95 & 11.57 \\ & HeiCity & 0.80 & 0.92 & 0.86 & 0.73 & 0.83 & 0.78 & 0.77 & 9.73 & 12.97 & 7.75 & 8.54 & 6.0 & 4.67 & 17.89 & 14.11 \\ \hline \\[-1.8ex] \end{tabular}

VP denotes all 5 VPs; ADP denotes all 8 ADPs; All denotes all points consisting of 5 VPs and 8 ADPs

Table 7. Comparison of the absolute parameter errors and reprojection errors on the SL-MH test set

\begin{tabular}{ccccccccccccc} \hline \multirow{3}{*}{\textit{Method}} & \multirow{3}{*}{\textit{Backbone}} & \multicolumn{7}{ c }{{Mean absolute error}$\downarrow$} & \multicolumn{2}{ c }{Exhaustible } & \multicolumn{2}{ c }{{\textit{Ours$\downarrow$}}} \\ & & \textit{Pan o.} & \textit{T{\#}h.} & \textit{Roll v.} & \textit{f} & \textit{k} & \textit{r} & \textit{m} & \textit{w} & \textit{u} & \textit{v} & \textit{w} \\ \hline \textit{Lopez-Angewerkzeug$\downarrow$ et al.} [35]:\!\!\! & CVPR'19 & 79\% & 16 & 27 & 66 & 44.90 & 2.32 & -- & 81.99 & 100.0 & 35.4 & 27.5M & 7.2 \\ \textit{Wakai und Yamashita.} [52]:\!\!\! & ICCV'19 & 80\% & 16 & 70 & 14.9 & 27.1 & 2.73 & -- & 30.02 & 100.0 & 33.1 & 26.9M & 7.2 \\ \textit{Wakai et al.} [57]:\!\!\! & FCCV'20 & 78\% & 16 & 4 & 5.71 & 0.04 & 0.07 & 7.9 & 100.0 & 75.4 & 77.5M & 7.7 \\ \textit{Pits et al.} [41]:\!\!\! & CVPR'18 & 80\% & 16 & 25.35 & 12.52 & 18.56 & -- & -- & 96.7 & 7.304 & -- & -- \\ \textit{Lochmann et al.} [12]:\!\!\! & WACV'21 & 1 & 22.56 & 44.42 & 33.20 & 6.07 & -- & -- & 59.1 & 11.50 & -- & -- \\ \hline \textit{Ours w/o ADPs.} & ( 5 points!) & HNN+W$\downarrow$VW$\uparrow$ & 18 & 39 & 15.4 & 7.1 & 0.04 & 0.07 & 78.90 & 100.0 & 17.7 & 57.5M & 4.5$\downarrow$ \\ \textit{Ours w/o VP.} & ( 5 points!) & HNN+W$\downarrow$Kz & 10.5 & 1.01 & 7.11 & 0.34 & 0.02 & 0.19 & 70.00 & 100.0 & 12.6 & 53.5M & 4.5$\downarrow$ \\ \textit{Curs} & (13 points!) & HNN+W$\downarrow$W3$\uparrow$ & \textbf{2.19} & \textbf{3.10} & \textbf{2.88} & 0.04 & \textbf{0.07} & \textbf{5.34} & \textbf{100.0} & \textbf{17.2} & \textbf{96.9M} & \textbf{7.1$\downarrow$} \\ \hline \textit{Chu's} & (13 points!) & HNN+W$\downarrow$W4$\uparrow$ & \textbf{2.19} & \textbf{3.10} & \textbf{2.88} & 0.04 & \textbf{0.07} & \textbf{5.34} & \textbf{100.0} & \textbf{17.2} & \textbf{96.9M} & \textbf{7.1$\downarrow$} \\ \hline \end{tabular}

Units: pan ϕ, tilt θ, and roll ψ [deg]; f [mm]; k1 [dimensionless]; REPE [pixel]; Executable rate [%]

Implementations: López-Antequera [33], Wakai [52], Wakai [53], and ours using PyTorch [40]; Pritts [41] and Lochman [32] using The MathWorks MATLAB (· points) is the number of VP/ADPs for VP estimators; VP estimator backbones are indicated; Rotation estimation in Figure 4 is not included in GFLOPs

Table 8. Comparison of the mean absolute rotation errors in de- grees on the test sets of each dataset

\begin{tabular}{ccccccccc} \hline \multirow{2}{*}{Dataset} &\multicolumn{3}{c}{Wakai \textit{et al.} [\textcolor{red}{53}]} &\multicolumn{3}{c}{Lochman \textit{et al.} [\textcolor{red}{32}]} &\multicolumn{3}{c}{Ours (HRNet-W32)} \\ \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10} &Pan&Tilt&Roll&Pan&Tilt&Roll&Pan&Tilt&Roll \\ \hline SL-MH &-- &4.13 &5.21 &22.36 &44.42 &33.20 &\textbf{2.20} &\textbf{3.15} &\textbf{3.00} \\ SL-PB &-- &4.06 &5.71 &23.45 &44.99 &30.68 &\textbf{2.30} &\textbf{3.13} &\textbf{3.09} \\ SP360 &-- &3.75 &5.19 &22.84 &45.38 &31.91 &\textbf{2.16} &\textbf{2.92} &\textbf{2.79} \\ HoliCti &-- &6.55 &16.05 &22.63 &45.11 &32.58 &\textbf{3.48} &\textbf{4.08} &\textbf{3.84} \\ \end{tabular}

ious datasets to validate its robustness. Table 8 shows that our method outperforms both existing state-of-the-art learning-based [53] and geometry-based [32] methods on all datasets in terms of rotation errors. Table 9 also reports that our method is superior to Wakai et al.’s method [53], which tended to estimate the roll angle poorly in the cross- domain evaluation, especially on the HoliCity test set.

Table 9. Comparison on the cross-domain evaluation of the mean absolute rotation errors in degrees

\begin{tabular}{cccccccc} \midrule \multicolumn{2}{c}{Dataset} & \multicolumn{3}{c}{Wakak {\em et al.} [\textcolor{red}{53}]} & \multicolumn{3}{c}{Ours (HRNet-W32)} \\ \cmidrule(r){1-2} \cmidrule(r){3-5} \cmidrule(r){6-8} Train & Test & Pan & Tilt & Roll & Pan & Tilt & Roll \\ \midrule \multirow{3}*{SL-MH} & SL-PB & -- & 5.51 & 12.02 & 2.98 & \textbf{3.72} & \textbf{3.63} \\ & SP360 & -- & 9.11 & 37.54 & 8.06 & \textbf{8.34} & \textbf{7.77} \\ & HoliCity & -- & 10.94 & 42.20 & 10.74 & \textbf{10.60} & \textbf{8.93} \\ \end{tabular}

dress arbitrary images independent of the number of arcs; that is, it demonstrates scene robustness. Compared with methods [32, 41] estimating the pan angles, our method us- ing HRNet-W32 achieved a mean frames per second (fps) that was at least 280 times higher. Note that our test plat- form was equipped with an Intel Core i7-6850K CPU and an NVIDIA GeForce RTX 3080Ti GPU.

We validated the effectiveness of the ADPs. Table 7 sug- gests that our method based on HRNet-W32 and VP/ADPs notably improved angle estimation compared with our method without the ADPs by 15.4◦ on average for pan, tilt, and roll angles. Therefore, the ADPs dramatically allevi- ated the problems caused by a lack of VPs.

Additionally, we tested our proposed method using var-

4.4.3 Qualitative evaluation

To evaluate the recovered image quality, we performed cal- ibration on synthetic images and off-the-shelf cameras.

Synthetic images. Figure 6 shows the qualitative results obtained on synthetic images. Our results are the most sim- ilar to the ground-truth images. By contrast, the quality of the recovered images that contained a few arcs was con- siderably degraded when the geometry-based methods pro- posed by Pritts et al. [41] and Lochman et al. [32] were used. Furthermore, the learning-based methods proposed by López-Antequera et al. [33], Wakai and Yamashita [52], and Wakai et al. [53] did not recover the pan angles. We note that our method can even calibrate images in which trees line a street.

Off-the-shelf cameras. Following [53], we also evalu- ated calibration methods using off-the-shelf cameras to val- idate the effectiveness of our method. Figure 7 shows the qualitative results using off-the-shelf fisheye cameras us- ing SL-MH for training. Our method meaningfully outper- formed Lochman et al.’s method [32] in terms of recovered images. These results indicate robustness in our method for various types of camera projection.

11890