Published as a conference paper at ICLR 2024

2 IN-CONTEXT AUTOENCODER

2.1 MODEL ARCHITECTURE

Like a typical autoencoder (Kramer, 1991), ICAE consists of an encoder and a decoder. Similar to the design of Gisting (Mu et al., 2023) and AutoCompressor (Chevalier et al., 2023), the ICAE performs both the encoding and decoding processes in an in-context manner, as illustrated in Figure 3.

wi+1w3 m̃k… e(wL−1)e(wi)…

Figure 3: The encoder of the ICAE is a LoRA-adapted LLM, which is used for encoding the original context c = (w1, w2, . . . , wL) into a few memory slots (m̃1, . . . , m̃k). The decoder of the ICAE is the target LLM itself that can condition on the memory slots produced by the encoder for various purposes (e.g., the autoencoding task as in this figure). e(·) denotes the word embedding lookup in the target LLM and em(·) denotes the learnable embedding lookup of memory tokens that are used for producing memory slots.“[AE]” is a special token to indicate the autoencoding pretraining task.

Given the intuition, we propose to use a LoRA-adapted LLM as the encoder of the ICAE, as illustrated in Figure 3. When encoding a context c = (w1, . . . , wL) with the length L, we first append k (k << L) memory tokens (m1, . . . ,mk) to the context c to obtain their outputs (m̃1, . . . , m̃k) as the memory slots for the context c. Therefore, the ICAE encoder is very lightweight – it only adds a LoRA adapter and an embedding lookup for memory tokens compared with the target LLM.

As introduced above, we expect the memory slots (m̃1, . . . , m̃k) to be conditioned on by the target LLM on behalf of the original context c. Therefore, we use the untouched target LLM as the decoder of the ICAE to ensure the compatibility of memory slots within the target LLM.

2.2 PRETRAINING 2.2.1 AUTOENCODING

Like a typical autoencoder, one of the ICAE’s pretraining objectives is to restore the original input text c of the length L from its produced memory slots (m̃1, . . . , m̃k) of the length k:

$$
\mathcal{L}_{\textrm{AE}}=\operatorname*{max}_{\widetilde{m}_{1},\dots,\widetilde{m}_{k}}P({\boldsymbol{c}}|\widetilde{m}_{1},\dots,\widetilde{m}_{k};\Theta_{LLM})=\operatorname*{max}_{\Theta_{LoRA},e_{m}}P({\boldsymbol{c}}|m_{1}\dots m_{k};\Theta_{LLM},\Theta_{LoRA},e_{m})
$$

To indicate the autoencoding task, we append a special token “[AE]” to (m̃1, . . . , m̃k) in the decoder, as Figure 3 shows. As this pretraining objective does not need any extra annotation, we can use massive text data to train the In-context Autoencoder.

2.2.2 TEXT CONTINUATION

While autoencoding pretraining offers a straightforward learning objective to encode a context, its inherent simplicity and exclusive focus on the single objective may lead to suboptimal generalization. To address this issue, we incorporate an additional objective during the pretraining phase: text contin- uation, as illustrated in Figure 7 in Appendix A. This self-supervised task is widely acknowledged to facilitate the learning of more generalizable representations in language models: where o = (wL+1, . . . , wL+N ) denotes the continuation of context c. This objective helps improve generalization and circumvent excessive reliance on, and overfitting to, the autoencoding task.

$$
\mathcal{L}_{\textrm{LM}}=\operatorname*{max}_{\widetilde{m}_{1},\dots,\widetilde{m}_{k}}P\langle\boldsymbol{o}|\widetilde{m}_{1},\dots,\widetilde{m}_{k};\Theta_{LLM}\rangle=\operatorname*{max}_{\Theta_{LoRA},e_{m}}P\langle\boldsymbol{o}|m_{1}\dots m_{k};\Theta_{LLM},\Theta_{LoRA},e_{m}\rangle
$$