History-Aware Hierarchical Transformer for Multi-session Open-domain

Dialogue System

Tong Zhang1, Yong Liu2,3, Boyang Li1, Zhiwei Zeng3, Pengwei Wang4, Yuan You4

Chunyan Miao1,2,3, Lizhen Cui5 1School of Computer Science and Engineering, Nanyang Technological University, Singapore 2Alibaba-NTU Singapore Joint Research Institute, Nanyang Technological University, Singapore 3Joint NTU-UBC LILY Research Centre, Nanyang Technological University, Singapore 4Alibaba Group, China 5School of Software, Shandong University, China

Abstract

With the evolution of pre-trained language models, current open-domain dialogue sys- tems have achieved great progress in conduct- ing one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is under-investigated. In this paper, we propose History-Aware Hierar- chical Transformer (HAHT) for multi-session open-domain dialogue. HAHT maintains a long-term memory of history conversations and utilizes history information to under- stand current conversation context and gen- erate well-informed and context-relevant re- sponses. Specifically, HAHT first encodes his- tory conversation sessions hierarchically into a history memory. Then, HAHT leverages historical information to facilitate the under- standing of the current conversation context by encoding the history memory together with the current context with attention-based mech- anisms. Finally, to explicitly utilize histori- cal information, HAHT uses a history-aware response generator that switches between a generic vocabulary and a history-aware vocab- ulary. Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline mod- els. Human evaluation results support that HAHT generates more human-like, context- relevant and history-relevant responses than baseline models.

1 Introduction

Open-domain dialogue systems, also known as chatbots, are designed to converse with and engage users on any topic with the aim of establishing, maintaining, and strengthening long-term relation- ships (Clark et al., 2019; Roller et al., 2020). Re- cently, open-domain dialogue systems built based on large-scale generative pre-trained models (Adi- wardana et al., 2020; Roller et al., 2021; Zhang et al., 2020) have substantially improved the per- formance of chatbots.

Session 1 Conversation

Figure 1: An illustrated example of a two-session con- versation between a user and an agent.

However, most existing chatbots are designed to interact with users in a single conversation ses- sion. When the current session ends, the chatbot forgets its contents and will commence a new in- dependent session with the same user next time. When previously discussed topics reemerge, such chatbots often appear ignorant and fail to reengage users appropriately. The apparent forgetfulness lim- its the chatbotsâ€™ ability to establish and maintain long-term relationships with users.

We argue that, to better engage users in multi- session conversations (MSCs), a chatbot should maintain a long-term memory of historical con- texts, which allows the chatbot to reengage the user appropriately when similar contexts reemerge. By learning from historical conversations, the chat- bot should gradually refine its understanding of and deepen its relationship with the user. Figure 1 shows an example of a two-session conversation between a user and a chatbot. In the second session, the chatbot infers that Sonny is a cat and generates the response based on the history information that Sonny likes watching TV with the user.

History-aware chatbots will be able to gener- ate more well-informed and context-relevant re- sponses, which can help to elicit long-term com- mitments and develop emotional attachments from users to sustain close relationships over time. To this end, we propose the History-Aware Hierarchi- cal Transformer (HAHT) for multi-session open- domain dialogue systems, which can effectively leverage history conversations to conduct more engaging MSCs. HAHT maintains a long-term memory to store historical conversational contexts, which is updated when a new session is conducted. Based on the long-term memory and the context in the current session, relevant tokens in historical contexts are selected to adapt the current response.

arXiv:2302.00907v1 [cs.CL] 2 Feb 2023