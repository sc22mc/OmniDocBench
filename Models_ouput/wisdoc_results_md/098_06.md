Published as a conference paper at ICLR 2024

Table 2: 1 example showing how the pretrained ICAE (k = 128) restores the original context.

\begin{tabular}{l|l} \textbf{Origin Context} & \textbf{Restoration} \\ \multicolumn{2}{l}{Large pretrained \colorbox{yellow!40}{language} models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen in-put without additional parameter updates. Despite the great success in performance, the working mechanism of ICL still remains an open problem. In order to better understand how ICL works, this paper explains language models as meta- & Large pretrained models have shown surprising In-Context Learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without additional parameter updates. Despite the great success in per- & formance, the working mechanism of ICL still remains an open \\ & problem. In order to better understand how ICL works, this paper explains \colorbox{yellow!40}{how} language models as meta-optimizers and \colorbox{yellow!40}{understands ICL} as a kind of implicit finetun- & ing. Theoretically, we figure out that the Transformer attention \\ & has a dual form of gradient descent based optimization. On & The gradient of gradient descent based \\ & top of it, we understand ICL as follows: GPT first produces & GPT first produces metagadients \\ & gradients according to the demonstration examples, and & according to the demonstration examples, and then these meta-\\ & then these meta gradients are applied to the original GPT to & gradients are applied to the original GPT to build an ICL model. Experimentally, we comprehensively compare the behavior of & \\ & Experimentally, we comprehensively compare the behavior of ICL and explicit finetuning based & ICL and explicit finetuning based \\ & on real tasks to provide empirical evidence that supports our & empirical evidence that supports our \textbf{findings}. The \colorbox{yellow!40}{experimental} & \colorbox{yellow!40}{evidence proves} that ICL behaves \colorbox{yellow!40}{like us to the same extent}. & \\ & \colorbox{yellow!40}{to explicit finetuning} at the \colorbox{yellow!40}{prediction} level, the representation & \colorbox{yellow!40}{Prediction} at the explicit \colorbox{yellow!40}{finetuning} level, the representation \\ & level, and the attention behavior level. Further, inspired by our & level, and the attention behavior level. Further, inspired by our & understanding of meta-optimization, we design a momentum- & momentum-based\\ & algorithm. Its consistently better performance \colorbox{yellow!40}{over} & \colorbox{yellow!40}{momentum} finetuning. & \colorbox{yellow!40}{momentum gradient} algorithm. Its consistently better perfor- & over \\ & vanilla attention supports \colorbox{yellow!40}{our understanding} again from an & another aspect, and more importantly, it shows the potential to & \colorbox{yellow!40}{use} our understanding for future \colorbox{yellow!40}{model} \\ & \colorbox{yellow!40}{utilize} our understanding for future \colorbox{yellow!40}{model designing}. & \end{tabular}

Table 3: Restoration performance for different types of 512-token content with 128 memory slots. Patterned random text is obtained by adding 1 to each token_id in a normal text.

\begin{tabular}{c|c|c} \toprule \textbf{Content type} & \textbf{Loss} & \textbf{BLEU}\\ \midrule Normal text & 0.01 & 99.3 \\ Patterned random text & 1.63 & 3.5 \\ Completely random text & 4.55 & 0.2 \\ \bottomrule \end{tabular}

Based on this intuition, it is very likely that a more powerful LLM may support a higher compression ratio without significant forgetting. We will discuss it in Section 3.3.1.

3.2.2 FINE-TUNED ICAE

In order to evaluate the fine-tuned ICAE’s performance, we evaluate on the PWC test set. We use the GPT-4 to compare the outputs of the two systems to determine which one performs better or if they are on par with each other, following Mu et al. (2023). Table 4 shows the comparison of results of the LLMs conditioned on memory slots and original contexts. For Llama-7b (fine-tuned ICAE), we compare with Alpaca and StableLM-tuned-alpha-7b since there is no official instruction-tuned Llama-1 model. The Llama-7b (ICAE) conditioned on 128 memory slots largely outperforms both Alpaca and StableLM which can access original contexts (∼512 tokens), with a win rate of 56.7% and 74.1% respectively and a win+tie rate of 73%∼81%. However, when compared to the GPT-4 (we regard it as the gold standard), there is still a significant gap, with around 70% of the cases underperforming the GPT-4’s results, and a win+tie ratio of about only 30%.

When we switch the base model to Llama-2-chat, we observe ICAE’s performance becomes much better than its counterpart based on Llama-1: when k = 128, its win+tie rate can reach around 75% againt the GPT-4 although it still lags behind its counterpart conditioning on the original context as the compression is lossy. As k increases, the win+tie rate further improves while the compression rate decreases. We perform the same comparative studies on Llama-2-13b-chat and observe better results of ICAE, supporting our assumption in Section 3.2.1 that the ICAE can benefit more on larger LLMs.

We investigate the impact of memory length on results. Table 5 shows pairwise comparisons between ICAE models with varying memory slot lengths. A higher compression ratio makes it harder to ensure response quality, but a larger ratio doesn’t always lead to worse performance. Table 5 highlights that a pretrained ICAE with 8× compression (k=64) can match a non-pretrained ICAE with 4× compression (k=128). Under the same ratio, the pretrained ICAE performs much better than its