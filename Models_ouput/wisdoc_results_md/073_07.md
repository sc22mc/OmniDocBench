Figure 3: Initial state sensitivity in learning dynamics in multi-memory games. In the top panels, colored lines are ′) time series of xi (X’s strategy). The black line is the distance between the solid (sample of x) and broken (x lines. In the bottom panels, the black lines indicate the maximum eigenvalue in the learning dynamics of the solid line.

Figure 4: A. Payoff matrices of three-action (rock-paper-scissors) and four-action (extended rock-paper-scissors) games.

The black broken B. In each panel, colored lines indicate time series of xa|s for random a ∈ A and s ∈ S .

line indicates the Kullback-Leibler divergence averaged over all the states s ∈ S , intuitively meaning a distance from the Nash equilibrium.

that in the zero-memory version of the game. This theorem means that in zero-sum games, the region of Nash equilibrium does not expand even if players have memories. Taking into account that having multiple memories expands the region of Nash equilibrium, such as a cooperative equilibrium in prisoner’s dilemma games [20], this theorem is nontrivial.

In order to discuss whether our algorithms converge to this unique Nash equilibrium under Assumption 1, we consider the neighbor of Nash equilibrium and define sufficient small deviation from the Nash equilibrium, i.e., δ := x − x∗1 and ǫ := y − y∗1. Here, we assume that these deviations have the same scale O(δ) := k) O(δi) = O(ǫi) for all i. Then, defining that the superscript (k) shows O(δ terms, the dynamics are approximated by ẋ ≃ ẋ(1) + ẋ(2) and ẏ ≃ ẏ(1) + ẏ(2);