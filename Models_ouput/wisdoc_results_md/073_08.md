∗ ẋ(1) = +x∗(1− x∗)(u · 1z)p ◦ ǫ, ∗ ẏ(1) = −y∗(1− y∗)(u · 1z)p ◦ δ, ∗ ẋ(2) = −(x∗ − x̃∗)(u · 1z)δ ◦ ǫ ◦ p ∗)ǫ + x∗x̃∗(u · 1z){(δ · p ◦ y∗ ◦ 1x ∗ ∗}, + (ǫ · p∗)ǫ ◦ x∗ ◦ 1y + (δ ◦ ǫ ◦ y · 1x)p ∗ ẏ(2) = +(y∗ − ỹ∗)(u · 1z)δ ◦ ǫ ◦ p ∗)δ − y∗ỹ∗(u · 1z){(δ · p ◦ y∗ ◦ 1x ∗ ∗}, + (ǫ · p∗)δ ◦ x∗ ◦ 1y + (δ ◦ ǫ ◦ x · 1y)p

(12) (13) (14) (15)

with x∗ := (x∗, x∗, x̃∗, x̃∗), y∗ := (y∗, ỹ∗, y∗, ỹ∗), p∗ := x∗◦y∗, 1x := (+1,+1,−1,−1), 1y := (+1,−1,+1,−1), and 1z := 1x ◦ 1y. Eqs. (12)-(15) are derived by considering small changes in the stationary condition pst =Mpst for deviations of δ and ǫ (see Appendix B.1 and B.2 for the detailed calculation). By this, we can avoid a direct calculation of pst, which is hard to be obtained.

5 Experimental Findings 5.1 Simulation and Low-Order Approximation

From the obtained dynamics, i.e., Eqs. (12)-(15), we interpret the learning dynamics in detail. In the first- order dynamics, multi-memory learning is no more than a simple extension of the zero-memory one. Indeed, the zero-memory learning draws an elliptical orbit given by Hamiltonian as the conserved quantity [30, 14]. Eqs. (12) and (13) mean that the multi-memory dynamics also draw similar elliptical orbits for each pair of xi and yi. In other words, the dynamics are given by a linear flow on a four-dimensional torus. Because no interaction occurs between the pair of i and i′ such that i 6= i′, the dynamics of the multi-memory learning for each state are qualitatively the same as learning without memories. Fig. 2 shows the time series of the multi-memory learning dynamics near the Nash equilibrium in an example of a two-action zero-sum game, the penny-matching game (u1 = u4 = 1, u2 = u3 = −1). The experimental trajectories are generated by the Runge-Kutta fourth-order method of Eq. (10) (see Appendix B.3 for details), while the approximated trajectories are by the Runge-Kutta fourth-order method for the first- (Eqs. (12) and (13)), the second- (Eqs. (14) and (15)), and the third-order approximations (in Appendix B.2). The step-size is 10−2 in common. The top-left panel in the figure shows that the dynamics roughly draw a circular orbit for each state and are well approximated by the first-order dynamics of Eqs. (12) and (13). However, the top-right panel, where a sufficiently long time has passed, shows that the dynamics deviate from the circular orbits.

Such deviation from the circular orbits is given by higher-order dynamics than Eqs. (12) and (13). In the second-order dynamics given by Eqs. (14) and (15), the multi-memory learning is qualitatively different from the zero-memory one. Indeed, Eqs. (14) and (15) obviously mean that interactions occur between the pair of i and i′ such that i 6= i′. Thereby, the dynamics of multi-memory learning become much more complex than that of zero-memory learning. In practice, no Hamiltonian function, denoted by H(2), exists in the second-order dynamics, as different from the first-order one. One can check this by calculating (2) (2) ′ ∂ẋ /∂ǫi′ + ∂ẏ /∂δi 6= 0 for i and i 6= i, if assuming that Hamiltonian should satisfy ẋ(2) = +∂H(2)/∂ǫ i i′

and ẏ(2) = −∂H(2)/∂δ. Thus, the multi-memory dynamics might not have any conserved quantities and not draw any closed trajectory. Indeed, the right panels in Fig. 2 show that the dynamics tend to diverge from the Nash equilibrium. This divergence from the Nash equilibrium is surprising because zero-memory learning in zero-sum games always has a closed trajectory and keeps the Kullback-Leibler divergence from the Nash equilibrium constant [31, 14]. Here, note that we need the third-order dynamics to fit the experimental dynamics well, as seen by comparing the middle-right and lower-right panels in Fig. 2. The error between the experiment (δ and ǫ) and approximation (δ′ and ǫ′) is evaluated by