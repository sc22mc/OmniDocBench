This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.

Except for this watermark, it is identical to the accepted version;

the final published version of the proceedings is available on IEEE Xplore.

OmniDocBench: Benchmarking Diverse PDF Document Parsing with

Comprehensive Annotations

Linke Ouyang1→ Yuan Qu1→ Hongbin Zhou1→ Jiawei Zhu1→ Rui Zhang1→ Qunshu Lin2→ Bin Wang1→† Zhiyuan Zhao1 Man Jiang1 Xiaomeng Zhao1 Jin Shi1 Fan Wu1 Pei Chu1 Minghao Liu3

Zhenxiang Li1 Chao Xu1 Bo Zhang1 Botian Shi1 Zhongying Tu1 Conghui He1‡

1Shanghai AI Laboratory 2Abaka AI 32077AI

Abstract

Document content extraction is a critical task in com- puter vision, underpinning the data needs of large lan- guage models (LLMs) and retrieval-augmented generation (RAG) systems. Despite recent progress, current docu- ment parsing methods have not been fairly and compre- hensively evaluated due to the narrow coverage of docu- ment types and the simplified, unrealistic evaluation pro- cedures in existing benchmarks. To address these gaps, we introduce OmniDocBench, a novel benchmark featur- ing high-quality annotations across nine document sources, including academic papers, textbooks, and more challeng- ing cases such as handwritten notes and densely typeset newspapers. OmniDocBench supports flexible, multi-level evaluations—ranging from an end-to-end assessment to the task-specific and attribute-based analysis—using 19 layout categories and 15 attribute labels. We conduct a thor- ough evaluation of both pipeline-based methods and end- to-end vision-language models, revealing their strengths and weaknesses across different document types. Om- niDocBench sets a new standard for the fair, diverse, and fine-grained evaluation in document parsing. Dataset and code are available at https://github.com/ opendatalab/OmniDocBench.

1. Introduction

As large language models [1, 28, 39, 44] increasingly rely on high-quality, knowledge-rich data, the importance of ac- curate document parsing has grown substantially. Docu- ment parsing, a core task in computer vision and document intelligence, aims to extract structured, machine-readable content from unstructured documents such as PDFs. This task is particularly critical for ingesting academic papers, technical reports, textbooks, and other rich textual sources

→ The authors contributed equally. † Project lead. ‡ Corresponding author (heconghui@pjlab.org.cn).

24838

Figure 1. Results of End-to-End Text Recognition on Om- niDocBench across 9 PDF page types.

into large language models, thereby enhancing their factual accuracy and knowledge grounding [19, 42, 45, 47, 52]. Moreover, with the emergence of retrieval-augmented gen- eration (RAG) systems [12, 22], which retrieve and generate answers conditionally with external documents, the demand for precise document understanding has further intensified.

To address this challenging task, two main paradigms have emerged: 1) Pipeline-based approaches that decom- pose the task into layout analysis, OCR, formula/table recognition, and reading order estimation [34, 42]; and 2) End-to-end vision-language models (VLMs) that directly output structured representations (e.g., Markdown) [3, 7, 8, 29, 45, 46, 48]. Although both approaches have demon- strated promising results, conducting a broad comparison of their effectiveness remains challenging due to the absence of a comprehensive and unified evaluation benchmark.

As shown in Table 1, for pipeline-based document pars- ing systems, dedicated benchmarks [10, 26, 54] have been