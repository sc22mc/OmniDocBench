2 Preliminary 2.1 Two-Player Normal-Form Game

Let us define two-player (of X and Y) m(∈ N)-action games (see illustration of Fig. 1-A). Player X and Y choose their actions from A = {a1, · · · , am} and B = {b1, · · · , bm} in a single round. After they finish choosing their actions a ∈ A and b ∈ B, each of them gains a payoff U(a, b) ∈ R and V (a, b) ∈ R, respectively.

2.2 Two-Player Multi-Memory Repeated Game

We further consider two-player n(∈ N)-memory repeated games as an iteration of the two-player normal- form game (see illustration Fig. 1-A). The players are assumed to memorize their actions in the last n

rounds. Since each player can take m actions, there are m2n cases for possible memorized states, described ∏n as S = × B). Under any memorized state, player X can choose any action stochastically. Such a k=1(A

stochastic choice of an action is described by a parameter xa|s, which means the probability of choosing an

action a ∈ A under memorized state s ∈ S. Thus, X’s strategy is represented by |S|(= m2n)-numbers of ∏ ∏ m−1, m−1. (m− 1)-dimension simplexes, x ∈ ∆ while Y’s is y ∈ ∆ s∈S s∈S

2.3 Formulation as Markov Games

In order to handle this multi-memory repeated game as a Markov game [28, 29], we define a vector notation of memorized states;

$$
{\boldsymbol{s}}=(\underbrace{a_{1}b_{1}\cdots a_{1}b_{1}}_{\times n},\underbrace{a_{1}b_{1}\cdots a_{1}b_{1}}_{\times(n-1)}a_{1}b_{2},\cdots,\underbrace{a_{m}b_{m}\cdots a_{m}b_{m}}_{\times n}),
$$

(1)

which orders all the elements of S as a vector. We also define a vector notation of utility function as

$$
\boldsymbol{u}=(\underbrace{U(a_{1},b_{1}),\cdots,U(a_{1},b_{1})}_{\times m^{2n-2}},\underbrace{U(a_{1},b_{2}),\cdots,U(a_{1},b_{2})}_{\times m^{2n-2}},\cdots,\underbrace{U(a_{m},b_{m}),\cdots,U(a_{m},b_{m})}_{\times m^{2n-2}}),
$$

(2)

which orders all the last-round payoffs for S as a vector. The utility function for Y, i.e., v, is defined similarly. In addition, we denote an index for these vectors as i ∈ {1, . . . ,m2n}. For example, when player X is in state si, the last-round payoff for player X was ui.

Let p ∈ ∆|S|−1 be a probability distribution on s in a round. As the name Markov matrix implies, a distribution in the next round p′ is given by p′ =Mp, whereM is a Markov transition matrix;

$$
M_{i^{\prime}i}=\left\{\begin{array}{ll}{{x^{a|s_{i}}y^{b|s_{i}}}}&{{(s_{i^{\prime}}=abs_{i}^{-})}}\\ {{0}}&{{\mathrm{(otherwise)}}}\end{array}\right.,
$$

(3)

which shows the transition probability from i-th state to i′-th one for i, i′ ∈ {1, . . . ,m2n}. Here, note that s−i shows the state si except for the oldest two actions. See Fig. 1-B illustrating an example of Markov transition.

2.4 Nash Equilibrium

We now analyze the Nash equilibrium in multi-memory repeated games based on the formulation of Markov games. Let us assume that every agent uses a fixed strategy x and y or learns slowly enough for the timescale of the Markov transitions. If we further assume that the strategies are within the interiors of simplexes (i.e., the Markov matrix is ergodic), this stationary distribution is unique and described as pst(x,y). This stationary distribution satisfies pst =Mpst. We also denote each player’s expected payoff in the stationary distribution as ust(x,y) = pst · u and vst(x,y) = pst · v. The goal of learning in the multi-memory game is to search for the Nash equilibrium, denoted by (x∗,y∗), where their payoffs are maximized as

$$
{\left\{\begin{array}{ll}{\mathbf{x}^{*}\in\operatorname{argmax}_{\mathbf{x}}u^{st}(\mathbf{x},\mathbf{y}^{*})}\\ {\mathbf{y}^{*}\in\operatorname{argmax}_{\mathbf{y}}v^{st}(\mathbf{x}^{*},\mathbf{y})}\end{array}\right.}
$$

.

(4)

Here, ust and vst are complex non-linear functions for high-dimensional variables of (x,y). This Nash equilibrium is difficult to find in general.