3. Proposed Method

3.1. Background and Basics

Before describing our imaging method in Section 3.2, we mention two previous imaging methods that use a coded aperture with a frame-based camera (no events available).

Coded-aperture imaging [10, 14, 22, 30, 38]: To opti- cally encode a light field L along the viewpoint dimension, a sequence of light-attenuating patterns, a(1), . . . , a(N), where a(n)u,v ∈ [0, 1] and u, v ∈ {1, . . . , 8}, is placed at the aperture plane. Each pixel under the n-th coding pattern is described as the weighted sum of the light rays over (u, v): ∑

$$
I_{x,y}^{(n)}=\sum_{u,v}a_{u,v}^{(n)}L_{x,y,u,v}.
$$

(1)

The light field L is computationally reconstructed from N images taken with different coding patterns, I(1), . . . , I(N). The images under different coding patterns would have parallax with each other, which is essential for 3-D/light- field reconstruction. As demonstrated in previous stud- ies [10, 14, 42], a relatively small N (e.g. N = 4) is sufficient for accurate reconstruction. However, since N (more than one) images should be acquired in sequence, the lengthy measurement time remains an issue.

Joint aperture-exposure coding [28, 41, 42, 46]: This is an advanced form of coded-aperture imaging; N coding patterns are applied to both the aperture and imaging planes synchronously during a single exposure. The coding pat- terns for the imaging plane are described as p(1), . . . , p(N), where p(n)x,y ∈ {0, 1}. The imaging process is described as

24925

∑ ∑

$$
I_{x,y}=\sum_{n}\sum_{u,v}a_{u,v}^{(n)}\,p_{x,y}^{(n)}\,L_{x,y,u,v}.
$$

(2)

While the light field L can be reconstructed from a single observed image alone, the increased complexity of the cod- ing scheme (Eq. (2)) makes its hardware implementation very difficult.

3.2. Combining Coded Aperture and Events

As shown in Fig. 1, our method uses a coded aperture with an event camera that can obtain both the image frames and events simultaneously (e.g., DAVIS 346 camera). Similar to the baseline coded-aperture imaging method (Eq. (1)), we applyN aperture-coding patterns (a(1), . . . , a(N)) in se- quence. However, we do so in a single exposure for an image frame. Therefore, we do not directly observe indi- vidual coded-aperture images, I(1), . . . , I(N), but we have their sum as the image frame: ∑

$$
\bar{I}_{x,y}=\sum_{n}I_{x,y}^{(n)}.
$$

(3)

The camera also measures the events at each pixel (x, y) asynchronously during the exposure. We denote an event occurring at time t as ex,y,t ∈ {+1,−1}, where the pos- itive/negative signs correspond to the increase/decrease in the intensity at pixel (x, y). Since the target scene is assumed static, the events are caused exclusively by the change in the coding patterns. Although the pattern changes instantly, the camera responds gradually in a very short but no-zero transient time. We denote the transient time be- tween a(n) and a(n+1) as T (n,n+1). We sum the events dur- ing the transient time to obtain an event stack as ∑

$$
E_{x,y}^{(n,n+1)}=\sum_{t\,\in\,T^{(n,n+1)}}e_{x,y,t}.
$$

(4)

As shown in Fig. 1, E(n,n+1) includes the parallax infor- mation between I(n) and I(n+1), which is essential for 3-D reconstruction. Our goal is to reconstruct the original light field L from a single image frame Ī and (N − 1) event stacks, E(1,2), . . . , E(N−1,N), all of which are measured during a single exposure.

Quasi-equivalence. We theoretically relate our imag- ing method to the baseline coded-aperture imaging method. The camera records an event at time t when the intensity change (in log scale) exceeds a contrast threshold τ as

$$
|\log(I_{x,y,t})-\log(I_{x,y,t_{0}})|>\tau
$$

(5)

where t0 is the time when the previous event was recorded. On the basis of Eq. (5), we derive an approximate relation

48, 52, 54], with which the camera-side optical elements and the computational algorithm are jointly optimized in a deep-learning-based framework. Our work is pioneering in applying deep-optics to event cameras.