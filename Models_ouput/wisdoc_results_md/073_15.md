which shows that X’s payoff in the stationary state is u∗, regardless of Y’s strategy y.

Below, we show that if X uses another strategy x 6= x∗1, there always is Y’s strategy such that vst > ∗ v∗ ⇔ ust < u∗. As X’s non-equilibrium strategy, we assume the case x1 6= x representatively. Then, Y’s strategy y = y∗1+ dy1e1 with sufficiently small dy1 satisfies  

$$
\boldsymbol{p}^{\mathrm{st}}=\left(\begin{array}{llll}{x_{1}(y^{*}+\mathrm{d}y_{1})}&{x_{2}y^{*}}&{x_{3}y^{*}}&{x_{4}y^{*}}\\ {x_{1}(\tilde{y}^{*}-\mathrm{d}y_{1})}&{x_{2}\tilde{y}^{*}}&{x_{3}\tilde{y}^{*}}&{x_{4}\tilde{y}^{*}}\\ {\tilde{x}_{1}(y^{*}+\mathrm{d}y_{1})}&{\tilde{x}_{2}y^{*}}&{\tilde{x}_{3}y^{*}}&{\tilde{x}_{4}y^{*}}\\ {\tilde{x}_{1}(\tilde{y}^{*}-\mathrm{d}y_{1})}&{\tilde{x}_{2}\tilde{y}^{*}}&{\tilde{x}_{3}\tilde{y}^{*}}&{\tilde{x}_{4}\tilde{y}^{*}}\end{array}\right)\boldsymbol{p}^{\mathrm{st}}.
$$

(A27)

k) In this equation, we approximate pst ≃ pst(0) + pst(1), where pst(k) describes the O((dy1) term in pst. We can derive these 0-th and 1-st order terms by comparing the left-hand and right-side of this equation. Here, the 0-th order term satisfies pst(0) · u = u∗, which means that the term does not contribute to the deviation from the Nash equilibrium payoff. On the other hand, the 1-st order term gives

$$
\boldsymbol{p}^{\mathrm{st}(1)}=p_{1}^{\mathrm{st}(0)}\mathrm{d}y_{1}(+x_{1},-x_{1},+\tilde{x}_{1},-\tilde{x}_{1})^{\mathrm{T}}
$$

$$
\Rightarrow v^{\mathrm{st}(1)}=p_{1}^{\mathrm{st}(0)}\mathrm{d}y_{1}\underbrace{(v_{1}-v_{2}-v_{3}+v_{4})}_{=\mathbf{v}\cdot\mathbf{v}_{1}\neq0}(x_{1}-x^{*}).
$$

(A28) (A29)

st(1) Here, we use 1z := (+1,−1,−1,+1). Thus, in the leading order, v > v∗ ⇔ ust(1) < u∗ holds by taking ∗) ∗) dy1 > 0 if v · 1z(x1 − x > 0, while by taking dy1 < 0 if v · 1z(x1 − x < 0. In other words, X’s minimax strategy is x = x∗1. Similarly, we can prove that Y’s minimax strategy is y = y∗1. Thus, the Nash equilibrium is given by (x,y) = (x∗1, y∗1).

B Analysis of Learning Dynamics B.1 Simpler MMGA for Two-action Games

This section is concerned with the contents in Section 4.2 in the main manuscript.

Especially in two-action games, we can use the formulation of Assumption 1 in the main manuscript. By replacing the strategies (x,y) by (x,y), we can formulation another simpler algorithm of MMGA as

Algorithm A1 Discretized MMGA for two-action

Input: η, γ

1: for t = 0, 1, 2, · · · do for i = 1, 2, . . . , |S| do x′ ← x ′ x′i ← x + γ i ust(x′,y)− ust(x,y) ∆i ← (1− xi) γ end for

for i = 1, 2, . . . , |S| do xi ← xi(1 + η∆i) end for x← Norm(x) 10: end for 11:

There is a major difference between the original and simpler MMGAs in lines 4 and 5. The equivalence