\begin{tabular}{l|l|l|l|l} \hline \bf Action & \bf Id & \bf Condition & \bf Template & \bf Example \\ \hline \multirow{6}{*}{\rotatebox[origin=c]{90}{\textit{TAPE}}} & 1 & \texttt{(objekt\_auto:sep\_austat $\neq$ NULL) $\wedge$ (\texttt{objekt\_attr:sep} $\neq$ $\alpha$)} & \texttt{Top {[\texttt{objekt}],\texttt{objekt}]}} & \texttt{Top$\uparrow$ ``OK'' button} \\ \cline{2-5} & 2 & \texttt{(objekt\_auto:sep\_austat $\neq$ NULL) $\wedge$ ($\lambda$ < \texttt{objekt\_attr:sep} < $\alpha$)} & \texttt{Top {[\texttt{objekt}],\texttt{objekt}] $\wedge$ [\texttt{objekt\_attr:sep}]}} & \texttt{Top$\uparrow$ ``menu'' icon : $\uparrow$ left \\[-0.2em]``-[object]$\rightarrow$[document]$\uparrow$} \\ & 3 & \texttt{([objekt\_auto:sep\_austat] == NULL) $\wedge$ (\texttt{objekt\_attr:sep} $\neq$ $\beta$)} & \texttt{Top {[\texttt{objekt}],\texttt{objekt}] $\wedge$ [\texttt{objekt\_attr:sep}] $\nrightarrow$ [\texttt{objekt}],\texttt{document}]}} & \texttt{Top$\uparrow$ the clickbox $\rightarrow$ next to ``Dark mode''} \\ \cline{2-5} & 4 & \texttt{objekt$\neq$ NULL} & \texttt{Scroll {[}\texttt{direction} \texttt{]} of the screen to {[} \texttt{objekt} ]} & \texttt{Scrol down half off the screen} \\ \cline{2-5} & 5 & \texttt{objekt$\nrightarrow$ NULL} & \texttt{Scroll {[}\texttt{direction} \texttt{]}$\overline{{\texttt{[direction]}}}$ of the screen} & \texttt{Scrol up a quarter of the screen} \\ \hline \multirow{3}{*}{\rotatebox[origin=c]{90}{\textit{INPUT}}} & 6 & \texttt{([\texttt{objekt}$_{u}$,$\texttt{objekt}$$_{d}$] $\neq$ NULL) $\wedge$ (\texttt{objekt$_{u}$}, \texttt{objekt$_{d}$} $\geq$ $\alpha$)} & \texttt{Input ``100'' in the ``Amount''} & \texttt{Inputbox:} \\ & 7 & \texttt{([\texttt{objekt}$_{u}$,$\texttt{objekt}$$_{d}$] $\neq$ NULL) $\wedge$ (\texttt{objekt$_{u}$}, \texttt{objekt$_{d}$} $\narrow$ $\alpha$)} & \texttt{Input ``100'' in the ``Amount''} & \texttt{Inputbox:} \\ \cline{2-5} & 8 & \texttt{([\texttt{objekt}$_{u}$,$\texttt{objekt}$$_{d}$] = NULL) $\wedge$ (\texttt{objekt$_{u}$}, \texttt{objekt$_{d}$} $\geq$ $\alpha$)} & \texttt{Top {[}\texttt{objekt}],\texttt{objekt} ]$\geq$ ``Dark mode''} & \texttt{Top$\uparrow$ ``-[document]$\rightarrow$[document]$\uparrow$} \\ \hline \end{tabular}

TABLE I: Description template, where “obj” and “nbr” denote the GUI element and its neighbor, α and β denote high- and low-confidence element.

Fig. 7: Example of GUI understanding.

2) Subtitle Creation: The main instruction of interest is to create a clear and concise subtitle description based on{ action, object . The global GUI information is further used to complement the description by position, relationship Based on the action obtained in Section II-B, the attribute of object inferred in Section II-C, and the corresponding GUI element information retrieved in Section II-D1, we propose description templates for TAP, SCROLL, INPUT, respectively. A summary of description templates can be seen in Table I.

For TAP action, the goal of the description should be clear

and concise, e.g., tap “OK” button. However, we find that this simple description may not articulate all TAP actions due to two reasons. First, the text and caption of object are prone to errors or undetected, as the OCR-obtained text and the caption- obtained annotation are not 100% accurate. Second, there may be multiple objects with the same text on the GUI. To resolve this, we set up an object confidence value objconfid as:

$$
obj_{\textit{confid}}=\left\{\!\!\!\begin{array}{ll}{\textit{OCR}_{\textit{confid}}}&{\mathrm{if~}\textit{obj}_{\textit{text}}\mathrm{\ is~unique~in~GUI}}\\ {0}&{\mathrm{otherwise}}\end{array}\right.
$$

(2)

where OCRconfid denotes the confidence predicted by OCR. Note that the confidence value of icon object is calculated likewise by captioning. The smaller the confidence value, the less intuitive the object is. Therefore, only the object with the highest confidence value (objconfid > α) will apply the simplest and most straightforward description (Template 1), otherwise, we add the context of absolute position to help locate the object (Template 2). For the object whose text is not detected or recognized with low confidence, we leverage the context of its neighbor to help locate the target object (Template 3), e.g., tap the checkbox next to “Dark Mode”.

It is easy to describe a SCROLL action by its scrolling direction and offset (Template 5), e.g., scroll up a quarter of the screen. However, such an offset description is not precise and intuitive. To address this, if a new element with text appears by scrolling, we add this context to help describe where to scroll to (Template 4), e.g., scroll down half of the screen to “Advanced Setting”.

The description of INPUT is similar to TAP. For the high-confidence object with text (Template 6), it generates: Input [text] in the [objtext] edittext. Different from the TAP descriptions, we do not apply the context of absolute position to help locate the low-confidence object. This is because the objects are gathering at the top when the keyboard pops up, so the absolute positioning may not help. Instead, we use the relative position of neighbor to describe the input object of which text is not detected or recognized with low confidence (Template 7), e.g., Input “John” in the edittext below “Name”.

After generating the natural language description for each action clip, we embed the description into the recording as subtitles as shown in Fig. 6. In detail, we create the subtitles by using the Wand image annotation library [42] and synchronize the subtitle display at the beginning of each action clip.