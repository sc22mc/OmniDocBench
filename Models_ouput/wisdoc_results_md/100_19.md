Published as a conference paper at ICLR 2024

While GPT3.5 with BoT may initially fall behind GPT-4 CoT, leveraging GPT-4 as the evaluator and analyzer to generate experience allows GPT-3.5 BoT (GPT-4) to outperformGPT-4 Complex CoT. With the GPT3.5, which has less capacity than GPT4, as the LLM, the solving rate obtained by BoT is at least 7.7% (on Algebra) lower than GPT4 ComplexCoT. It is evident that when less powerful LLMs produce lower-quality trial-and-error analyses, the BoT is unable to outperform GPT4 ComplexCoT. Thus, after using the GPT4 in the experience generation part while GPT3.5 is only used to generate reasoning steps, GPT3.5 BoT (GPT4) shows a significant improvement in all categories, leading to a solving rate of 55.8%, which outperforms GPT4 ComplexCoT by 5.5% and is even 1.9% higher than the current state-of-the-art GPT4 PHP+ComplexCoT. These observations further demonstrate that the accumulation of experience over iterations in the prompt constitutes the primary factor contributing to the success of the BoT framework.

G REASONING RESULTS OF “GAME OF 24”

First, in Table 5 - Table 9, we present the detailed prompts that BoT used during the reasoning process, thus providing a comprehensive understanding of what BoT does within each iteration. Then, starting from Table 10, we show some exact examples containing the whole reasoning process of BoT. Following the basic settings shown in the experiment section, these experiments are obtained using BoT with the GPT-3.5-turbo model.

Table 5: Reasoning steps generated by gpt-3.5-turbo when no experience is included in the input prompt. We first let the model generate one step of reasoning five times to check the diversity and then present the final reasoning chain after finishing the first iteration of BoT.

\begin{tabular}{l} \multicolumn{1}{c}{\bfseries Prompt of Game of 24 without experience} \\ \multicolumn{1}{c}{In the game of 24, you are given four numbers, and the goal is to use basic arithmetic operations (+, -, *, /) to combine these numbers and obtain a result of 24. You \\ can only use each number once, and parentheses can be used to change the order of operations. \\ \multicolumn{1}{c}{Analysis for each of step.} \hfill Step . Current set: , Selected two numbers: , Operation: , Computed new number: , Remaining numbers: , New set: . \\ The given four numbers are: $1 \; 14 \; 6$. \\ \multicolumn{1}{c}{Let's think step by step.} \\ \multicolumn{1}{c}{\bfseries Recall historical reasoning experience (Ignore when experience is empty):}\\ \hfill \multicolumn{1}{c}{---------------------------------}\\ \multicolumn{1}{c}{Prediction by analysis and conclusion in the experience to avoid making similar mistakes by following the advice}\\ \multicolumn{1}{c}{Below is a list of ordered reasoning steps, accompanied by their evaluated scores (A higher score means the reasoning step is more likely to complete the task: )}\\ \hfill \multicolumn{1}{c}{---------}\\ \multicolumn{1}{c}{Eased on listed reasoning steps only within the above ``---------'' (i.e., Not the ones in the experience block), please make one step of reasoning to generate only one\\ \multicolumn{1}{c}{subsequent possible reasoning step.}\\ \hfill \multicolumn{1}{c}{\bfseries Five responses from the gpt-3.5-turbino} \\ \hfill Step 1. Current set: $1 \; 14 \; 6$, Selected two numbers: $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $1$, $

H REASONING RESULTS OF “GSM8K”

BoT uses similar basic prompts and the specific format as shown in Table 5 - Table 9. Only the task prompt will be changed, as shown in Table 15. Then, starting from Table 16, we show some exact examples containing the whole reasoning process of BoT. Following the basic settings shown in the experiment section, these experiments are obtained using BoT with the GPT-3.5-turbo model.