Table 1. Comparison of the features of conventional methods and our proposed method

\begin{tabular}{cccccccc} \hline\noalign{\smallskip} Method & DL$^{\dagger}$ & Heatunap$^{\dagger}$ & Manhattan$^{\dagger}$ & Pan & Tilt \& Roll & Distortion:& Projection \\ \noalign{\smallskip} \hline \noalign{\smallskip} Non-Manhattan world \\ Lopez-Antequra \textit{et al.} [\textcolor{gray}{33}] & CVPR'19 & $\checkmark$ & & & $\checkmark$ & $\checkmark$ & Perspective\\ Wakis and Yamaguchi [\textcolor{gray}{52}] & ICCW'21 & $\checkmark$ & & & $\checkmark$ & $\checkmark$ & Equilateral angle\\ Wakis \textit{et al.} [\textcolor{gray}{53}] & ECCV'22 & $\checkmark$ & & & $\checkmark$ & $\checkmark$ & Generic camera [\textcolor{gray}{53}]\\ \hline \noalign{\smallskip} Manhattan world \\ Wildenberg \textit{et al.} [\textcolor{gray}{57}] & BMVC'13 & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & Division model [\textcolor{gray}{14}]\\ Antunes \textit{et al.} [\textcolor{gray}{7}] & CVPR'17 & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & Division model [\textcolor{gray}{14}]\\ Prits \textit{et al.} [\textcolor{gray}{41}] & CVPR'18 & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & Division model [\textcolor{gray}{14}]\\ Lochten \textit{et al.} [\textcolor{gray}{32}] & WACV'21 & & & $\checkmark$ & $\checkmark$ & $\checkmark$ & Division model [\textcolor{gray}{14}]\\ Curs & & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ & Generic camera [\textcolor{gray}{33}]\\ \hline \end{tabular}

DL is learning-based methods; Heatmap is heatmap regression; Manhattan is based on the Manhattan world for world coordinates

objects because these methods need to detect many arcs to estimate the VPs. Therefore, city scenes in which sky or street trees dominate the images degrade the performance of geometry-based methods.

On the basis of the observations above, to achieve ac- curate and robust estimation, we propose a learning-based calibration method that estimates extrinsics (pan, tilt, and roll angles), focal length, and a distortion coefficient simul- taneously from a single image in Figure 1. Our heatmap re- gression estimates each direction using labeled image coor- dinates to distinguish the four directions of a road intersec- tion in a Manhattan world. Furthermore, we introduce addi- tional geometric keypoints, called auxiliary diagonal points (ADPs), to compensate for the lack of VPs in each image.

To investigate the effectiveness of the proposed methods, we conducted extensive experiments on three large-scale datasets [9, 38, 64] as well as off-the-shelf cameras. This evaluation demonstrated that our method notably outper- forms conventional geometry-based [32, 41] and learning- based [33, 52, 53] methods. The major contributions of our study are summarized as follows:

• We propose a heatmap-based VP estimator for recovering the rotation from a single image to achieve higher accu- racy and robustness than geometry-based methods using arc detectors.

• We introduce auxiliary diagonal points with an optimal 3D arrangement based on the spatial uniformity of regular octahedron groups to address the lack of VPs in an image.

2. Related work

Camera model. For geometric tasks, camera calibration estimates the parameters in a camera model. This model expresses a mapping from world coordinates p̃ to image coordinates ũ in homogeneous coordinates. This mapping is conducted using extrinsic and intrinsic parameters. Ex- trinsic parameters [R | t ] consist of a rotation matrix R and a translation vector t to represent the relation between the origins of the camera coordinates and Manhattan world coordinates (or other world coordinates). The intrinsic pa- rameters are distortion γ, image sensor pitch (du, dv), and a

principal point (cu, cv). The subscripts u and v indicate the horizontal and vertical directions, respectively. The map-   ping is formulated as

$$
\tilde{\mathbf{u}}=\left[\begin{array}{ccc}{\gamma/d_{u}}&{0}&{c_{u}}\\ {0}&{\gamma/d_{v}}&{c_{v}}\\ {0}&{0}&{1}\end{array}\right]\left[\,\mathbf{R}\mid\mathbf{t}\,\right]\tilde{\mathbf{p}}.
$$

(1)

Kannala and Brandt [16] proposed the generic camera model, which includes fisheye lens cameras and is given by

$$
\gamma=\tilde{k}_{1}\eta+\tilde{k}_{2}\eta^{3}+\cdots,
$$

(2)

where k̃1, k̃2, . . . are distortion coefficients and η is an inci- dent angle. Wakai et al. [53] proposed an alternative generic camera model for learning-based methods, expressed as

$$
\gamma=f\cdot(\eta+k_{1}\eta^{3}),
$$

(3)

where f is focal length and k1 is a distortion coefficient. Al- though Equation (3) is only a third-order polynomial with respect to η, the model can practically express fisheye pro- jection with sub-pixel error [53].

Manhattan world. Coughlan et al. [12] proposed the Manhattan world for human navigation on the basis of the prior over edge models. The Manhattan world assumption regards the world as consisting of grid-shaped roads; that is, two of the three orthogonal coordinate axes lie along a crossroads, and the remaining axis is vertical. Given the Manhattan world OM -XMYMZM in Figure 2, camera an- gles are defined as a rotation matrix R that is compatible with pan, tilt, and roll angles. In this paper, we ignore the relations between the extrinsics of the camera and the body of cars, drones, or robots because these relations can be de- termined using designed values or calibration. Therefore, the task of camera calibration is to determine camera angles of a 3D-rotated camera in a Manhattan world.

Camera calibration. Perspective camera calibration methods in the Manhattan world have been proposed for Hough-transform-based methods [43, 44] and VP-based methods [8, 11, 18, 19, 22–24, 30, 35, 45, 46, 48, 49, 58, 63]. However, these methods address only narrow FOV cameras without distortion.

11885