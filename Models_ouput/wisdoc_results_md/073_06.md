Assumption 1 (One-memory two-action zero-sum game). We assume a two-action (i.e., A = {a1, a2} and B = {b1, b2}), one-memory (i.e., s = (a1b1, a1b2, a2b1, a2b2)), and zero-sum game (i.e., v = −u). In particular, we discuss zero-sum games where both u1 and u4 are smaller or larger than both u2 and u3.

Under Assumption 1, we exclude uninteresting zero-sum payoff matrices that the Nash equilibrium exists as a set of pure strategies because the learning dynamics trivially converge to such pure strategies. The condition that both u1 and u4 are smaller or larger than both u2 and u3 is necessary and sufficient for the existence of no dominant pure strategy.

In the rest of this paper, we use a vector notation for strategies of X and Y; x := {xi}i=1,...,4 and a1|si b1|si b2|si y := {yi}i=1,...,4 as xi := x and yi := y . Indeed, xa2|si = 1− xi and y = 1− yi hold.

Theorem 4 (Uniqueness of Nash equilibrium). Under Assumption 1, the unique Nash equilibrium of this ∗, game is (xi, yi) = (x y∗) for all i as

$$
x^{*}=\frac{-u_{3}+u_{4}}{u_{1}-u_{2}-u_{3}+u_{4}},\ y^{*}=\frac{-u_{2}+u_{4}}{u_{1}-u_{2}-u_{3}+u_{4}}.
$$

(11)

Proof Sketch. Let us prove that X’s strategy in Nash equilibrium is uniquely x = x∗1. First, we define u∗ and v∗ as X’s and Y’s payoffs in the Nash equilibrium in the zero-memory game. If x = x∗1, X’s expected payoff is ust = u∗, regardless of Y’s strategy y. Second, we consider that X uses another strategy x 6= x∗1. Then, there is Y’s strategy such that vst > v∗ ⇔ ust < u∗. Thus, X’s minimax strategy is uniquely x = x∗1, completing the proof. See Appendix A.4 for the full proof.

Figure 2: Multi-memory learning dynamics near the Nash equilibrium in the penny-matching game. In the upper six panels, colored lines indicate the time series of δi (X’s strategy). The solid (resp. broken) lines are approximated (resp. experimental) trajectories of learning dynamics. From the top, the trajectories are predicted by approximations up to the first, second, and third orders. The bottom panel shows the errors between the approximated and experimental trajectories.

Regarding Theorem 4, X (Y) chooses each action in the same probability independent of the last state. Here, they do not utilize their memory. Thus, note that in this sense, the Nash equilibrium is the same as