\begin{tabular}{l|c|cccc|cccc|cccc} \toprule \multirow{2}{*}{\textbf{Benchmark}} & \textbf{Document} & \multicolumn{4}{c}{\textbf{Annotation Type}} & \multicolumn{4}{c}{\textbf{Single-Task Eval}} & \multicolumn{4}{c}{\textbf{End-to-End Eval}}\\ & \textbf{Domain1} & \textsc{BBaX} & \textsc{Text} & \textsc{Tab} & \textsc{Formula} & \textsc{Attribute} & \textsc{OCR} & \textsc{DLA} & \textsc{TR} & \textsc{MFR} & \textsc{OCR} & \textsc{TR} & \textsc{MFR} & \textsc{ROC} \\ \midrule \multicolumn{13}{l}{\textbf{\textit{Single-Task Eval.}}}\\ \multicolumn{13}{l}{Robust Reading~\cite{schwartz2014read}}\\ & 1 & \ding{52} & \ding{52} & & & \ding{52} & & & & & & & \\ & 1, 1 & \ding{52} & & & & & \ding{52} & & & & & & \\ & 1, 6 & \ding{52} & & & & & & \ding{52} & & & & & \\ \midrule & 1, 1 & & \ding{52} & & & & & & \ding{52} & & & & \\ & 1 & \ding{52} & \ding{52} & & & & & & & \ding{52} & & & \\ & 1 & \ding{52} & \ding{52} & & & & & & & & \ding{52} & & \\ \midrule & 1,27 & & & \ding{52} & & & & & & & & & \\ & 1 & & & & \ding{52} & & & & \ding{52} & & & & \\ \midrule \multicolumn{13}{l}{\textbf{\textit{End-to-End Eval.}}}\\ \midrule \multicolumn{13}{l}{\texttt{Fox~\cite{fox2014high}}}&\\ & 2 & \ding{52} & \ding{52} & & & \ding{52} & & & & & \ding{52} & & \\ & 1 & & \ding{52} & \ding{52} & \ding{52} & \ding{52} & & & & & \ding{52} & \ding{52} & \ding{52}\\ & 2 & & \ding{52} & \ding{52} & \ding{52} & & & & \ding{52} & & \ding{52} & \ding{52} & \\ \midrule \multicolumn{13}{l}{\textbf{\texttt{OrnamentDocBench}}}\\ & 9 & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52} & \ding{52}\\ \bottomrule \end{tabular}

Table 1. A Comparison between OmniDocBench and existing benchmarks. BBox: Bounding boxes. Text: Text in Unicode. Table: Table in LaTeX/HTML/Markdown. Formula: Formula in LaTeX. Attributes: Page- and BBox-Level Attributes. OCR: Optical Character Recognition; DLA: Document Layout Analysis; TR: Table Recognition;MFR: Math Formula Recognition; ROD: Reading Order Detection

this sense, such methods can utilize different expert models to address each specific task. Marker [34] integrates open- source models to parse documents into structured formats such as Markdown, JSON, and HTML. To get higher ac- curacy, an optional LLM-enabled version can also be inte- grated to merge tables across pages, handle inline math, and so on. Similarly, MinerU [42] first utilizes a layout detec- tion model to segment the document page into different re- gions, then applies task-specific models for corresponding regions. Finally, it outputs the complete content in Mark- down format with a reading order algorithm. By leveraging lightweight models and parallelized operations, pipeline- based methods can achieve efficient parsing speeds.

2.2. VLM-based Document Content Extraction

Document understanding and optical character recognition (OCR) are crucial tasks for evaluating the perception capa- bilities of vision-language models (VLMs). By incorporat- ing extensive OCR corpus into the pretraining stage, VLMs like GPT4o [2] and Qwen2-VL [3] have demonstrated com- parable performance in document content extraction tasks. Unlike pipeline-based methods, VLMs perform document parsing in an end-to-end manner. Furthermore, without re- quiring specialized data fine-tuning, these models are able to deal with diverse and even unseen document types for their generalization capabilities.

To integrate the efficiency of lightweight models and the generalizability of VLMs, many works [7, 14, 29, 32, 45, 46] have focus on training specialized end-to-end expert models for document parsing. These VLM-driven models excel at comprehending both visual layouts and textual con- tents, balancing a trade-off between accuracy and efficiency.

Document content extraction requires the ability to under- stand document layouts and recognize various types of con- tent. However, current benchmarks fall short of a compre- hensive page-level evaluation, as they focus solely on evalu- ating the model’s performance on module-level recognition. PubLayNet [49] and concurrent benchmarks [9, 26, 35] spe- cialize in evaluating a model’s ability to detect document page layouts. OCRBench [31] proposes five OCR-related tasks with a greater emphasis on evaluating the model’s vi- sual understanding and reasoning capabilities. Only line- level assessments are provided for text recognition and handwritten mathematical expression recognition (HMER). Similarly, single-module benchmarks [20, 26, 40, 54] disen- tangle the task into different dimensions and focus narrowly on specific parts. Such paradigm overlooks the importance of structural and semantic information like the reading or- der and fails to evaluate the model’s overall ability when processing the full-page documents as a whole.

Page-level benchmarks have been proposed alongside some recent VLM-driven expert models [7, 29, 45]. How- ever, the robustness of these benchmarks is compromised by limitations in data size, language, document type, and an- notation. For example, Nougat [7] evaluates models using only printed English documents collected from arXiv while the page-level benchmark introduced by GOT-OCR [45] consists of only 90 pages of Chinese and English docu- ments in total. Commonly-seen document types like hand- written notes, newspapers, and exam papers are further ne- glected. Lacking detailed annotations, the benchmarks can only conduct naive evaluation between the full-page results of Ground Truths and predictions without special handling for different output formats and specialized metrics for dif- ferent content types. The evaluation of the model perfor- mance can be severely biased due to limited document do- mains, unaligned output format and mismatched metrics. Therefore, there is an urgent need for a more finely anno- tated, diverse, and reasonable page-level document content extraction benchmark.

24840

2.3. Benchmarks for Document Content Extraction