QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning

\begin{tabular}{llc||c|c|c|c} \hline \hline \multicolumn{2}{c}{\cellcolor[gray]{0.9} \scriptsize{Method}} & \cellcolor[gray]{0.9} \scriptsize{Training Mode} & \cellcolor[gray]{0.9} \scriptsize{Accuracy} \scriptsize{($\sim$ Rank@1)} & \cellcolor[gray]{0.9} \scriptsize{Rank@5} & \cellcolor[gray]{0.9} \scriptsize{Example-F1} & \cellcolor[gray]{0.9} \scriptsize{F1-Score} \\ \hline \hline \multicolumn{2}{c}{\cellcolor[gray]{0.9}\scriptsize{Location Resoning}}\\ \hline \scriptsize{1} & \scriptsize{ResNet-50} \scriptsize{(He et al., 2016)} & \scriptsize{Supervised} & \scriptsize{3.18\%} & \scriptsize{9.82\%} & \scriptsize{22.19\%} & \scriptsize{2.27\%}\\ \scriptsize{2} & \scriptsize{Swin-T} \scriptsize{(Liu et al., 2021)} & \scriptsize{Supervised} & \scriptsize{6.70\%} & \scriptsize{17.07\%} & \scriptsize{33.56\%} & \scriptsize{5.02\%}\\ \scriptsize{3} & \scriptsize{CLIP} \scriptsize{(Rafford et al., 2021)} & \scriptsize{Zero-Shot} & \scriptsize{111.11\%} & \scriptsize{27.85\%} & \scriptsize{44.96\%} & \scriptsize{9.74\%}\\ \scriptsize{4} & \scriptsize{CLIP+} \scriptsize{(Fu et al., 2022)} & \scriptsize{Fine-tune} & \scriptsize{15.72\%} & \scriptsize{37.13\%} & \scriptsize{19.74\%} & \scriptsize{15.82\%}\\ \scriptsize{5} & \scriptsize{CLIP-Seg} \scriptsize{(Fu et al., 2022)} & \scriptsize{Fine-tune} & \scriptsize{16.45\%} & \scriptsize{37.48\%} & \scriptsize{50.52\%} & \scriptsize{14.63\%}\\ \scriptsize{6} & \scriptsize{\textbf{QR-CLIP}} \scriptsize{(Ours)} & \scriptsize{Fine-tune} & \scriptsize{\textbf{19.31\%}} & \scriptsize{\textbf{38.78\%}} & \scriptsize{\textbf{50.96\%}} & \scriptsize{\textbf{17.70\%}}\\ \hline \multicolumn{2}{c}{\cellcolor[gray]{0.9}\scriptsize{\textit{Improvements} \scriptsize{(AVG: 10.66\%)}}} & \scriptsize{\textbf{+17.91\%}} & \scriptsize{\textbf{+3.47\%}} & \scriptsize{\textbf{+0.87\%}} & \scriptsize{\textbf{+20.98\%}} & \scriptsize{\textbf{+20.98\%}} \\ \hline\hline \multicolumn{2}{c}{\cellcolor[gray]{0.9}\scriptsize{Time Resoning}}\\ \hline \scriptsize{7} & \scriptsize{ResNet-50} \scriptsize{(He et al., 2016)} & \scriptsize{Supervised} & \scriptsize{0.84\%} & \scriptsize{5.14\%} & \scriptsize{39.99\%} & \scriptsize{0.46\%}\\ \scriptsize{8} & \scriptsize{Swin-T} \scriptsize{(Liu et al., 2021)} & \scriptsize{Supervised} & \scriptsize{0.97\%} & \scriptsize{5.53\%} & \scriptsize{13.95\%} & \scriptsize{0.72\%}\\ \scriptsize{9} & \scriptsize{CLIP} \scriptsize{(Rafford et al., 2021)} & \scriptsize{Zero-Shot} & \scriptsize{0.46\%} & \scriptsize{2.42\%} & \scriptsize{39.90\%} & \scriptsize{0.25\%}\\ \scriptsize{10} & \scriptsize{CLIP+} \scriptsize{(Fu et al., 2022)} & \scriptsize{Fine-tune} & \scriptsize{1.00\%} & \scriptsize{3.07\%} & \scriptsize{43.09\%} & \scriptsize{0.54\%}\\ \scriptsize{11} & \scriptsize{CLIP-Seg} \scriptsize{(Fu et al., 2022)} & \scriptsize{Fine-tune} & \scriptsize{0.92\%} & \scriptsize{3.15\%} & \scriptsize{42.89\%} & \scriptsize{0.71\%}\\ \scriptsize{12} & \scriptsize{\textbf{QR-CLIP}} \scriptsize{(Ours)} & \scriptsize{Fine-tune} & \scriptsize{\textbf{3.53\%}} & \scriptsize{\textbf{10.90\%}} & \scriptsize{\textbf{47.89\%}} & \scriptsize{\textbf{2.01\%}}\\ \hline \multicolumn{2}{c}{\cellcolor[gray]{0.9}\scriptsize{\textit{Improvements} \scriptsize{(AVG: 134.38\%)}}} & \scriptsize{\textbf{+233\%}} & \scriptsize{\textbf{+97.11\%}} & \scriptsize{\textbf{+8.23\%}} & \scriptsize{\textbf{+7.89\%}} & \scriptsize{\textbf{+7.89\%}} \\ \hline \end{tabular}

Table 1. Summary of the performance for different baselines on the image location and time prediction. † means fine-tune the original CLIP (Radford et al., 2021). ‘AVG’: average relative lift.

owki and W are the weights of the CLS] owk and [CLS] q in this place is the addition of weight vision-language featuresW owki × [CLS]owki vi × [CLS]vi ; q is the ground- truth features generated by FGT = Enct(GT ).

Location and Time Reasoning. We use the fused features f used = ∑61(W owk × [CLS]owki vi × [CLS]vi ) as our final features to predict the location and time. The prediction is completed by calculating the similarity between F f used and the candidate location/time embeddings.

We believe that by using the CLIP pre-trained 400M open- world corpus and then fine-tuning it by adding additional

[CLS] with location-and-time-specific data, it can basi- cally reason about meta information. QR-CLIP will then improve its performance by retrieving valuable open-world knowledge and using it as auxiliary cues. Finally, the model balances vision and language embeddings, and by incorpo- rating them into prediction, the model achieves its peak per- formance. The process is related to Horn’s QR rule (Horn, 1984). Also, it mimics a procedure of information spread- ing (Wang et al., 2011): diverse individuals have diverse per- spectives and attitudes regarding the same thing (sec 3.2), but combining them effectively fosters a more profound comprehension (sec 3.3).

4. Experiments

4.1. Training Settings

ing 12,306 instances and evaluate our method using a test set containing 1,644 instances. The OWK dataset is derived from the WIT dataset (Srinivasan et al., 2021). Consider- ing the limited computation resource, we only use 122,408 texts from the 37.5 million entity-rich image-text examples in English Wikipedia that correspond to the countries and years as our open-world knowledge.

Evaluation Metrics. For a fair comparison, we first follow the same evaluation metrics on the TARA benchmark (Fu et al., 2022): Accuracy and Example-F1. Accuracy is cal- culated by comparing the predicted results with the entire labels. Example-F1 is calculated by comparing predictions with hierarchical labels:

$$
\textnormal{Example-F1}=\frac{1}{N}\sum_{i=1}^{N}\frac{2|\textnormal{GT}_{i}\cap\textnormal{Pred}_{i}|}{|\textnormal{GT}_{i}|+|\textnormal{Pred}_{i}|},
$$

(8)

where GTi represents the hierarchical label, and Predi represents the hierarchical prediction. If the entire label is {‘Zurich, Switzerland, Europe’}, the progressive hier- archical labels are the three combinations of true label as {‘Zurich, Switzerland, Europe’}, {‘Switzerland, Eu- rope’5948} and {‘Europe’}. In addition, Rank@5 and F1-Score are utilized to evaluate the performance of the proposed method.

Implementation Details. QR-CLIP is based on CLIP+VIT- B/32 model with an input size of 224× 224, and it is im- plemented on the PyTorch 1.10.1 platform with the Adam optimizer to update the neural network’s weights and biases. The training batch size is 32, and the initial learning rate is 1e− 6. Our model utilizes a pre-trained model and takes hours in the fine-tune process on an NVIDIA RTX 3090 GPU running CUDA 11.7.1.

Dataset. We used two datasets: TARA dataset (Fu et al., 2022) and our collected OWK dataset. TARA dataset in- cludes 15,429 samples. Each sample contains a news picture and the corresponding location, time description. Following the original setup, we train QR-CLIP on a train set contain- Location Reasoning. We compare the results of our QR- CLIP with other methods for location reasoning in Tab. 1. QR-CLIP, achieves accuracy of 19.31% Accuracy (R@1). Meanwhile, its Example-F1 score for the hierarchical labels is 50.96%. All the results clearly show that our method outperforms other methods.