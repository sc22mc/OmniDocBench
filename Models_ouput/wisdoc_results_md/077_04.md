Timewarp: Transferable Acceleration of Molecular Dynamics by Learning Time-Coarsened Dynamics

Algorithm 1 Timewarp MCMC with batched proposals Require: Initial state X0 = (Xp0 , Xv0 ), chain length M , proposal batch size B. m← 0 whilem < M do

Sample X̃1, . . . , X̃B ∼ pθ( · |Xpm) {Batch sample} for b = 1, . . . , B do ε ∼ N (0, I) {Resample auxiliary variables} Xb ← (Xpm, ε)

Sample Ib ∼ Bernoulli(α(Xb, X̃b)) end for

if S := {b : Ib = 1, 1 ≤ b ≤ B} 6= ∅ then a = min(S) {First accepted sample} (Xpm+1, . . . , X p m+a−1)← (Xpm, . . . , Xpm) Xpm+a ← X̃pa m← m+ a else (Xpm+1, . . . , X p m+B)← (Xpm, . . . , Xpm) m← m+B end if end while output Xp0 , . . . X p M

3.5. Fast exploration of the state space without MH

Although the MH correction ensures that Timewarp pro- vides asymptotically unbiased samples, it can lead to much slower exploration of the state space due to the rejected pro- posals. For some of the peptides we consider, the acceptance probabilities are too low to apply Algorithm 1 effectively. Instead, we can apply Timewarp in a simple exploration algorithm, where we ignore the MH correction and accept all proposals with an energy change lower than some cutoff. This allows much faster exploration of the state space, and in Section 6 we show that the algorithm, although technically biased, often leads to qualitatively accurate free energy esti- mates. It also succeeds in discovering all metastable states of a peptide orders of magnitude faster than Algorithm 1 and standard MD. Timewarp applied in exploration mode can be used to efficiently find the metastable states of a new molecule, which could be used, e.g., to provide initialisation states for a subsequent MSM method, although we do not pursue this here. We provide pseudocode for the exploration algorithm in Algorithm 2 in Appendix C.

4. Model architecture

We now describe the architecture of our conditional normal- ising flow fθ(zp, zv;xp(t)), which is shown in Figure 2.

RealNVP coupling flow Our architecture is based on Re- alNVP (Dinh et al., 2017), which consists of a stack of coupling layers which affinely transform subsets of the di- mensions of the latent variable based on the other dimen- sions. Specifically, we transform the position variables based on the auxiliary variables, and vice versa. In the `th coupling layer of the flow, the following transformations are implemented:

$$
z_{\ell+1}^{v}=s_{\ell,\theta}^{v}(\ensuremath{z}_{\ell+1}^{p};x^{p}(t))\odot z_{\ell,\theta}^{v}+t_{\ell,\theta}(\ensuremath{z}_{\ell+1}^{p};x^{p}(t)).
$$

$$
z_{\ell+1}^{p}=s_{\ell,\theta}^{p}(z_{\ell}^{v},x^{p}(t))\odot z_{\ell}^{p}+t_{\ell,\theta}^{p}(z_{\ell}^{v};x^{p}(t)),
$$

(9)

(10)

Going forward, we suppress the coupling layer index `. Here is the element-wise product, and spθ : R3N → R3N is our atom transformer, a neural network based on the trans- former architecture (Vaswani et al., 2017) that takes the auxiliary latent variables zv and the conditioning state x(t) and outputs scaling factors for the position latent variables zp. The function tpθ : R3N → R3N is implemented as an- other atom transformer, which uses zv and x(t) to output a translation of the position latent variables zp. The affine transformations of the position variables (in Equation (9)) are interleaved with similar affine transformations for the auxiliary variables (in Equation (10)). Since the scale and translation factors for the positions depend only on the auxil- iary variables, and vice versa, the Jacobian of the transforma- tion is lower triangular, allowing for efficient computation of the density. The full flow fθ consists ofNcoupling stacked coupling layers, beginning from z ∼ N (0, I) and ending with a sample from pθ(x(t+ τ)|x(t)). This is depicted in Figure 2, Left. Note that there is a skip connection such that the output of the flow predicts the change x(t+ τ)− x(t), rather than outputting x(t+ τ) directly.

Atom transformer We now describe the atom trans- former network. Let xpi (t), z p i , z v i , all elements of R3, de- note respectively the position of atom i in the conditioning state, the position latent variable for atom i, and the aux- iliary latent variable for atom i. To implement an atom transformer which takes zv as input (such as spθ(z v, xp(t)) and tpθ(z v, xp(t)) in Equation (9)), we first concatenate the variables associated with atom i. This leads to a vector ai := [x p i (t), hi, z v i ∈ RH+6, where zpi has been ex- cluded since spθ, t p θ are not allowed to depend on z p. Here hi ∈ RH is a learned embedding vector which depends only on the atom type. The vectors ai are fed into an MLP φin : RH+6 → RD, where D is the feature dimension of the transformer. The vectors φin(a1), . . . , φin(aN ) are then fed into Ntransformer stacked transformer layers. After the transformer layers, they are passed through another atom- wise MLP, φout : RD → R3. The final output is in R3N as required. This is depicted in Figure 2, Middle. When imple- menting svθ and t v θ from Equation (10), a similar procedure is performed on the vector [xpi (t), hi, z p i but now including zpi and excluding z v i . There are two key differences between

the atom transformer and the architecture in Vaswani et al. (2017). First, to maintain permutation equivariance, we do