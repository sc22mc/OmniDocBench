Published as a conference paper at ICLR 2024

\begin{tabular}{l|cc|ccc} Model & Auto-J & GPT-4 & Rank & Auto-J & GPT-4 \\ & Rating & Win-rac & GPT-4 & Auto-J & GPT-4 \\ \hline XwinLM 70B V0.1 & 5.694 & 95.57 & 1 & 1 & 0 \\ LLaMA27 CL0B 70B & 5.678 & 92.66 & 2 & 2 & 0 \\ XwinLM I3B V0.1 & 5.647 & 91.76 & 3 & 3 & 0 \\ OpeVrChar V3 V1.1 & 5.572 & 89.99 & 4 & 8 & 4 \\ WizdumKL 13B V1.2 & 5.557 & 89.17 & 5 & 6 & 1 \\ VeeNa 33B V1.3 & 5.570 & 88.95 & 6 & 5 & -1 \\ \textit{Humpback}LLaMA 70B & 5.498 & 87.94 & 7 & 11 & 4 \\ LLaMA7 CL0V V0.1 & 5.584 & 87.83 & 8 & 4 & -1 \\ OpeRbV1:70B-1.1 & 5.533 & 87.13 & 10 & 7 & -3 \\ OpenCPU V2-W13B & 5.519 & 86.53 & 11 & 9 & -4 \\ Humpback KL 13B V0.1 & 5.518 & 83.71 & 14 & 19 & 5 \\ VeeNa 13B V1.1 & 5.538 & 82.51 & 15 & 18 & 3 \\ OpeRbV1:70B-303V-57.1 & 5.591 & 81.55 & 16 & 17 & -1 \\ LLaMA27 CL0B & 5.518 & 81.09 & 17 & 10 & -7 \\ OpeChar V-13B & 5.572 & 80.87 & 18 & 5 & -3 \\ OpeRbV1:70B-00B-v9 & 5.573 & 79.70 & 20 & 20 & 1 \\ UlaMN 13B & 5.342 & 80.64 & 20 & 22 & 2 \\ OpenCPU8192-13B & 5.429 & 79.54 & 21 & 16 & -3 \\ OpenCPU16-15B & 5.357 & 78.70 & 22 & 21 & -1 \\ OpeRbV1:70B-1.1 & 5.340 & 77.49 & 23 & 73 & 0 \\ Victra7B V1.3 & 5.332 & 76.84 & 24 & 25 & 1 \\ WizardOfCloud 13B & 5.247 & 75.31 & 25 & 32 & 7 \\ LinNaJt & 5.319 & 74.13 & 26 & 26 & 0 \\ arborosus 55B & 5.318 & 73.91 & 27 & 27 & 0 \\ Inadaa3B V1.1 & 5.289 & 73.29 & 28 & 30 & 2 \\ Enigma5B & 5.314 & 71.80 & 79 & 39 & 0 \\ LLaMA27 CL0B & 5.334 & 71.37 & 30 & 24 & -6 \\ VeeNa 13B & 5.314 & 70.43 & 31 & 28 & -3 \\ OpenCPU8192-70b-v6 & 5.314 & 70.36 & 32 & 34 & 2 \\ Rahez-V2:70B & 5.165 & 66.96 & 33 & 38 & 5 \\ LLaMA 33B OASST RLHF & 5.173 & 66.52 & 34 & 33 & 3 \\ Minitaur 13B & 5.210 & 66.02 & 35 & 36 & 1 \\ Enigma 5B & 5.317 & 65.96 & 36 & 74 & -3 \\ NeeDle 2.71B & 5.083 & 65.85 & 39 & 39 & 0 \\ ILLA73B OASST SSTF & 4.985 & 54.97 & 40 & 41 & 1 \\ Inadaa 73B & 5.127 & 52.61 & 41 & 46 & -1 \\ GaGGM2-6B & 4.896 & 47.13 & 45 & 46 & 4 \\ Pythia 12B SSTF & 4.809 & 41.86 & 46 & 47 & 1 \\ Alpaca Farm PPO Sim (CPT-4) 7B & 4.978 & 44.10 & 45 & 42 & -3 \\ Pythia 7B & 1.97-3 & -6.58 & 47 & -3 & 1 \\ Pythia 40B Instruct & 4.934 & 45.71 & 44 & 44 & 0 \\ Alpaca Farm PPO Sim (CPT-4) 7B & 4.978 & 44.10 & 45 & 42 & -3 \\ Pythia 7B & 1.97-3 & -6.58 & 47 & -3 & 1 \\ Pythia 40B Instruct & 4.934 & 45.71 & 44 & 44 & 0 \\ Alpaca Farm PPO Human 7B & 4.907 & 44.10 & 45 & 42 & -3 \\ Pythia 7B & 1.97-3 & -6.58 & 47 & -3 & 1 \\ Pythia 40B Human & 4.934 & 45.71 & 44 & 44 & 0 \\ Alpaca Farm PPO Human 7B & 4.978 & 44.10 & 45 & 42 & -3 \\ Pythia 7B & 1.97-3 & -6.58 & 47 & -3 & 1 \\ Pythia 40B Human & 4.934 & 45.71 & 44 & 44 & 0 \\ \end{tabular}

Table 24: Values and ranking by Auto-J and GPT-4 for open-source LLMs on AlpacaEval. Value of AUTO-J is the model’s average rating on AlpacaEval dataset assigned by AUTO-J in single-response evaluation protocol, and value of GPT-4 is the model’s win-rate against Davinci003 determined by GPT-4 on AlpacaEval dataset. ∆ = RankAuto-J − RankGPT-4