Fig. 2: The overview of CAPdroid.

generate semantic captions for GUI recording. Considering the diversity of touch indicators in the general GUI recordings, a more advanced approach to detect and infer user actions is required.

Fig. 3: An illustration of consecutive frame similarity.

Fig. 4: Examples of keyboard detection.

ert”) “123”)

B. Phase 1: Action Segmentation

A video consists of a sequence of frames to deliver the visual detail of the story for particular scenes. Different from the recognition of discontinuities in the visual-content flow of natural-scene videos, detecting clips in the GUI recording is to infer scenes of user actions that generally display significant changes in the GUIs. To that end, we leverage the similarity of consecutive frames to segment user actions (i.e., TAP, SCROLL, INPUT) from GUI recording.

1) Consecutive Frame Comparison: Inspired by signal pro- cessing [4], [17], we leverage the image processing tech- niques to build a perceptual similarity score for consecutive frame comparisons based on Y-Difference (or Y-Diff). YUV is a color space usually used in video encoding, enabling transmission errors or compression artifacts to be more ef- ficiently masked by the human perception than using a RGB- representation [21], [22]. Y-Diff is the difference in Y (lumi- nance) values of two images in the YUV color space, used as a major input for the human perception of motion [23].

Consider a visual recording , where f0, f1, .., fN−1, fN is the current frame and is the previous frame. fN fN−1

To calculate the Y-Diff of the current frame with the fN

previous fN−1, we first obtain the luminance mask YN−1, YN by splitting the YUV color space converted by the RGB color space. Then, we apply the perceptual comparison metric, SSIM (Structural Similarity Index) [24], to produce a per- pixel similarity value related to the local difference in the average value, the variance, and the correlation of luminances. A SSIM score is a number between 0 and 1, and a higher value indicates a strong level of similarity.

2) Action Classification: To identify the user actions in the GUI recording, we look into the similarity scores of consecutive frames as shown in Fig. 3. The first step is to group frames belonging to the same atomic activity according to

tailored pattern analysis. This procedure is necessary because discrete activities performed on the screen will persist across several frames, and thus, need to be grouped and segmented accordingly. Consequently, we observe three patterns of user

actions, i.e., TAP, SCROLL, and INPUT. Note that we focus on the most commonly-used actions for brevity in this paper, other actions could be extended by comparing the consecutive frame similarity.

(a) TAP: As shown in Fig. 3A (user taps a button), the similarity score starts to drop drastically which reveals an in- stantaneous transition from one screen to another. In addition, one common case is that the similarity score becomes steady for a small period of time ts between two drastically droppings as shown in Fig. 3C. The occurrence of this short steady duration ts is because GUI has not finished loading. While the GUI layout of GUI rendering is fast, resource loading may take time. For example, rendering images from the web depends on device bandwidth, image loading efficiency, etc.

(b) SCROLL: As shown in Fig. 3D (user scrolls up/down the screen), the similarity score starts with a drastic drop and then continues to increase slightly over a period of time, which implicates a continuous transition from one GUI to another.

(c) INPUT: As shown in Fig. 3B (user inputs text), the similarity score starts to drop and rise multiple times, revealing typing characters and digits. However, the similarity score cannot reliably detect INPUT actions, as it may coincide with the TAP actions. To address this, we further supplement with Optical Character Recognition (OCR) technique [25] (a detailed description is demonstrated in Section II-C3) to detect whether there is a virtual keyboard in the GUI. Note that we focus on English apps, and it may take additional efforts to