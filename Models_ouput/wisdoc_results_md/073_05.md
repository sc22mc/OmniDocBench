Algorithm 2 (Discretized MMGA) takes not only its learning rate η but a small value γ in measuring an approximate gradient as inputs. In each time step, each player measures the gradients of its payoff for each variable of its strategy (lines 2-6). Then, the player updates its strategy by the gradients (lines 7-10). Here, note that the strategy update is weighted by the probability xa|s (line 8) in order to correspond to Algorithm 1. Here, each of lines 3-5 and line 8 can be updated in parallel with respect to a and s.

4 Theoretical Analysis 4.1 Continuous-Time Equivalence of Algorithms

The following theorems provide a unified understanding of different algorithms. Theorem 1 and 2 are concerned with continualization of the two discrete algorithms. Surprisingly, Theorem 3 proves the corre- spondence between these different continualized algorithms by Theorem 1 and 2.

Theorem 1 (Coutinualized MMRD). Let pa|s be the expected distribution when X chooses a under state s;

$$
p_{i^{\prime}}^{|s|}:=\left\{\begin{array}{ll}{y^{|b|s}}&{(s_{i^{\prime}}=abs^{-})}\\ {0}&{(\mathrm{otherwise})}\end{array}\right..
$$

(7)

In the limit of η → 0, Algorithm 1 is continualized as dynamics

$$
\dot{x}^{a|s_{i}}({\bf x},{\bf y})=p_{i}^{\mathrm{st}}x^{a|s_{i}}\left(\pi(\boldsymbol{p}^{a|s_{i}},{\bf x},{\bf y})-\bar{\pi}^{s_{i}}({\bf x},{\bf y})\right),
$$

∑

$$
\bar{\pi}^{s_{i}}({\bf x},{\bf y})=\sum_{a}x^{a|s_{i}}\pi(\boldsymbol{p}^{a|s_{i}},{\bf x},{\bf y}),
$$

(8) (9)

for all a ∈ A and s ∈ S. Here, π̄si is the expected payoff under state si.

Theorem 2 (Continualized MMGA). In the limit of γ → 0 and η → 0, Algorithm 2 is continualized as dynamics

for all a ∈ A and s ∈ S.

$$
\dot{x}^{a|s}({\bf x},{\bf y})=x^{a|s}\frac{\partial}{\partial x^{a|s}}u^{\mathrm{st}}(\mathrm{Norm}({\bf x}),{\bf y}),
$$

(10)

See Appendix A.1 and A.2 for the proof of Theorems 1 and 2.

Theorem 3 (Equivalence between the algorithms). The dynamics Eqs. (8) and (10) are equivalent.

Proof Sketch. Let x′ be the strategy given by xa|s ← xa|s+γ in x for a ∈ A and s ∈ S. Then, we consider the changes of the Markov transition matrix dM :=M(Norm(x′),y)−M (x,y) and the stationary distribution dpst := pst(Norm(x′),y) − pst(x,y). By considering this changes in the stationary condition pst =Mpst, we get dpst = (E−M)−1dMpst in O(γ). The right-hand (resp. left-hand) side of this equation corresponds to the continualized MMRD (resp. MMGA). See Appendix A.3 for the full proof.

For games with a general number of actions, the study [7] has proposed a gradient ascent algorithm in relation to replicator dynamics. In light of this study, Theorem 3 extends the relation to the multi-memory games. This extension is neither simple nor trivial. The relation between replicator dynamics and gradient ascent has been proved by directly calculating ust = pst ·u [17]. In multi-memory games, however, ust = pst ·u is too hard to calculate. Thus, as seen in the proof sketch, we proved the relation by considering a slight change in the stationary condition pst =Mpst, technically avoiding such a hard direct calculation.

4.2 Learning Dynamics Near Nash Equilibrium

Below, let us discuss the learning dynamics in multi-memory games, especially divergence from Nash equilib- rium in zero-sum payoff matrices. In order to obtain a phenomenological insight into the learning dynamics simply, we assume one-memory two-action zero-sum games in Assumption 1.