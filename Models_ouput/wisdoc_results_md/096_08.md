Figure 6. Qualitative results on the test sets. (a) Results of conventional methods. From left to right: input images, ground truth (GT), and results of LÃ³pez-Antequera et al. [33], Wakai and Yamashita [52], Wakai et al. [53], Pritts et al. [41], and Lochman et al. [32]. (b) Results of our method. From left to right: input images, GT, and the results of our method using HRNet-W32 in a Manhattan world.

Figure 7. Qualitative results for images from off-the-shelf cameras. From top to bottom: input images, the results of the compared method (front and side direction images obtained by Lochman et al. [32]), and our method using HRNet-W32 (front and side direction images). The identifiers (IDs) correspond to the camera IDs used in [53], and the projection names are shown below the IDs.

5. Conclusion

Limitations. It is difficult to recover images when an input image includes one or no unique axes that can be identified from the VP/ADPs. We believe that subsequent studies can extend this work to address this open challenge. Another promising direction for future work is to use several images or videos for input. We consider one image because our focus is to develop deep single image camera calibration. In a Manhattan world, we proposed a learning-based method to address rotation and distortion from an image. To recover the rotation, our heatmap-based VP estimator detects the VP/ADPs. Experiments demonstrated that our method substantially outperforms conventional methods.

11891

SL-MNSL-PBSP360HoliCity SL-MNSL-PBSP360HoliCity