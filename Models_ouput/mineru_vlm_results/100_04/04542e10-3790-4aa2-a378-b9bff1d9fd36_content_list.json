[
    {
        "type": "image",
        "img_path": "images/574aa3f88e259ed535bfcedd79577ed05753a3cbddaa0fee816e8c2452fddc05.jpg",
        "image_caption": [
            "Figure 2: The overview of the pipeline in each iteration of BoT. To show how boosting is achieved in this experience-driven iteration process, we present detailed intermediate results obtained from an experiment on ChapGPT-4 on the Game of 24 dataset. Given  $Q$  : \"The given four numbers are: 2, 4, 5, 5\", BoT performs three stages sequentially. With the simple prompt  $\\mathbb{I}^t$  as input, The Thought Structures Generation (Stage 1) outputs massive heterogeneous tree thought structures. Thought Structures Aggregation (Stage 2) aggregated them into a thought chain  $\\overline{z}_{1\\ldots n}$ , which is analyzed in Stage 3 to produce experience to further enhance the prompt."
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Instead of pre- preparing examples in the prompt, a more adaptive way is to design prompts to guide the LLMs to gradually generate the thought  $z_{i}$  during the reasoning process. This can be formalized as  $z_{i} \\sim p_{0}\\left(z_{i} \\mathbb{I}\\left(z_{1\\ldots i - 1}, X, Q\\right)\\right)$ . Finally, the solution is formalized as  $y \\sim p_{\\theta}\\left(y \\mathbb{I}\\left(z_{1\\ldots n}, X, Q\\right)\\right)$ . The representative approach, ToT Yao et al. (2024), further extends this sequential reasoning steps into a tree structure in which  $C$  next- step thoughts can be generated. Thus, the thought structure can be chain or tree.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.2 FRAMEWORK",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Existing literature that aims to generate the prompt with correct CoT examples or design delicate thought generation structures has three limitations. First, ineffective thoughts in those approaches are generally ignored or discarded. However, a human, who is not an expert in one field, particularly relies on analyzing previous errors to collect more experience to perform correctly on the next try. Second, they are less scalable because, for each task, an example of generating the next thoughts, such as  $\\mathbb{I}\\left(z_{1\\ldots n}, X, Q\\right)$ , should be provided in the prompt. Finally, the thought structure, such as the tree Yao et al. (2024), is generated to be overly complex to explore more reasoning steps for a better solution. This is largely due to the obtained solution may not be further revised.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this paper, we argue that the prompt can be enhanced by continuously collecting the analysis of LLMs on those ineffective thoughts - wrong reasoning steps in a chain of thought. Therefore, even a simple prompt, such as  $\\mathbb{I}\\left(X, Q\\right)$ , potentially leading to ineffective thoughts, can be progressively refined by relying on such analysis to gain powerful thoughts toward the solution.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We propose Boosting of Thoughts (BoT), an automated prompting framework incorporating, which achieves prompt boosting with an experience- driven iteration process commencing with a simple prompt. As summarized in Fig. 2, each iteration  $t$  of BoT includes three stages. The Thought Structures Generation stage is able to fully explore reasoning chains generated by LLMs with the input prompt  $\\mathbb{I}^t$ . In the second stage, these thought structures are aggregated to form a reasoning chain, which is to be analyzed by LLMs in the third stage to produce feedback containing error reports and detailed revision advice. Combining the aggregated reasoning chain with the feedback results in a new experience, denoted as  $\\mathbf{F}^t$ . Thus, the prompt is enhanced by accumulating these experiences  $\\mathbf{F}^{1\\ldots t}$  over iterations.",
        "page_idx": 0
    }
]