[
    {
        "type": "image",
        "img_path": "images/aeff54241ae85758cfd847c1e15e71b07e6b4fad65774463676abfba03c33054.jpg",
        "image_caption": [
            "Figure 6. Visual result tested on Origami light field. Top: reconstructed top-left views with epipolar plane images along blue/green lines. Bottom: view-by-view reconstruction quality in PSNR."
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/bf2676967eafde766768dd6fb013f03e4dfe858d5fe2e59e80fa5a33a2738886.jpg",
        "image_caption": [
            "Figure 7. Experiment with our prototype camera. Coding patterns and experimental setup (left), image frame and event stacks obtained from camera (center), and reconstructed light field view (at top-left viewpoint) with epipolar plane images along blue/green lines (right)."
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.2. Real-World Experiment",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To capture real 3- D scenes, we used our prototype camera and flexible-  $\\tau$  model. The aperture- coding patterns were set to the corresponding parameters in the learned AcqNet model. Figure 7 shows the coding patterns and experimental setup (left), an image frame and event stacks obtained from the camera (center), and the light field reconstructed from them (right). Our method performed well with real 3- D scenes; the parallax information was successfully embedded into the events, and a light field was reconstructed with visually convincing quality. Please refer to the supplementary video for more results with better visualization.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "5. Conclusion",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We proposed a new light- field acquisition method that takes advantage of coded- aperture imaging and an event camera. Although the measurement on the camera can be completed in a single exposure, our method is theoretically quasiequivalent to the baseline coded- aperture imaging method.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We carefully designed our algorithm pipeline to be end- toend trainable and compatible with real camera hardware, which enabled both accurate light- field reconstruction and successful development of our hardware prototype.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "For our future work, we will explore the design space with respect to, e.g., the number of coding patterns during an exposure, while considering the hardware restrictions. Seeking better hardware implementation to achieve RGB color reconstruction and higher spatial resolutions is also an important avenue. We will also extend our method to moving scenes (light field videos), which would benefit from the time- efficiency of our method. Finally, our method would be extended to a more event- centered direction, where event streams over the continuous time (instead of discretized event stacks) could be fully utilized and light fields could be reconstructed from events alone.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Acknowledgement: This work was supported by JSPS Grant- in- Aid for Scientific Research (B) 22H03611 and NICT contract research JPJ012368C06801.",
        "page_idx": 0
    }
]