[
    {
        "type": "text",
        "text": "former models (Kenton & Toutanova, 2019; Dosovitskiy et al., 2020).",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To simplify the illustration and better represent our model in the following sections:",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\n[\\mathsf{CLS}]_i^{\\nu}\\leftarrow \\mathsf{Enc}_\\nu ([\\mathsf{CLS}]_i^t)\\mathrm{~and~}[\\mathsf{CLS}]^t\\leftarrow \\mathsf{Enc}_t([\\mathsf{CLS}]^t), \\tag{2}\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "by default, when talking about  $[\\mathsf{CLS}]$ $([\\mathsf{CLS}]_i^{\\nu}$  and  $[\\mathsf{CLS}]^t)$  they are the CLIP's output embeddings rather than the input token  $([\\hat{\\mathsf{CLS}} ])$  the same as shown in Fig.2.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "However, the output embedding [CLS] cannot adequately represent an image as the single embedding provides limited cues to the location and time reasoning. Therefore, we consider enlarging the representations for an image. It is evident that in real life, we would get different ideas about what an image means from different individuals. Following in this vein, we propose a simple yet effective methods: in our technical implementation, we are inspired by MVR (Zhang et al., 2022) and introduce additional  $[\\mathsf{CLS}]_i^{\\nu}(i = 1,\\dots,n)$  to replace the original single [CLS] representation. Through the observation of ablations, we finally use 6 different  $[\\hat{\\mathsf{CLS}} ]_i^{\\nu}$  at the beginning of the image patch token embeddings  $(\\hat{I} = \\hat{I}_{1}^{patch},\\dots,\\hat{I}_{7}^{patch})$  like  $([\\hat{\\mathsf{CLS}} ]_i^{\\nu}\\dots [\\mathsf{CLS}]_i^{\\nu}\\hat{I})$  , and after going through the encoder  $\\operatorname {Enc}_\\nu$  , we get a list of embeddings  $((\\mathsf{CLS}]_1^{\\nu}\\dots [\\mathsf{CLS}]_6^{\\nu}I)$  . Using this design, the pre- trained model can investigate an image from multiple perspectives and dimensions. It follows the Q- principle and increases the quantity of information from multiple perspectives.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Since the text contains explicit semantic information and most language inputs carry clear messages, we only use the original  $[\\mathsf{CLS}]^t$  at the beginning of the text token embedding  $(\\mathrm{T})$  , like  $([\\mathsf{CLS}]^t T)$  . Hence, we search for corresponding information through the image from the CLIP model by conducting:",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\n\\big(\\big[\\mathsf{CLS}\\big]^t\\big)\\cdot \\big(\\big[\\mathsf{CLS}\\big]_i^{\\nu}\\big). \\tag{3}\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In the following fine- tuning or searching for open- world knowledge, each  $[\\mathsf{CLS}]_i^{\\nu}$  of the  $\\operatorname {Enc}_\\nu$  calculates the similarity with  $[\\mathsf{CLS}]^t$  of the candidate information by inner- product.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Location/Time Fine- tune. We first initialize and position- encode each  $[\\mathsf{CLS}]_i^{\\nu}$  individually, aiming to extend the distance between each  $[\\mathsf{CLS}]_i^{\\nu}$  . Then, we fine- tune CLIP with local and global losses (He et al., 2020; Zhang et al., 2022) to ensure each  $[\\mathsf{CLS}]_i^{\\nu}$  is aligned with the location and time linguistic features  $[\\mathsf{CLS}]^t$",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "For the local loss, the correspondence between each  $[\\mathsf{CLS}]_i^{\\nu}$  and  $[\\mathsf{CLS}]^t$  is achieved by a contrastive learning loss:",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\n\\begin{array}{r}L_{local} = -\\frac{1}{i + 1}\\sum_{0}^{i}\\log \\frac{e^{f_{i}(q_{\\nu},k_{t + })}}{\\sum_{1}^{n}[e^{f_{i}(q_{\\nu},k_{t + })} + e^{f_{i}(q_{\\nu},k_{t - })}]}, \\end{array} \\tag{4}\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "here,  $q_{\\nu}$  denote the query image embedding  $([\\mathsf{CLS}]_i^{\\nu})$  .  $k_{t + }$  and  $k_{t - }$  are the positive and negative key text embeddings (a batch of  $[\\mathsf{CLS}]^t)$  . We calculate the correlation score between them by inner product  $f_{i}(x,y)$",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Then, the global loss further constrains the correspondence between image features and location/time features, and the calculation method is as follows:",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\nL_{global} = -\\log \\frac{e^{f_{max}(q_{\\nu},k_{t + })}}{\\sum_{1}^{n}[e^{f_{max}(q_{\\nu},k_{t + })} + e^{f_{max}(q_{\\nu},k_{t - })}]}, \\tag{5}\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "where  $f_{max}(q_{\\nu},k_{t}) = \\max_{i}\\{f_{i}(q_{\\nu},k_{t})\\}$ $max\\{\\}$  represents the maximum value. The entire training loss is defined as a linear combination of two losses as  $L_{total} = L_{local} + L_{global}$",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Open- World Knowledge Search. After fine- tuning, each  $[\\mathsf{CLS}]_i^{\\nu}$  output by CLIP- V can represent image location/time information from various perspectives. We use these different representations to retrieve more valuable open- world knowledge from the OwK dataset (Sec 4.1) to increase the quantity of knowledge.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Given an image  $I$  and the corresponding Open- World Knowledge  $(O = T_{1}^{owk},T_{2}^{owk},\\dots,T_{k}^{owk},k = 122,408)$  , the process of searching follows Eq. 3: each  $[\\mathsf{CLS}]_i^{\\nu}$  calculates the similarity with 122,408 candidate Wikipedia corpus (OWK). Here, we select the candidate Wikipedia with the top- 1 similarity for each  $[\\mathsf{CLS}]_i^{\\nu}$  yielding a total of 6 OWKs. After that, the Quantity module (sec 3.2) finished its job by collecting a list of highly- related OWKs items that the next Relevance module (sec 3.3) would use as input for CLIP- T.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3.Relevance Module",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Scoring Mechanism. The amount of useful information varies for each  $[\\mathsf{CLS}]_i^{\\nu}$  of an image and the corresponding embeddings of open- world knowledge. As a result, it is critical to weigh the importance of different features dynamically. Based on the above motivation, we propose a scoring mechanism to further highlight the relevant features.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We adopt two layers of MLP  $(\\mathrm{MLP}_{2 - layer})$  as our relevance scoring component and find it helpful:",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\nW^{x} = \\mathbf{MLP}_{2 - layer}([\\mathsf{CLS}]_{i}^{\\nu}), \\tag{6}\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Here,  $[\\mathsf{CLS}]_i^x$  is the input embedding, and  $W^{x}$  is the calculated weight. We use contrastive learning to optimize the model. To facilitate implementation, we directly adopt the loss functions from the first step of the Quantity module (sec 3.2). In this case, we keep the CLIP- T and CLIP- V frozen and only update the parameters of the relevance scoring component.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In the local loss, the information of two features is integrated to optimize the scoring mechanism jointly:",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\nf_{i}(q,k_{+}) = (W_{i}^{owk}\\times [\\mathsf{CLS}]_{i}^{owk} + W_{i}^{\\nu}\\times [\\mathsf{CLS}]_{i}^{\\nu}),\\cdot F^{gt},\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    }
]