[
    {
        "type": "text",
        "text": "Assumption 1 (One- memory two- action zero- sum game). We assume a two- action (i.e.,  $\\mathcal{A} = \\{a_1,a_2\\}$  and  $\\mathcal{B} = \\{b_1,b_2\\}$ ), one- memory (i.e.,  $\\mathbf{s} = (a_{1}b_{1},a_{1}b_{2},a_{2}b_{1},a_{2}b_{2})$ ), and zero- sum game (i.e.,  $\\mathbf{v} = - \\mathbf{u}$ ). In particular, we discuss zero- sum games where both  $u_{1}$  and  $u_{4}$  are smaller or larger than both  $u_{2}$  and  $u_{3}$ .",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Under Assumption 1, we exclude uninteresting zero- sum payoff matrices that the Nash equilibrium exists as a set of pure strategies because the learning dynamics trivially converge to such pure strategies. The condition that both  $u_{1}$  and  $u_{4}$  are smaller or larger than both  $u_{2}$  and  $u_{3}$  is necessary and sufficient for the existence of no dominant pure strategy.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In the rest of this paper, we use a vector notation for strategies of X and Y;  $\\mathbf{x}\\coloneqq \\{x_{i}\\}_{i = 1,\\ldots ,4}$  and  $\\mathbf{y}\\coloneqq \\{y_{i}\\}_{i = 1,\\ldots ,4}$  as  $x_{i}\\coloneqq x^{a_{1}|s_{i}}$  and  $y_{i}\\coloneqq y^{b_{1}|s_{i}}$ . Indeed,  $x^{a_{2}|s_{i}} = 1 - x_{i}$  and  $y^{b_{2}|s_{i}} = 1 - y_{i}$  hold.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Theorem 4 (Uniqueness of Nash equilibrium). Under Assumption 1, the unique Nash equilibrium of this game is  $(x_{i},y_{i}) = (x^{*},y^{*})$  for all  $i$  as",
        "page_idx": 0
    },
    {
        "type": "equation",
        "text": "\n$$\nx^{*} = \\frac{-u_{3} + u_{4}}{u_{1} - u_{2} - u_{3} + u_{4}},y^{*} = \\frac{-u_{2} + u_{4}}{u_{1} - u_{2} - u_{3} + u_{4}}. \\tag{11}\n$$\n",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Proof Sketch. Let us prove that X's strategy in Nash equilibrium is uniquely  $\\mathbf{x} = x^{*}\\mathbf{1}$ . First, we define  $u^{*}$  and  $v^{*}$  as X's and Y's payoffs in the Nash equilibrium in the zero- memory game. If  $\\mathbf{x} = x^{*}\\mathbf{1}$ , X's expected payoff is  $u^{\\mathrm{st}} = u^{*}$ , regardless of Y's strategy  $\\mathbf{y}$ . Second, we consider that X uses another strategy  $\\mathbf{x}\\neq x^{*}\\mathbf{1}$ . Then, there is Y's strategy such that  $v^{\\mathrm{st}} > v^{*}\\Leftrightarrow u^{\\mathrm{st}}< u^{*}$ . Thus, X's minimax strategy is uniquely  $\\mathbf{x} = x^{*}\\mathbf{1}$ , completing the proof. See Appendix A.4 for the full proof.",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/780c7bf682c41315ab7fa9c91c3c7de2b616bc766f104c75f2760031b165b230.jpg",
        "image_caption": [
            "Figure 2: Multi-memory learning dynamics near the Nash equilibrium in the penny-matching game. In the upper six panels, colored lines indicate the time series of  $\\delta_{i}$  (X's strategy). The solid (resp. broken) lines are approximated (resp. experimental) trajectories of learning dynamics. From the top, the trajectories are predicted by approximations up to the first, second, and third orders. The bottom panel shows the errors between the approximated and experimental trajectories."
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Regarding Theorem 4, X (Y) chooses each action in the same probability independent of the last state. Here, they do not utilize their memory. Thus, note that in this sense, the Nash equilibrium is the same as",
        "page_idx": 0
    }
]