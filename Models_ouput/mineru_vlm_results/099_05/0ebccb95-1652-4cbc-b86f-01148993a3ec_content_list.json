[
    {
        "type": "table",
        "img_path": "images/1bbe76fb674bb25727e576f377612545a3412687ce26f1b9414cb0fe1cfea59d.jpg",
        "table_caption": [
            "Table 1: Agreement rates for pairwise comparison on different scenario groups and overall results. Results with underline are the best among all models and results in bold are the second-best. The mapping from abbreviations to names of scenario groups are:  $\\mathrm{Summ}\\rightarrow \\mathrm{Sum}$  marization. Crea  $\\mathrm{W}\\rightarrow$  Creative Writing, Func  $\\mathrm{W}\\rightarrow$  Functional Writing, and Comm  $\\longrightarrow$  General Communication."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Summ</td><td>Exam</td><td>Code</td><td>Rewriting</td><td>Crea W</td><td>Func W</td><td>Comm</td><td>NLP</td><td>Overall</td></tr><tr><td>Closed-source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ChatGPT</td><td>33.3</td><td>40.3</td><td>36.6</td><td>31.6</td><td>48.2</td><td>40.4</td><td>47.6</td><td>45.8</td><td>42.7</td></tr><tr><td>Claude-2</td><td>30.6</td><td>36.1</td><td>41.7</td><td>34.2</td><td>48.1</td><td>42.5</td><td>40.6</td><td>48.5</td><td>42.4</td></tr><tr><td>GPT-4</td><td>59.7</td><td>51.4</td><td>69.2</td><td>58.3</td><td>66.7</td><td>60.4</td><td>58.3</td><td>65.2</td><td>61.9</td></tr><tr><td>Open-source Models</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>SteamSHP</td><td>33.3</td><td>29.2</td><td>26.7</td><td>33.3</td><td>40.7</td><td>31.3</td><td>51.4</td><td>51.9</td><td>40.6</td></tr><tr><td>PandaLM</td><td>29.2</td><td>33.3</td><td>31.7</td><td>23.3</td><td>43.5</td><td>32.9</td><td>44.8</td><td>48.9</td><td>38.9</td></tr><tr><td>LLaMA-2-Chat-13B</td><td>20.8</td><td>27.8</td><td>19.2</td><td>20.0</td><td>31.5</td><td>27.5</td><td>35.8</td><td>31.8</td><td>29.0</td></tr><tr><td>Vicuna-13B-v1.5</td><td>30.6</td><td>23.6</td><td>35.0</td><td>28.3</td><td>36.1</td><td>37.5</td><td>45.5</td><td>39.8</td><td>37.3</td></tr><tr><td>WizardLM-13B-v1.2</td><td>24.2</td><td>20.8</td><td>32.5</td><td>19.2</td><td>28.7</td><td>25.4</td><td>29.2</td><td>33.0</td><td>27.8</td></tr><tr><td>LLaMA-2-Chat-13B</td><td>24.7</td><td>20.3</td><td>36.7</td><td>28.8</td><td>51.4</td><td>54.2</td><td>47.2</td><td>47.7</td><td>45.9</td></tr><tr><td>AUTO-J</td><td>45.8</td><td>38.9</td><td>59.2</td><td>47.5</td><td>54.6</td><td>57.1</td><td>58.0</td><td>57.6</td><td>54.8</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "annotated with a reformatted judgment (or discarded), or we have collected 100 samples for this scenario. The final size of pairwise training data is 3,436, and the detailed statistics are in Tab. [21]",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Single- response: For single- response, we pick 960 query- response pairs from Chatbot Arena Conversations with a balanced sampling on different scenarios. In preliminary experiments, directly incorporating the scenario criteria as the system message (as in pairwise evaluation) impairs GPT- 4's performance on single- response assessment, overly constraining its generated output to the scenario- specific criteria. Therefore, we adopt a \"divide- and- conquer\" strategy: We collect two pieces of critiques from GPT- 4 for a single response with and without scenario criteria as a system message, and then in the third inference, we get the final evaluation judgment by asking GPT- 4 to combine these two critiques into a more comprehensive critique and give a final rating. The user message prompt and the prompt for combining critiques are in Tab. [12] and [13] and the detailed statistics are shown in Tab. [22] Tab. [20] shows an example from the \"planning\" scenario. We find that critiques generated with and without scenario criteria exhibit distinct stylistic differences: The former is longer and closely adheres to the given criteria, whereas the latter is more concise yet capable of incorporating details not covered by the criteria. Finally, combining the above two critiques, a comprehensive critique simultaneously contains general criteria for this scenario and specific details for this sample.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Input format: Besides the collected evaluation judgments, we also need to determine the input format for AUTO- J. In early- stage experiments, we attempted to include the scenario criteria as the system message in the input. However, models trained in this manner performed poorly, often simply paraphrasing the scenario criteria. Therefore, we adopt a technique akin to Context Distillation (Bai et al., 2022b) and Ghost Attention (Touvron et al., 2023b), where we omit the inclusion of scenario criteria in the input for the training data, allowing the model to learn them from the output end implicitly. This design significantly enhances the generality of AUTO- J. The final input formats for pairwise comparison and single- response evaluation are in Tab. [17] and Tab. [18] respectively.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4 TRAINING AUTO-J",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "By integrating data from both pairwise and single- response evaluations, we train our model to seamlessly toggle between diverse evaluation protocols simply by applying the corresponding prompts. To lessen the positional bias (Wang et al., 2023a) in pairwise comparison, we apply a simple data augmentation trick. For each pairwise training sample, we swap the order of two responses in the input and alternate the \"Response 1\" and \"Response 2\" in the evaluation judgment. Since this doubles the pairwise data, we balanced the dataset by duplicating each single- response samples as well.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We train AUTO- J from LLaMA- 2- 13B- chat (Touvron et al., 2023b) with the DeepSpeed (Rasley et al., 2020) library, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al., 2020) Ren et al., 2021) Stage 3, gradient checkpointing (Chen et al., 2016) and FlashAttention (Dao et al., 2022) Dao, 2023) on 8 NVIDIA A100 GPUs. We use the bfloat16 (BF16) and tfloat32 (TF32) mix computation precision",
        "page_idx": 0
    }
]