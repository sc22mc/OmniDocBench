[
    {
        "type": "table",
        "img_path": "images/1c93dfc79909c2ffc1bc19c83a24d5a072bd97fffa881ee064a069e1274312ce.jpg",
        "table_caption": [
            "Table 2. Comprehensiv evaluation of document paring algorithms on OmniDocBench: performance metrics for text, formula, table, and reading order extraction, with overall scores derived from ground truth comparisons."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Method Type</td><td rowspan=\"2\">Methods</td><td colspan=\"2\">TextEdit↓</td><td colspan=\"2\">FormulaEdit↓</td><td colspan=\"2\">FormulaCDM↑</td><td colspan=\"2\">TableTEDS↑</td><td colspan=\"2\">TableEdit↓</td><td colspan=\"2\">Read OrderEdit↓</td><td colspan=\"2\">OverallEdit↓</td></tr><tr><td>EN</td><td>ZH</td><td>EN</td><td>ZH</td><td>EN</td><td>ZH</td><td>EN</td><td>ZH</td><td>EN</td><td>ZH</td><td>EN</td><td>ZH</td><td>EN</td><td>ZH</td></tr><tr><td rowspan=\"3\">Pipeline Tools</td><td>MinerU [42]</td><td>0.061</td><td>0.215</td><td>0.278</td><td>0.577</td><td>57.3</td><td>42.9</td><td>78.6</td><td>62.1</td><td>0.18</td><td>0.344</td><td>0.079</td><td>0.292</td><td>0.15</td><td>0.357</td></tr><tr><td>Marker [34]</td><td>0.08</td><td>0.315</td><td>0.53</td><td>0.883</td><td>17.6</td><td>11.7</td><td>67.6</td><td>49.2</td><td>0.619</td><td>0.685</td><td>0.114</td><td>0.34</td><td>0.336</td><td>0.556</td></tr><tr><td>Mathpix 4</td><td>0.105</td><td>0.384</td><td>0.306</td><td>0.454</td><td>62.7</td><td>62.1</td><td>77.0</td><td>67.1</td><td>0.243</td><td>0.32</td><td>0.108</td><td>0.304</td><td>0.191</td><td>0.365</td></tr><tr><td rowspan=\"2\">Expert VLMs</td><td>GOT-OCR [45]</td><td>0.189</td><td>0.315</td><td>0.360</td><td>0.528</td><td>74.3</td><td>45.3</td><td>53.2</td><td>47.2</td><td>0.459</td><td>0.52</td><td>0.141</td><td>0.28</td><td>0.287</td><td>0.411</td></tr><tr><td>Nougat [7]</td><td>0.365</td><td>0.998</td><td>0.488</td><td>0.941</td><td>15.1</td><td>6.8</td><td>39.9</td><td>0.0</td><td>0.572</td><td>1.000</td><td>0.382</td><td>0.942</td><td>0.452</td><td>0.973</td></tr><tr><td rowspan=\"3\">General VLMs</td><td>GPT4o [4]</td><td>0.144</td><td>0.409</td><td>0.425</td><td>0.606</td><td>72.8</td><td>42.8</td><td>72.0</td><td>62.9</td><td>0.234</td><td>0.329</td><td>0.128</td><td>0.251</td><td>0.233</td><td>0.399</td></tr><tr><td>Qwen2-VL-72B [44]</td><td>0.096</td><td>0.218</td><td>0.404</td><td>0.487</td><td>82.2</td><td>61.2</td><td>76.8</td><td>76.4</td><td>0.387</td><td>0.408</td><td>0.119</td><td>0.193</td><td>0.252</td><td>0.327</td></tr><tr><td>InternVL2-76B [8]</td><td>0.353</td><td>0.290</td><td>0.543</td><td>0.701</td><td>67.4</td><td>44.1</td><td>63.0</td><td>60.2</td><td>0.547</td><td>0.555</td><td>0.317</td><td>0.228</td><td>0.44</td><td>0.443</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/6661beb5f5ddcb03fe9efd749d7518d7b525191589597f58c5bafe8591521794.jpg",
        "table_caption": [
            "Table 3. End-to-end text recognition performance on OmniDocBench: evaluation using edit distance across 9 PDF page types."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model Type</td><td>Models</td><td>Book</td><td>Slides</td><td>Financial Report</td><td>Textbook</td><td>Exam Paper</td><td>Magazine</td><td>Academic Papers</td><td>Notes</td><td>Newspaper</td><td>Overall</td></tr><tr><td rowspan=\"3\">Pipeline Tools</td><td>MinerU [42]</td><td>0.055</td><td>0.124</td><td>0.033</td><td>0.102</td><td>0.159</td><td>0.072</td><td>0.025</td><td>0.984</td><td>0.171</td><td>0.206</td></tr><tr><td>Marker [34]</td><td>0.074</td><td>0.34</td><td>0.089</td><td>0.319</td><td>0.452</td><td>0.153</td><td>0.059</td><td>0.651</td><td>0.192</td><td>0.274</td></tr><tr><td>Mathpix 4</td><td>0.131</td><td>0.22</td><td>0.202</td><td>0.216</td><td>0.278</td><td>0.147</td><td>0.091</td><td>0.634</td><td>0.69</td><td>0.3</td></tr><tr><td rowspan=\"2\">Expert VLMs</td><td>GOT-OCR [45]</td><td>0.111</td><td>0.222</td><td>0.067</td><td>0.132</td><td>0.204</td><td>0.198</td><td>0.179</td><td>0.388</td><td>0.771</td><td>0.267</td></tr><tr><td>Nougat [7]</td><td>0.734</td><td>0.958</td><td>1.000</td><td>0.820</td><td>0.930</td><td>0.83</td><td>0.214</td><td>0.991</td><td>0.871</td><td>0.806</td></tr><tr><td rowspan=\"3\">General VLMs</td><td>GPT4o [2]</td><td>0.157</td><td>0.163</td><td>0.348</td><td>0.187</td><td>0.281</td><td>0.173</td><td>0.146</td><td>0.607</td><td>0.751</td><td>0.316</td></tr><tr><td>Qwen2-VL-72B [44]</td><td>0.096</td><td>0.061</td><td>0.047</td><td>0.149</td><td>0.195</td><td>0.071</td><td>0.085</td><td>0.168</td><td>0.676</td><td>0.179</td></tr><tr><td>InternVL2-76B [8]</td><td>0.216</td><td>0.098</td><td>0.162</td><td>0.184</td><td>0.247</td><td>0.150</td><td>0.419</td><td>0.226</td><td>0.903</td><td>0.3</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/d0f7a7251508ba15b1ef418c395a5cbc84d79cefb46126ecd3322f952d1513f4.jpg",
        "table_caption": [
            "Table 4. End-to-end text recognition on OmniDocBench: evaluation under various page attributes using the edit distance metric. The value is Mean/Variance of scores in the attribute group. Columns represent: Fuzzy (Fuzzy scan), Water (Watermark), Color (Colorful background). None (No special issue)"
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Models</td><td>Fuzzy</td><td>Water</td><td>Color</td><td>None</td></tr><tr><td>MinerU [42]</td><td>0.15/0.048</td><td>0.151/0.031</td><td>0.107/0.052</td><td>0.079/0.035</td></tr><tr><td>Marker [34]</td><td>0.333/0.092</td><td>0.484/0.126</td><td>0.319/0.127</td><td>0.062/0.125</td></tr><tr><td>Mathpix</td><td>0.294/0.064</td><td>0.290/0.039</td><td>0.216/0.097</td><td>0.135/0.043</td></tr><tr><td>GOT-OCR [45]</td><td>0.175/0.05</td><td>0.190/0.056</td><td>0.186/0.097</td><td>0.177/0.081</td></tr><tr><td>Nougat [7]</td><td>0.934/0.051</td><td>0.915/0.071</td><td>0.873/0.096</td><td>0.615/0.208</td></tr><tr><td>GPT4o [2]</td><td>0.263/0.078</td><td>0.195/0.057</td><td>0.184/0.078</td><td>0.186/0.072</td></tr><tr><td>Qwen2-VL-72B [44]</td><td>0.082/0.01</td><td>0.172/0.078</td><td>0.104/0.05</td><td>0.084/0.042</td></tr><tr><td>InternVL2-76B [8]</td><td>0.120/0.013</td><td>0.197/0.042</td><td>0.155/0.059</td><td>0.261/0.082</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/d748ab421f060e967f82b0fc69cbc372a7bc19421141af807862479372a8ad03.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 5. End-to-end reading order evaluation on OmniDocBench: results across different column layout types using Normalized Edit Distance. The value is Mean/Variance of scores in the attribute group."
        ],
        "table_body": "<table><tr><td>Models</td><td>Single</td><td>Double</td><td>Three</td><td>Complex</td></tr><tr><td>MinerU [42]</td><td>0.311/0.181</td><td>0.101/0.013</td><td>0.117/0.046</td><td>0.385/0.057</td></tr><tr><td>Marker [34]</td><td>0.299/0.143</td><td>0.299/0.299</td><td>0.149/0.063</td><td>0.363/0.086</td></tr><tr><td>Mathpix 4</td><td>0.207/0.123</td><td>0.188/0.07</td><td>0.225/0.029</td><td>0.452/0.177</td></tr><tr><td>GOT-OCR [45]</td><td>0.163/0.106</td><td>0.145/0.059</td><td>0.257/0.072</td><td>0.468/0.185</td></tr><tr><td>Nougat [7]</td><td>0.852/0.084</td><td>0.601/0.224</td><td>0.662/0.093</td><td>0.873/0.09</td></tr><tr><td>GPT4o [2]</td><td>0.109/0.112</td><td>0.204/0.076</td><td>0.254/0.046</td><td>0.426/0.188</td></tr><tr><td>Qwen2-VL-72B [44]</td><td>0.066/0.048</td><td>0.145/0.049</td><td>0.204/0.055</td><td>0.394/0.203</td></tr><tr><td>InternVL2-76B [8]</td><td>0.082/0.052</td><td>0.312/0.069</td><td>0.682/0.098</td><td>0.444/0.174</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Ignore Handling. We implement an ignore logic for certain components in PDF page content, meaning they participate in matching but are excluded from metric calculations. This is mainly because of inconsistent output standards among models, which should not affect the validation results. For fairness, we ignore: (1) Headers, footers, page numbers, and page footnotes, which are handled inconsistently by different models. (2) Captions for figures, tables, and footnotes often have uncertain placements, thus complicating the reading order. Additionally, some models embed table captions in HTML or LaTeX tables, while others treat them as plain text.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.3. Metric Calculation",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Pure Text. We calculate Normalized Edit Distance [21], averaging these metrics at the sample level to obtain the final scores.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Tables. All tables are converted to HTML format before calculating the Tree- Edit- Distance- based Similarity (TEDS) [54] metric and Normalized Edit Distance.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Formulas. Formulas are currently evaluated using the Character Detection Matching (CDM) metric [41], Normalized Edit Distance, and BLEU [33].",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Reading Order. Reading order is evaluated using the Normalized Edit Distance as metric. It only involves text com",
        "page_idx": 0
    }
]