[
    {
        "type": "table",
        "img_path": "images/4b30a389ae7c7db96c71765964bb5f6c4ab85261e9a1ac19c2c687d3842a4dc2.jpg",
        "table_caption": [
            "Table 4: Memory slots VS Original contexts  $\\sim 512$  tokens) on the PwC test set"
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">System 1 \n(k memory slots)</td><td rowspan=\"2\">System 2 \n(original context)</td><td rowspan=\"2\">win</td><td colspan=\"3\">Judgement (%)</td></tr><tr><td>lose</td><td>tie</td><td>on par (win+tie)</td></tr><tr><td rowspan=\"3\">Llama-7b (ICAE, k=128)</td><td>Alpach</td><td>56.7</td><td>26.9</td><td>16.4</td><td>73.1</td></tr><tr><td>StableLM-7b</td><td>74.1</td><td>18.8</td><td>7.2</td><td>81.3</td></tr><tr><td>GPT-4 (gold)</td><td>3.4</td><td>69.4</td><td>27.2</td><td>30.6</td></tr><tr><td rowspan=\"2\">Llama-2-7b-chat (ICAE, k=64)</td><td>Llama-2-7b-chat</td><td>13.6</td><td>51.6</td><td>34.8</td><td>48.4</td></tr><tr><td>GPT-4 (gold)</td><td>1.9</td><td>44.7</td><td>53.4</td><td>55.3</td></tr><tr><td rowspan=\"2\">Llama-2-7b-chat (ICAE, k=128)</td><td>Llama-2-7b-chat</td><td>19.6</td><td>45.4</td><td>35.0</td><td>54.6</td></tr><tr><td>GPT-4 (gold)</td><td>2.8</td><td>25.8</td><td>71.4</td><td>74.2</td></tr><tr><td rowspan=\"2\">Llama-2-7b-chat (ICAE, k=256)</td><td>Llama-2-7b-chat</td><td>22.0</td><td>22.2</td><td>55.8</td><td>77.8</td></tr><tr><td>GPT-4 (gold)</td><td>3.8</td><td>20.5</td><td>75.7</td><td>79.5</td></tr><tr><td rowspan=\"2\">Llama-2-13b-chat (ICAE, k=256)</td><td>Llama-2-13b-chat</td><td>21.9</td><td>20.8</td><td>57.3</td><td>79.2</td></tr><tr><td>GPT-4 (gold)</td><td>4.0</td><td>19.2</td><td>76.8</td><td>80.8</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/7cea994279b31c0767e88ebedae01e5a005fecc6384266f658a192c901bff9da.jpg",
        "table_caption": [
            "Table 5: ICAE with different memory slot lengths and different pretraining setups. The last row is the comparison between 128-length ICAE's memory and 128-token summary produced by the GPT-4."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">ICAE (Llama-2-7b-chat)</td><td colspan=\"4\">Judgement</td></tr><tr><td>win (%)</td><td>lose (%)</td><td>tie (%)</td><td>win/lose</td></tr><tr><td>k = 128 (pretrained) VS k = 64 (pretrained)</td><td>57.6</td><td>19.5</td><td>22.9</td><td>3.0</td></tr><tr><td>k = 64 (pretrained) VS k = 32 (pretrained)</td><td>44.7</td><td>21.8</td><td>33.5</td><td>2.1</td></tr><tr><td>k = 64 (pretrained) VS k = 128 (no pretraining)</td><td>33.1</td><td>28.0</td><td>38.9</td><td>1.2</td></tr><tr><td>k = 128 (pretrained) VS k = 128 (no pretraining)</td><td>60.4</td><td>9.5</td><td>30.1</td><td>6.4</td></tr><tr><td>k = 128 (pretrained) VS k = 128 (pretrained only with AE)</td><td>36.4</td><td>28.5</td><td>35.1</td><td>1.3</td></tr><tr><td>k = 128 (pretrained) VS k = 128 (pretrained only with LM)</td><td>35.1</td><td>24.9</td><td>40.0</td><td>1.4</td></tr><tr><td>k = 128 (pretrained) VS 128-token summary (by GPT-4)</td><td>34.1</td><td>17.6</td><td>48.3</td><td>1.9</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "non- pretrained counterpart, emphasizing the importance of pretraining. By comparing the outputs generated via the pretrained and non- pretrained ICAE, we find the pretrained ICAE suffers less from hallucination than the non- pretrained counterpart (see the examples in Table 9 in Appendix D). We assume the pretraining of ICAE improves the LLM's working memory as it shares some analogies with humans enhancing their memory capacity via extensive memory training which improves the brain's memory encoding capabilities. We also examine pretraining objectives and find combining<sup>3</sup> AE and LM yields better results than using AE or LM individually (the 4th row in Table 5).",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The last row of Table 5 compares ICAE's 128- length memory slots with a summary<sup>4</sup> within 128 tokens ( $\\sim 100$  words). Memory slots significantly outperform summaries under the same context length, with  $\\sim 2\\times$  win/lose ratio, proving to be more compact and informative than natural language.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3 ANALYSIS",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3.1 SCALABILITY",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As discussed above, ICAE should achieve better compression performance with a more powerful target LLM. To verify this assumption, we compare the ICAE's performance on three target LLMs: Llama- 7b, Llama- 2- 7b and Llama- 2- 13b in Table 6, which align well with our expectations - more powerful target LLMs can achieve better context compression ratios.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3.2 LATENCY",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We conducted an empirical test to evaluate the impact of ICAE's  $4\\times$  context compression on inference efficiency. For this efficiency test, we fix the context (i.e., input) length to either 512 or 2048 and the generation length to 128. Table 7 shows that context compression by ICAE is helpful to improve LLM (i.e., Llama- 7b) inference efficiency, achieving over  $2\\times$  speedup. Its acceleration becomes",
        "page_idx": 0
    }
]