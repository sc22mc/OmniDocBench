[
    {
        "type": "image",
        "img_path": "images/27bc69d9ea5aadd126add5372df2a6f6fb9b07f76f25853a403bb3616fccdcc6.jpg",
        "image_caption": [
            "Figure 3. We show the visualizations of 5 procedures of QR-CLIP. For each process, the reader can refer to Fig. 2"
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.5. Limitation and Future Work",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We are still in the early stages of investigating how to best use CLIP and the QR principle to explore open- world knowledge to support location and time reasoning. And the modules and techniques developed are simple but effective. In the future: 1) we will investigate more efficient and elegant implementations; 2) while addressing the limited computational resources, collect a larger OWK dataset as input candidates; 3) using multimodal OWKs to see if images from Instagram, Twitter, etc. could help with this task.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "5. Conclusion",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We designed a novel QR- CLIP model. It consists of two modules: 1) the Quantity module and 2) the Relevance module. Experiments show that it outperforms all previous SOTA on location and time reasoning by a wide margin. To show how our designed components affect the model, we conduct comprehensive ablation studies and verify that open- world knowledge is beneficial for solving our problem. We hope this paper will serve as a technical foundation for this study area and inspire more fascinating research.",
        "page_idx": 0
    }
]