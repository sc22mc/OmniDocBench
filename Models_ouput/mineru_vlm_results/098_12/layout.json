{
    "pdf_info": [
        {
            "para_blocks": [
                {
                    "bbox": [
                        441,
                        343,
                        2106,
                        481
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                441,
                                343,
                                2106,
                                481
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        441,
                                        343,
                                        2106,
                                        481
                                    ],
                                    "type": "text",
                                    "content": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288, 2023b."
                                }
                            ]
                        }
                    ],
                    "index": 0
                },
                {
                    "bbox": [
                        443,
                        518,
                        2106,
                        656
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                443,
                                518,
                                2106,
                                656
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        443,
                                        518,
                                        2106,
                                        656
                                    ],
                                    "type": "text",
                                    "content": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017."
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        443,
                        693,
                        2108,
                        834
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                443,
                                693,
                                2108,
                                834
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        443,
                                        693,
                                        2108,
                                        834
                                    ],
                                    "type": "text",
                                    "content": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self- instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022."
                                }
                            ]
                        }
                    ],
                    "index": 2
                },
                {
                    "bbox": [
                        443,
                        867,
                        2106,
                        1009
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                443,
                                867,
                                2106,
                                1009
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        443,
                                        867,
                                        2106,
                                        1009
                                    ],
                                    "type": "text",
                                    "content": "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task- solving agent through multi- persona self- collaboration. arXiv preprint arXiv:2307.05300, 2023."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "bbox": [
                        443,
                        1042,
                        2106,
                        1184
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                443,
                                1042,
                                2106,
                                1184
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        443,
                                        1042,
                                        2106,
                                        1184
                                    ],
                                    "type": "text",
                                    "content": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero- shot learners. arXiv preprint arXiv:2109.01652, 2021."
                                }
                            ]
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        443,
                        1217,
                        2106,
                        1359
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                443,
                                1217,
                                2106,
                                1359
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        443,
                                        1217,
                                        2106,
                                        1359
                                    ],
                                    "type": "text",
                                    "content": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain- of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824- 24837, 2022."
                                }
                            ]
                        }
                    ],
                    "index": 5
                },
                {
                    "bbox": [
                        443,
                        1395,
                        2106,
                        1534
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                443,
                                1395,
                                2106,
                                1534
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        443,
                                        1395,
                                        2106,
                                        1534
                                    ],
                                    "type": "text",
                                    "content": "David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162, 2022."
                                }
                            ]
                        }
                    ],
                    "index": 6
                },
                {
                    "bbox": [
                        441,
                        1570,
                        2106,
                        1663
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                441,
                                1570,
                                2106,
                                1663
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        441,
                                        1570,
                                        2106,
                                        1663
                                    ],
                                    "type": "text",
                                    "content": "Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        441,
                        1696,
                        2103,
                        1795
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                441,
                                1696,
                                2103,
                                1795
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        441,
                                        1696,
                                        2103,
                                        1795
                                    ],
                                    "type": "text",
                                    "content": "Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, and Furu Wei. K- level reasoning with large language models. arXiv preprint arXiv:2402.01521, 2024."
                                }
                            ]
                        }
                    ],
                    "index": 8
                },
                {
                    "bbox": [
                        441,
                        1828,
                        2106,
                        1966
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                441,
                                1828,
                                2106,
                                1966
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        441,
                                        1828,
                                        2106,
                                        1966
                                    ],
                                    "type": "text",
                                    "content": "Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough- to- beat baseline for instruction fine- tuning. arXiv preprint arXiv:2402.04833, 2024."
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        438,
                        2006,
                        2106,
                        2098
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                438,
                                2006,
                                2106,
                                2098
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        438,
                                        2006,
                                        2106,
                                        2098
                                    ],
                                    "type": "text",
                                    "content": "Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self- attention mechanism. In International Conference on Machine Learning, 2022."
                                }
                            ]
                        }
                    ],
                    "index": 10
                },
                {
                    "bbox": [
                        446,
                        2191,
                        1292,
                        2244
                    ],
                    "type": "title",
                    "lines": [
                        {
                            "bbox": [
                                446,
                                2191,
                                1292,
                                2244
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        446,
                                        2191,
                                        1292,
                                        2244
                                    ],
                                    "type": "text",
                                    "content": "A MODEL TRAINING CONFIGURATION"
                                }
                            ]
                        }
                    ],
                    "index": 11,
                    "level": 1
                },
                {
                    "bbox": [
                        438,
                        2293,
                        2106,
                        2392
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                438,
                                2293,
                                2106,
                                2392
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        438,
                                        2293,
                                        2106,
                                        2392
                                    ],
                                    "type": "text",
                                    "content": "We show how to perform pretraining with the text continuation objective and instruction fine- tuning in Figure 7 and 8."
                                }
                            ]
                        }
                    ],
                    "index": 12
                },
                {
                    "bbox": [
                        438,
                        2412,
                        2106,
                        2508
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                438,
                                2412,
                                2106,
                                2508
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        438,
                                        2412,
                                        2106,
                                        2508
                                    ],
                                    "type": "text",
                                    "content": "We train the ICAE on 8 Nvidia A100 GPUs (80GB). The hyperparameters for pretraining and fine- tuning ICAE are presented in Table 8. We by default train the ICAE with bf16."
                                }
                            ]
                        }
                    ],
                    "index": 13
                },
                {
                    "type": "table",
                    "bbox": [
                        787,
                        2640,
                        1759,
                        3016
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                787,
                                2640,
                                1759,
                                3016
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        787,
                                        2640,
                                        1759,
                                        3016
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                787,
                                                2640,
                                                1759,
                                                3016
                                            ],
                                            "type": "table",
                                            "html": "<table><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>learning rate</td><td>1e-4 (pretrain); 5e-5 (fine-tuning)</td></tr><tr><td>batch size</td><td>256</td></tr><tr><td>warmup</td><td>300</td></tr><tr><td>#updates</td><td>200k (pretrain); 30k (fine-tuning)</td></tr><tr><td>clip norm</td><td>2.0</td></tr></table>",
                                            "image_path": "9ae7ee0ca99ae006f9dbb54a5dd115d139d5a79fd3a49802be17968eeecbc058.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 15,
                            "type": "table_body"
                        },
                        {
                            "bbox": [
                                953,
                                2554,
                                1593,
                                2603
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        953,
                                        2554,
                                        1593,
                                        2603
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                953,
                                                2554,
                                                1593,
                                                2603
                                            ],
                                            "type": "text",
                                            "content": "Table 8: Hyperparameters for training"
                                        }
                                    ]
                                }
                            ],
                            "index": 14,
                            "type": "table_caption"
                        }
                    ],
                    "index": 15
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                2550,
                3300
            ],
            "page_idx": 0
        }
    ],
    "_backend": "vlm",
    "_version_name": "2.1.9"
}