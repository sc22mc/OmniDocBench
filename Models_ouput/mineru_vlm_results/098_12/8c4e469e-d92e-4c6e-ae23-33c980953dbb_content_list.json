[
    {
        "type": "text",
        "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self- instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task- solving agent through multi- persona self- collaboration. arXiv preprint arXiv:2307.05300, 2023.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero- shot learners. arXiv preprint arXiv:2109.01652, 2021.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain- of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824- 24837, 2022.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162, 2022.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, and Furu Wei. K- level reasoning with large language models. arXiv preprint arXiv:2402.01521, 2024.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough- to- beat baseline for instruction fine- tuning. arXiv preprint arXiv:2402.04833, 2024.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self- attention mechanism. In International Conference on Machine Learning, 2022.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "A MODEL TRAINING CONFIGURATION",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We show how to perform pretraining with the text continuation objective and instruction fine- tuning in Figure 7 and 8.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We train the ICAE on 8 Nvidia A100 GPUs (80GB). The hyperparameters for pretraining and fine- tuning ICAE are presented in Table 8. We by default train the ICAE with bf16.",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/9ae7ee0ca99ae006f9dbb54a5dd115d139d5a79fd3a49802be17968eeecbc058.jpg",
        "table_caption": [
            "Table 8: Hyperparameters for training"
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>learning rate</td><td>1e-4 (pretrain); 5e-5 (fine-tuning)</td></tr><tr><td>batch size</td><td>256</td></tr><tr><td>warmup</td><td>300</td></tr><tr><td>#updates</td><td>200k (pretrain); 30k (fine-tuning)</td></tr><tr><td>clip norm</td><td>2.0</td></tr></table>",
        "page_idx": 0
    }
]