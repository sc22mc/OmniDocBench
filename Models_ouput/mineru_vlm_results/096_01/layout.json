{
    "pdf_info": [
        {
            "para_blocks": [
                {
                    "bbox": [
                        277,
                        429,
                        2203,
                        577
                    ],
                    "type": "title",
                    "lines": [
                        {
                            "bbox": [
                                277,
                                429,
                                2203,
                                577
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        277,
                                        429,
                                        2203,
                                        577
                                    ],
                                    "type": "text",
                                    "content": "Deep Single Image Camera Calibration by Heatmap Regression to Recover Fisheye Images Under Manhattan World Assumption"
                                }
                            ]
                        }
                    ],
                    "index": 0,
                    "level": 1
                },
                {
                    "bbox": [
                        239,
                        669,
                        2238,
                        848
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                239,
                                669,
                                2238,
                                848
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        239,
                                        669,
                                        2238,
                                        848
                                    ],
                                    "type": "text",
                                    "content": "Nobuhiko Wakai1 Satoshi Sato' Yasunori Ishii1 Takayoshi Yamashita2 1 Panasonic Holdings Corporation 2 Chubu University {wakai.nobuhiko,sato.satoshi,ishii.yasunori}@jp.panasonic.com takayoshi@sc.chubu.ac.jp"
                                }
                            ]
                        }
                    ],
                    "index": 1
                },
                {
                    "bbox": [
                        604,
                        960,
                        795,
                        1016
                    ],
                    "type": "title",
                    "lines": [
                        {
                            "bbox": [
                                604,
                                960,
                                795,
                                1016
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        604,
                                        960,
                                        795,
                                        1016
                                    ],
                                    "type": "text",
                                    "content": "Abstract"
                                }
                            ]
                        }
                    ],
                    "index": 2,
                    "level": 1
                },
                {
                    "bbox": [
                        204,
                        1075,
                        1193,
                        2019
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                204,
                                1075,
                                1193,
                                2019
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        204,
                                        1075,
                                        1193,
                                        2019
                                    ],
                                    "type": "text",
                                    "content": "A Manhattan world lying along cuboid buildings is useful for camera angle estimation. However, accurate and robust angle estimation from fisheye images in the Manhattan world has remained an open challenge because general scene images tend to lack constraints such as lines, arcs, and vanishing points. To achieve higher accuracy and robustness, we propose a learning- based calibration method that uses heatmap regression, which is similar to pose estimation using keypoints, to detect the directions of labeled image coordinates. Simultaneously, our two estimators recover the rotation and remove fisheye distortion by remapping from a general scene image. Without considering vanishing- point constraints, we find that additional points for learning- based methods can be defined. To compensate for the lack of vanishing points in images, we introduce auxiliary diagonal points that have the optimal 3D arrangement of spatial uniformity. Extensive experiments demonstrated that our method outperforms conventional methods on large- scale datasets and with off- the- shelf cameras."
                                }
                            ]
                        }
                    ],
                    "index": 3
                },
                {
                    "type": "image",
                    "bbox": [
                        1287,
                        957,
                        2246,
                        1666
                    ],
                    "blocks": [
                        {
                            "bbox": [
                                1287,
                                957,
                                2246,
                                1666
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        1287,
                                        957,
                                        2246,
                                        1666
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                1287,
                                                957,
                                                2246,
                                                1666
                                            ],
                                            "type": "image",
                                            "image_path": "6802632ff023325d861a3993c6ac4f7f8ce40682407fb544b0c26a694a6a4abb.jpg"
                                        }
                                    ]
                                }
                            ],
                            "index": 4,
                            "type": "image_body"
                        },
                        {
                            "bbox": [
                                1282,
                                1709,
                                2274,
                                2026
                            ],
                            "lines": [
                                {
                                    "bbox": [
                                        1282,
                                        1709,
                                        2274,
                                        2026
                                    ],
                                    "spans": [
                                        {
                                            "bbox": [
                                                1282,
                                                1709,
                                                2274,
                                                2026
                                            ],
                                            "type": "text",
                                            "content": "Figure 1. Our network estimates extrinsics and intrinsics in a Manhattan world from a single image. Our estimated camera parameters are used to fully recover images by remapping them while distinguishing the front and side directions on the basis of the Manhattan world. Cyan, magenta, and yellow lines indicate the three orthogonal planes of the Manhattan frame in each of the images. The input image is generated from [38]."
                                        }
                                    ]
                                }
                            ],
                            "index": 5,
                            "type": "image_caption"
                        }
                    ],
                    "index": 4
                },
                {
                    "bbox": [
                        209,
                        2131,
                        525,
                        2187
                    ],
                    "type": "title",
                    "lines": [
                        {
                            "bbox": [
                                209,
                                2131,
                                525,
                                2187
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        209,
                                        2131,
                                        525,
                                        2187
                                    ],
                                    "type": "text",
                                    "content": "1. Introduction"
                                }
                            ]
                        }
                    ],
                    "index": 6,
                    "level": 1
                },
                {
                    "bbox": [
                        206,
                        2227,
                        1190,
                        2973
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                206,
                                2227,
                                1190,
                                2973
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        206,
                                        2227,
                                        1190,
                                        2973
                                    ],
                                    "type": "text",
                                    "content": "In city scenes, image- based recognition methods are widely used for cars, drones, and robots. It is desirable to recognize the directions in which roads exist for navigation, selfdriving, and driver assistance. To avoid colliding with cars and pedestrians, it is more important to detect these objects in front of a vehicle rather than at the sides in Figure 1. We can obtain the travel direction from odometry, gyroscopes, or accelerator sensors using these specific devices. However, for cars, drones, and robots, image- based angle estimation of the travel direction without these devices is better for miniaturized and lightweight design. To determine the origin of angles, a Manhattan world [12] defines orthogonal world coordinates along cuboid buildings and a grid of streets. Although this image- based angle estimation is a long- studied topic in areas of geometric tasks [1, 4, 51], "
                                }
                            ]
                        },
                        {
                            "bbox": [
                                1280,
                                2125,
                                2272,
                                2270
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        1280,
                                        2125,
                                        2272,
                                        2270
                                    ],
                                    "type": "text",
                                    "content": "accurate and robust angle estimation has remained an open challenge because general scene images tend to lack constraints such as lines, arcs, and vanishing points (VPs)."
                                }
                            ]
                        }
                    ],
                    "index": 7
                },
                {
                    "bbox": [
                        1280,
                        2125,
                        2272,
                        2270
                    ],
                    "type": "text",
                    "lines": [],
                    "index": 8,
                    "lines_deleted": true
                },
                {
                    "bbox": [
                        1282,
                        2273,
                        2272,
                        2768
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                1282,
                                2273,
                                2272,
                                2768
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        1282,
                                        2273,
                                        2272,
                                        2768
                                    ],
                                    "type": "text",
                                    "content": "To control cars, drones, and robots, images for recognition are needed that have a large field of view (FOV).Fisheye cameras have a larger FOV than other cameras, but fisheye images are highly distorted. After fisheye distortion has been removed, we can use various learning- based recognition methods, such as object detection [25, 27], semantic segmentation [10, 26], lane detection [15, 61], action recognition [50, 59], and action prediction [6, 20]. To recover fisheye images, performing camera calibration before the recognition tasks mentioned above is desirable."
                                }
                            ]
                        }
                    ],
                    "index": 9
                },
                {
                    "bbox": [
                        1280,
                        2775,
                        2272,
                        2973
                    ],
                    "type": "text",
                    "lines": [
                        {
                            "bbox": [
                                1280,
                                2775,
                                2272,
                                2973
                            ],
                            "spans": [
                                {
                                    "bbox": [
                                        1280,
                                        2775,
                                        2272,
                                        2973
                                    ],
                                    "type": "text",
                                    "content": "Geometry- based calibration methods can estimate the camera rotation and distortion from a distorted image [3, 32, 41, 57]. However, it is difficult for geometry- based methods to calibrate cameras from images that contain few artificial"
                                }
                            ]
                        }
                    ],
                    "index": 10
                }
            ],
            "discarded_blocks": [],
            "page_size": [
                2550,
                3300
            ],
            "page_idx": 0
        }
    ],
    "_backend": "vlm",
    "_version_name": "2.1.9"
}