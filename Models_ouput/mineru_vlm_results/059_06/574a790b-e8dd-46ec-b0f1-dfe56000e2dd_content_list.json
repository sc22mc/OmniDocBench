[
    {
        "type": "table",
        "img_path": "images/148a54cb4972d619a47d825a2de901cadaf4cad68c3edb56c68de5908ceed24a.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table 2: Automatic evaluation results of different models on all session data. Session  $i$  indicates there are  $i - 1$  history conversation sessions. B-2, B-3, and R-L denote BLEU-2, BLEU-3, and Rouge-L respectively. The best results are in boldface."
        ],
        "table_body": "<table><tr><td rowspan=\"2\">Model</td><td colspan=\"3\">Session 2</td><td colspan=\"3\">Session 3</td><td colspan=\"3\">Session 4</td><td colspan=\"3\">Session 5</td></tr><tr><td>B-2</td><td>B-3</td><td>R-L</td><td>B-2</td><td>B-3</td><td>R-L</td><td>B-2</td><td>B-3</td><td>R-L</td><td>B-2</td><td>B-3</td><td>R-L</td></tr><tr><td>BlenderBot</td><td>2.79</td><td>0.65</td><td>13.73</td><td>2.41</td><td>0.45</td><td>13.06</td><td>2.14</td><td>0.39</td><td>12.76</td><td>2.26</td><td>0.45</td><td>12.75</td></tr><tr><td>BlenderBotmsc</td><td>4.76</td><td>1.51</td><td>16.18</td><td>5.03</td><td>1.61</td><td>16.39</td><td>4.78</td><td>1.49</td><td>15.56</td><td>4.98</td><td>1.48</td><td>16.10</td></tr><tr><td>FID-RAG</td><td>4.82</td><td>1.54</td><td>16.53</td><td>5.04</td><td>1.61</td><td>16.42</td><td>4.84</td><td>1.48</td><td>15.89</td><td>5.06</td><td>1.57</td><td>16.01</td></tr><tr><td>HAHT (ours)</td><td>5.07</td><td>1.57</td><td>16.90</td><td>5.27</td><td>1.67</td><td>16.72</td><td>5.00</td><td>1.55</td><td>15.97</td><td>5.16</td><td>1.60</td><td>16.42</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/f3796dd9b62bb1646f600901a96db2a5f6bff3c6e61b04521d7e5640e04a2b6d.jpg",
        "table_caption": [
            "Table 3: Human evaluation of the response generation by different methods. All scores are rated in four levels  $0 / 1 / 2 / 3$  .The best results are in boldface. We measure the inter-rater reliability with Fleiss'Kappa (Fleiss and Cohen, 1973). Our annotations obtain \"good agreement\" for Readability (0.614) and \"moderate agreement\" for Context Relevancy (0.526) and History Relevancy (0.573)."
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Readability</td><td>Context Relevancy</td><td>History Relevancy</td></tr><tr><td>BlenderBot</td><td>1.78</td><td>1.13</td><td>0.09</td></tr><tr><td>BlenderBotmsc</td><td>1.82</td><td>1.56</td><td>0.13</td></tr><tr><td>RAG-FID</td><td>1.89</td><td>1.84</td><td>0.21</td></tr><tr><td>HAHT (ours)</td><td>2.05</td><td>2.03</td><td>0.33</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "versations. Only responses that are consistent with history conversations are considered relevant to history.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Each aspect is rated in four different levels  $0 / 1 / 2 / 3$  and the final score of each aspect is the average of the scores given by all annotators. We measure the inter- annotator reliability with Fleiss' Kappa (Fleiss and Cohen, 1973). For all evaluation metrics, the higher value indicates better performance.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.3 Baseline Methods",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We compare the proposed HAHT model with the following baseline methods.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "BlenderBot (Roller et al., 2021): This is a largescale open- domain dialogue model pre- trained on the dialogue data scraped from social discussions on the web. BlenderBotmsc: This is the BlenderBot model finetuned on the MSC dataset. FID- RAG (Shuster et al., 2021): In this method, RAG- trained retriever (Lewis et al., 2020) is used to retrieve top-  $N$  history conversations, and Fusion- Decoder (FiD) (Izacard and Grave, 2021) is adopted to generate a final response considering the retrieved history conversations and current conversations. Following (Xu et al., 2022),  $N$  is empirically set to 5.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.4 Model Settings",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this work, all the evaluated methods are trained following the same settings. Due to the limitation of computation resources, we use the BlenderBot model with 90M parameters as the initial pretrained model and finetune it on the Facebook MSC dataset. The input length truncation is set to 256. The number of Transformer encoder layers  $n_{\\text{enc}}$  and decoder layers  $n_{\\text{dec}}$  are both set to 12. For model training, we use the Adamax optimizer (Kingma and Ba, 2014) with a learning rate of  $1 \\times 10^{- 6}$ , batch size of 16, dropout ratio of 0.1, and early stopping patience of 10. All the fine- tuned models are trained with a maximum of two 32GB GPUs (NVIDIA V100).",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "5 Experimental Results",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "This section presents the experimental results of the automatic evaluation, human evaluation, evaluation on session openings, ablation study, and case study.",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "5.1 Automatic Evaluation",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The automatic evaluation results of different models are shown in Table 2. It can be observed that BlenderBotmsc performs much better when finetuned on the MSC dataset. FID- RAG performs better than BlenderBotmsc. The potential reason is that RAG can retrieve important history conversations, and FID can combine the retrieved conversations with current conversations to generate better responses. Moreover, the proposed HAHT model consistently outperforms baseline methods in terms of all the evaluation metrics. This indicates that HAHT can better encode the history conversations, leverage history conversations to understand the current conversation context and generate more human- like responses.",
        "page_idx": 0
    }
]