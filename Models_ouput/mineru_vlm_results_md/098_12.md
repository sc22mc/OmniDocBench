Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine- tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.

Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self- instruct: Aligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.

Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task- solving agent through multi- persona self- collaboration. arXiv preprint arXiv:2307.05300, 2023.

Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero- shot learners. arXiv preprint arXiv:2109.01652, 2021.

Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain- of- thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824- 24837, 2022.

David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162, 2022.

Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022.

Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, and Furu Wei. K- level reasoning with large language models. arXiv preprint arXiv:2402.01521, 2024.

Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough- to- beat baseline for instruction fine- tuning. arXiv preprint arXiv:2402.04833, 2024.

Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self- attention mechanism. In International Conference on Machine Learning, 2022.

# A MODEL TRAINING CONFIGURATION

We show how to perform pretraining with the text continuation objective and instruction fine- tuning in Figure 7 and 8.

We train the ICAE on 8 Nvidia A100 GPUs (80GB). The hyperparameters for pretraining and fine- tuning ICAE are presented in Table 8. We by default train the ICAE with bf16.

Table 8: Hyperparameters for training  

<table><tr><td>Hyperparameter</td><td>Value</td></tr><tr><td>Optimizer</td><td>AdamW</td></tr><tr><td>learning rate</td><td>1e-4 (pretrain); 5e-5 (fine-tuning)</td></tr><tr><td>batch size</td><td>256</td></tr><tr><td>warmup</td><td>300</td></tr><tr><td>#updates</td><td>200k (pretrain); 30k (fine-tuning)</td></tr><tr><td>clip norm</td><td>2.0</td></tr></table>