<table><tr><td>ID</td><td>Method</td><td>Training Mode</td><td>Accuracy (â‰ˆ Rank@1)</td><td>Rank@5</td><td>Example-F1</td><td>F1-Score</td></tr><tr><td colspan="7">Location Reasoning</td></tr><tr><td>1</td><td>ResNet-50 (He et al., 2016)</td><td>Supervised</td><td>3.18%</td><td>9.82%</td><td>22.19%</td><td>2.27%</td></tr><tr><td>2</td><td>Swin-T (Liu et al., 2021)</td><td>Supervised</td><td>6.70%</td><td>17.07%</td><td>33.56%</td><td>5.02%</td></tr><tr><td>3</td><td>CLIP (Radford et al., 2021)</td><td>Zero-Shot</td><td>11.11%</td><td>27.85%</td><td>44.96%</td><td>9.74%</td></tr><tr><td>4</td><td>CLIP+ (Fu et al., 2022)</td><td>Fine-tune</td><td>15.72%</td><td>37.13%</td><td>49.74%</td><td>13.82%</td></tr><tr><td>5</td><td>CLIP+Seg (Fu et al., 2022)</td><td>Fine-tune</td><td>16.46%</td><td>37.48%</td><td>50.52%</td><td>14.63%</td></tr><tr><td>6</td><td>QR-CLIP (Ours)</td><td>Fine-tune</td><td>19.31%</td><td>38.78%</td><td>50.96%</td><td>17.70%</td></tr><tr><td colspan="3">Improvements (AVG: 10.66%)</td><td>+17.31%</td><td>+3.47%</td><td>+0.87%</td><td>+20.98%</td></tr><tr><td colspan="7">Time Reasoning</td></tr><tr><td>7</td><td>ResNet-50 (He et al., 2016)</td><td>Supervised</td><td>0.84%</td><td>5.14%</td><td>39.99%</td><td>0.46%</td></tr><tr><td>8</td><td>Swin-T (Liu et al., 2021)</td><td>Supervised</td><td>0.97%</td><td>5.53%</td><td>43.95%</td><td>0.72%</td></tr><tr><td>9</td><td>CLIP (Radford et al., 2021)</td><td>Zero-Shot</td><td>0.46%</td><td>2.42%</td><td>39.90%</td><td>0.25%</td></tr><tr><td>10</td><td>CLIP+ (Fu et al., 2022)</td><td>Fine-tune</td><td>1.00%</td><td>3.07%</td><td>43.09%</td><td>0.54%</td></tr><tr><td>11</td><td>CLIP+Seg (Fu et al., 2022)</td><td>Fine-tune</td><td>0.92%</td><td>3.15%</td><td>42.89%</td><td>0.71%</td></tr><tr><td>12</td><td>QR-CLIP (Ours)</td><td>Fine-tune</td><td>3.53%</td><td>10.90%</td><td>47.89%</td><td>2.01%</td></tr><tr><td colspan="3">Improvements (AVG: 134.38%)</td><td>+253%</td><td>+97.11%</td><td>+8.23%</td><td>+179.17%</td></tr></table>

Table 1. Summary of the performance for different baselines on the image location and time prediction.  $\dagger$  means fine-tune the original CLIP (Radford et al., 2021).  $\mathrm{AVG}^*$  : average relative lift.

$W_{i}^{owk}$  and  $W_{i}^{\nu}$  are the weights of the  $\mathsf{CLS}]_i^{owk}$  and  $[\mathsf{CLS}]_i^{\nu}$ $q$  in this place is the addition of weight vision- language features  $W_{i}^{owk}\times [\mathsf{CLS}]_{i}^{owk} + W_{i}^{\nu}\times [\mathsf{CLS}]_{i}^{\nu}$  q is the groundtruth features generated by  $F^{GT} = \mathrm{Enc}_t(GT)$

Location and Time Reasoning. We use the fused features  $\begin{array}{r}F^{fused} = \sum_{1}^{6}(W_{i}^{owk}\times [\mathsf{CLS}]_{i}^{owk} + W_{i}^{\nu}\times [\mathsf{CLS}]_{i}^{\nu}) \end{array}$  as our final features to predict the location and time. The prediction is completed by calculating the similarity between  $F^{fused}$  and the candidate location/time embeddings.

We believe that by using the CLIP pre- trained 400M open- world corpus and then fine- tuning it by adding additional [CLS] with location- and- time- specific data, it can basically reason about meta information. QR- CLIP will then improve its performance by retrieving valuable open- world knowledge and using it as auxiliary cues. Finally, the model balances vision and language embeddings, and by incorporating them into prediction, the model achieves its peak performance. The process is related to Horn's QR rule (Horn, 1984). Also, it mimics a procedure of information spreading (Wang et al., 2011): diverse individuals have diverse perspectives and attitudes regarding the same thing (sec 3.2), but combining them effectively fosters a more profound comprehension (sec 3.3).

# 4. Experiments

# 4.1. Training Settings

Dataset. We used two datasets: TARA dataset (Fu et al., 2022) and our collected OWK dataset. TARA dataset includes 15,429 samples. Each sample contains a news picture and the corresponding location, time description. Following the original setup, we train QR- CLIP on a train set contain ing 12,306 instances and evaluate our method using a test set containing 1,644 instances. The OwK dataset is derived from the WIT dataset (Srinivasan et al., 2021). Consider- . in the limited computation resource, we only use 122,408 texts from the 37.5 million entity- rich image- text examples in English Wikipedia that correspond to the countries and years as our open- world knowledge.

Evaluation Metrics. For a fair comparison, we first follow the same evaluation metrics on the TARA benchmark (Fu et al., 2022): Accuracy and Example- F1. Accuracy is calculated by comparing the predicted results with the entire labels. Example- F1 is calculated by comparing predictions with hierarchical labels:

$$
\mathrm{Example - F1} = \frac{1}{N}\sum_{i = 1}^{N}\frac{2|\mathrm{GT}_i\cap\mathrm{Pred}_i|}{|\mathrm{GT}_i| + |\mathrm{Pred}_i|}, \tag{8}
$$

where  $\mathrm{GT}_i$  represents the hierarchical label, and  $\mathrm{Pred}_i$  represents the hierarchical prediction. If the entire label is  $\{Zurich, Switzerland, Europe\}$ , the progressive hierarchical labels are the three combinations of true label as  $\{Zurich, Switzerland, Europe\}$ ,  $\{Switzerland, Europe'5948\}$  and  $\{Europe'\}$ . In addition, Rank@5 and F1- Score are utilized to evaluate the performance of the proposed method.

Implementation Details. QR- CLIP is based on CLIP+VIT- B/32 model with an input size of  $224 \times 224$ , and it is implemented on the PyTorch 1.10.1 platform with the Adam optimizer to update the neural network's weights and biases. The training batch size is 32, and the initial learning rate is  $1e - 6$ . Our model utilizes a pre- trained model and takes hours in the fine- tune process on an NVIDIA RTX 3090 GPU running CUDA 11.7.1.