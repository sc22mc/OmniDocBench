![](images/27bc69d9ea5aadd126add5372df2a6f6fb9b07f76f25853a403bb3616fccdcc6.jpg)  
Figure 3. We show the visualizations of 5 procedures of QR-CLIP. For each process, the reader can refer to Fig. 2

# 4.5. Limitation and Future Work

We are still in the early stages of investigating how to best use CLIP and the QR principle to explore open- world knowledge to support location and time reasoning. And the modules and techniques developed are simple but effective. In the future: 1) we will investigate more efficient and elegant implementations; 2) while addressing the limited computational resources, collect a larger OWK dataset as input candidates; 3) using multimodal OWKs to see if images from Instagram, Twitter, etc. could help with this task.

# 5. Conclusion

We designed a novel QR- CLIP model. It consists of two modules: 1) the Quantity module and 2) the Relevance module. Experiments show that it outperforms all previous SOTA on location and time reasoning by a wide margin. To show how our designed components affect the model, we conduct comprehensive ablation studies and verify that open- world knowledge is beneficial for solving our problem. We hope this paper will serve as a technical foundation for this study area and inspire more fascinating research.