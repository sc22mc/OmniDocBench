# 4.2. Comparative Results

Location Reasoning. We compare the results of our QRCLIP with other methods for location reasoning in Tab. 1. QR- CLIP, achieves accuracy of  $19.31\%$  Accuracy  $(\mathrm{R}\@ 1)$  Meanwhile, its Example- F1 score for the hierarchical labels is  $50.96\%$  .All the results clearly show that our method outperforms other methods.

(1) Compared with ResNet-50 (He et al., 2016) and SwinT (Liu et al., 2021), vanilla CLIP achieves  $7.93\%$  and  $4.41\%$  absolute improvement in location prediction accuracy (ID: 1,2,3). It indicates that compared with the vision model only trained on ImageNet (Deng et al., 2009), CLIP already possesses some knowledge for reasoning. Meanwhile, our QR-CLIP achieves a more significant advantage with  $16.13\%$  and  $12.61\%$  absolute improvements in terms of accuracy (ID: 1,2,6). These results show that traditional image classification methods cannot accomplish inference of the abstract information behind the images. While the CLIP model trained on large-scale internet data have the ability to identify locations based on image data, and QR-CLIP significantly enhances this capability.

(2) Besides, compared to  $\mathrm{CLIP\dagger}$  and the state-of-the-art method  $\mathrm{CLIP + Seg}$  QR-CLIP improves the accuracy by  $3.59\%$  and  $2.85\%$  absolute improvement, the F1-Score has increased by  $3.88\%$  and  $3.07\%$  respectively (ID:4,5,6). Other evaluation metrics also improved. The results show that QR-CLIP can effectively utilize open-world knowledge to establish a closer connection between image and location information through fine-tuning CLIP. However, we also find that the improvement in Example-F1 is not as obvious. We argue that this is because the mechanism of Example-F1: take the image of Fig. 1 as an example-the picture show many Arabia elements (turban and Arabic). It is not difficult for many models to recognize that this image was captured in the Middle East and to predict its hierarchical label as  $\{\mathrm{^{\circ}A s i a^{\circ}}\}$  .However, they failed when asked to predict the entire label { Riyadh, Saudi Arabia, Asia"). Therefore, the discrepancy in other metrics may be more noticeable.

Time Reasoning. Tab. 1 also presents the performance of our method and existing techniques for time reasoning. The Accuracy  $(\mathrm{R}\@ 1)$  of QR- CLIP is  $3.53\%$  and Example- F1 is  $47.89\%$  compared to the CLIP model, the two metrics have been absolutely improved by  $3.07\%$  and  $7.99\%$  ,respectively (ID: 9,12).Compared with  $\mathrm{CLIP\dagger}$  and  $\mathrm{CLIP + Seg}$  ,which are also based on CLIP fine- tuning, our method obtains  $2.53\%$  and  $2.61\%$  improvement in the accuracy of prediction time, respectively. Compared with traditional image classification methods, QR- CLIP has achieved absolute advantages in all metrics (ID: 7,8,12).In addition, due to the lack of timerelated information in the image, the prediction accuracy of fine- tuning CLIP methods for image time can only reach about  $1\%$  , which is significantly lower than the accuracy of location prediction (ID: 10,11). This is not surprising, also take the image on Fig. 1 as sample: even for humans, it is difficult to determine that  $\{^{\circ}O3 - O1 - 2O23^{\circ}\}$  is the time when this photograph was taken, if they are unfamiliar with Cristiano Ronaldo or some specific knowledge. Nevertheless, the method proposed in this paper is still effective (that our model achieves  $+253.00\%$  relative lift) for predicting time and significantly closes the gap with location prediction.

Table 2.Performance of additional [CLS] in QR-CLIP with different number and prediction methods. Whereas  $[\mathsf{CLS}^* ]_i^v$  refers to fusing all additional [CLS] by MLPs and then calculating the similarity with location and time labels,  $[\mathsf{CLS}]_i^v$  refers to calculating the similarity between each additional [CLS] with labels separately, and then using the  $([\mathsf{CLS}]_i^v$  -label) pair with the greatest similarity as the prediction.  $n$  represents the number of [CLS].  

<table><tr><td>ID Method</td><td>Accuracy (â‰ˆ Rank@1)</td><td>Rank@5</td><td>Example-F1</td></tr><tr><td colspan="4">Location Reasoning</td></tr><tr><td>13 CLIP+ [CLS*]y(n=2)</td><td>9.69%</td><td>27.17%</td><td>44.37%</td></tr><tr><td>14 CLIP+ [CLS*]y(n=4)</td><td>9.53%</td><td>26.25%</td><td>43.23%</td></tr><tr><td>15 CLIP+ [CLS*]y(n=6)</td><td>9.21%</td><td>27.05%</td><td>43.69%</td></tr><tr><td>16 CLIP+ [CLS]y(n=2)</td><td>16.84%</td><td>37.47%</td><td>49.22%</td></tr><tr><td>17 CLIP+ [CLS]y(n=4)</td><td>17.11%</td><td>37.60%</td><td>49.51%</td></tr><tr><td>18 CLIP+ [CLS]y(n=6)</td><td>17.25%</td><td>37.80%</td><td>49.98%</td></tr><tr><td>19 CLIP+ [CLS]y(n=8)</td><td>17.03%</td><td>37.62%</td><td>48.93%</td></tr><tr><td colspan="4">Time Reasoning</td></tr><tr><td>20 CLIP+ [CLS*]y(n=2)</td><td>0.98%</td><td>3.03%</td><td>42.18%</td></tr><tr><td>21 CLIP+ [CLS*]y(n=4)</td><td>1.03%</td><td>2.99%</td><td>43.98%</td></tr><tr><td>22 CLIP+ [CLS*]y(n=6)</td><td>1.08%</td><td>3.15%</td><td>43.62%</td></tr><tr><td>23 CLIP+ [CLS]y(n=2)</td><td>1.84%</td><td>5.14%</td><td>45.57%</td></tr><tr><td>24 CLIP+ [CLS]y(n=4)</td><td>1.92%</td><td>5.21%</td><td>45.63%</td></tr><tr><td>25 CLIP+ [CLS]y(n=6)</td><td>2.00%</td><td>5.37%</td><td>45.60%</td></tr><tr><td>26 CLIP+ [CLS]y(n=8)</td><td>1.53%</td><td>5.06%</td><td>45.15%</td></tr></table>

# 4.3.Ablation Study

Analysis on Additional [CLS].Following the network design process, all experiments of this part were conducted on the setting with only the step 1 in Quantity module (Sec 3.2).

As shown in Tab. 2, both of different [CLS] aggregation methods and different numbers of [CLS] can affect network performance. Comparing  $[\mathsf{CLS}_i^* ]$  and  $[\mathsf{CLS}]_i^v$  with the same number (i.e.,  $n = 2$  of [CLS], the latter has  $7.15\%$  and  $0.86\%$  higher location and time prediction accuracy (ID: 13,16,20,23). Besides, the performance of  $[\mathsf{CLS}_i^* ]$  is not significantly affected by the number of [CLS] (ID: 13- 15,20- 22). We argue that using MLP to aggregate the embeddings may destroy CLIP's original representation. It is better to separately calculate the similarities across each  $[\mathsf{CLS}]_i^v$  with the location and time labels and then select the one with the most significant value as the prediction. Then we analyze how different numbers of [CLS] affect the model performance. When  $n$  was increased to 8, no significant performance difference was observed, so we finally chose  $n = 6$  in the following experiments (ID: 17- 19, 24- 26). The results indicate that the additional [CLS] effectively