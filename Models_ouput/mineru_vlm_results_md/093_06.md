![](images/18ae694e5b7068cdd63a31af1ea7bb075c381d2150194345bd0467f1a31188f0.jpg)  
Fig. 2. Prediction images obtained by applying Soft PLS-DA and s-Soft PLS-DA models alongside the corresponding RGB images as references.

The application of Soft PLS- DA yielded promising results in both cross- validation and prediction of the external test set. It achieved sensitivity and specificity for the BMSB class exceeding  $90.0\%$  [36] (see Table I). Furthermore, employing sparse variable selection through the s- Soft PLS- DA algorithm maintained high classification performance while considering a reduced subset of 60 relevant spectral variables. The selected wavelengths can be grouped into five main spectral regions, corresponding to absorption bands associated with cellulose, hemicellulose, and lignin (1220- 1295 and  $1420 - 1480 \mathrm{nm}$ ). These are indicative of different background types. In addition, we selected absorption bands related to water, proteins, chitin, and lipids (980- 1070, 1330- 1350, and  $1370 - 1400 \mathrm{nm}$ ), reflecting the biochemical structure of insects' exoskeleton [36]. Subsequently, we applied both Soft PLS- DA and s- Soft PLS- DA models to the test set images to generate prediction images, facilitating the evaluation of classification performance across the entire images. This assessment allowed an overall understanding of the models' effectiveness in accurately classifying BMSB specimens and vegetal backgrounds

Fig. 2 presents some representative prediction images alongside their corresponding RGB counterparts. It is evident that, in general, the pixels are accurately classified into their respective classes. This observation highlights the challenge of detecting BMSB specimens against dark brown vegetal backgrounds using only RGB images, whereas the bugs are distinctly identified using SWIR- HSI. The prediction images of tree branches and soil as background reveal that spectral variable selection slightly enhances the classification of background pixels. To enhance BMSB detection capabilities for field applications, we integrated spectral information modeling with CNN algorithms to leverage spatial relationships among pixels. U- Net, in particular, is well suited for implementation on UAVs due to its ability to achieve satisfactory model performance with minimal computational resources [39].

To streamline network complexity in the spectral dimension, we effectively applied U- Net using only the spectral bands identified by s- Soft PLS- DA [36]. Notably, the spectral regions selected by s- Soft PLS- DA can serve as a basis for developing more affordable and rapid multispectral sensors, ideal for field monitoring using UAVs. When coupled with RGB cameras, these sensors can enable the deployment of efficient monitoring systems capable of autonomously detecting BMSB and providing real- time information on pest spread to farmers.

![](images/3727529b5ced31a98b79466a7c1581a7c8a658aba98ff8f383425e2a9b65d4ce.jpg)  
Fig. 3. Utilization of deep learning object detection models for BMSB detection with the false-color image constructed using manually selected wavebands of the NIR range.

Furthermore, light and cost- efficient hyperspectral cameras covering the SwIR range are increasingly accessible in the market [40]. This trend enhances the practicality of developing advanced monitoring solutions for agricultural applications, facilitating timely and accurate pest management decisions.

# B. Evaluation of Vis-NIR MSI

We conducted a pilot study employing a snapshot Vis- NIR UAV camera in the field to detect BMSB. The camera consists of two distinct acquisition devices capturing in the ranges 457- 593 nm (16 bands) and 605- 845 nm (15 bands), respectively. For our experiments, we used the latter 15 bands of the images. In one approach (camera I, Fig. 3), we performed manual wavelength selection based on the visibility of BMSB to construct the false- color image. We identified the wavelengths 727.24, 811.24, and 608.75 as suitable for BMSB detection.

Then, we trained two object detection models, namely, Faster RCNN and RetinaNet, on these images [41], [42]. The pretrained RetinaNet yielded the best results with an F1 score of  $78.45\%$ , precision of  $74.61\%$ , recall of  $76.39\%$ , and accuracy of  $78.45\%$  (see Table I). In the other approach, we utilized the entire Vis- NIR hypercube from camera II (see Fig. 3) to train the two object detection models. We customized the input layer of these models to accommodate the hyperspectral image with 15 channels. RetinaNet trained from scratch exhibited the best performance, achieving an F1 score of  $70.74\%$ , precision of  $84.25\%$ , recall of  $60.96\%$ , and accuracy of  $54.73\%$ .

# V. EDGE-BASED SMART STICKY TRAP IMAGING SYSTEM

As part of the HALY- ID project, we developed IoT imaging systems with integrated pheromone enhanced sticky traps to monitor the population of BMSB and to combat the spread of this invasive species. Sticky traps are widely used in orchards to attract and capture insects based on their color or pheromones. Agronomists can use these traps to estimate the insect populations by visiting their orchards regularly with manual trap inspection and insect counting. A preliminary version of the trap using mainly lab- based experiments and artificial insects to