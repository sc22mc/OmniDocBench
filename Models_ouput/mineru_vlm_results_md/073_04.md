# 3 Algorithm

In the following, we define multi- memory versions of two major learning algorithms, i.e., replicator dynamics and gradient ascent. Although we consider the learning of player X, that of player Y can be formulated in the same manner.

Definition 1 (expected future payoff). We define the expected future payoff from the distribution  $\mathbf{p}$  as

$$
\pi (\pmb {p},\mathbf{x},\mathbf{y})\coloneqq \sum_{t = 0}^{\infty}\pmb{M}^{t}(\pmb {p} - \pmb{p}^{\mathrm{st}})\cdot \pmb {u}, \tag{5}
$$

which is the total payoff player  $X$  obtains from the present round to the future.

In this definition, the stationary payoff  $\pmb{p}^{\mathrm{st}}\cdot \pmb {u} = \pmb{u}^{\mathrm{st}}$  is the offset term every round, and thus  $\pi (\pmb{p}^{\mathrm{st}},\mathbf{x},\mathbf{y}) =$  0.

Definition 2 (normalization). We define the normalization function  $\begin{array}{r}\mathrm{Norm}:\prod_{s\in \mathcal{S}}\mathbb{R}_+^m\mapsto \prod_{s\in \mathcal{S}}\operatorname {int}(\Delta^{m - 1}) \end{array}$  as

$$
\mathrm{Norm}(\mathbf{x}) = \left\{\frac{x^{a|s}}{\sum_{a'}x^{a'|s}}\right\}_{a,s}, \tag{6}
$$

In this definition,  $\mathrm{Norm}(\mathbf{x})$  satisfies the condition of probability variables for all  $s$  Based on these definitions, we formulate discretized MMRD and MMGA as Algorithm 1 and 2.

# Algorithm 1 Discretized MMRD

Input:  $\eta$

1:for  $t = 0,1,2,\dots$  do 2: X chooses  $\alpha$  with probability  $x^{a|s_i}$  3: (Y chooses  $b$  with probability  $y^{b|s_i}$  4:  $s_{i^{\prime}}\gets ab s_{i}^{- }$  5:  $x^{a|s_i}\gets x^{a|s_i} + \eta \pi (e_{i'},\mathbf{x},\mathbf{y})$  6:  $\mathbf{x}\gets \mathrm{Norm}(\mathbf{x})$  7:  $s_i\gets s_{i'}$  8:end for

Algorithm 1 (Discretized MMRD) takes its learning rate  $\eta$  as an input. In each time step, the players choose their actions following their strategies (lines 2 and 3), while the state is updated by their chosen actions (lines 4 and 7). Then, each player reinforces its strategy by how much payoff the chosen action brings up to the future. Here, note that for simplicity, this payoff is given by an expected payoff (line 5).

# Algorithm 2 Discretized MMGA

Input:  $\eta$ $\gamma$  1:for  $t = 0,1,2,\dots$  do 2: for  $a\in \mathcal{A}$ $s\in S$  do 3:  $\mathbf{x}^{\prime}\gets \mathbf{x}$  4:  $x^{\prime a|s}\gets x^{\prime a|s} + \gamma$  5:  $\Delta^{a|s}\gets \frac{u^{\mathrm{st}}(\mathrm{Norm}(\mathbf{x}^{\prime}),\mathbf{y}) - u^{\mathrm{st}}(\mathbf{x},\mathbf{y})}{\gamma}$  6: end for 7: for  $a\in \mathcal{A}$ $s\in S$  do 8:  $x^{a|s}\gets x^{a|s}(1 + \eta \Delta^{a|s})$  9: end for 10:  $\mathbf{x}\gets \mathrm{Norm}(\mathbf{x})$  11:end for