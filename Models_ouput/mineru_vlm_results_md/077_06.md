Table 1. Dataset details  

<table><tr><td>Dataset name</td><td>AD</td><td>2AA</td><td>4AA</td></tr><tr><td>Training set simulation time</td><td>100 ns</td><td>50 ns</td><td>50 ns</td></tr><tr><td>Test set simulation time</td><td>100 ns</td><td>1 μs</td><td>1 μs</td></tr><tr><td>MD integration step Δt</td><td>0.5 fs</td><td>0.5 fs</td><td>0.5 fs</td></tr><tr><td>Timewarp prediction time τ</td><td>0.5 × 106fs</td><td>0.5 × 106fs</td><td>0.5 × 106fs</td></tr><tr><td>No. of training peptides</td><td>1</td><td>200</td><td>1400</td></tr><tr><td>No. of training pairs per peptide</td><td>2 × 105</td><td>1 × 104</td><td>1 × 104</td></tr><tr><td>No. of test peptides</td><td>1</td><td>100</td><td>30</td></tr></table>

permutation equivariant coupling layer. If the flow only took  $2^{p}$  as input without  $z^{p}$  , then to maintain permutation equivariance, each coupling layer would have to unnaturally split the Cartesian components of  $z_{i}^{p}$  into two disjoint sets.

Translation and rotation equivariance Consider a transformation  $T = (R,a)$  that acts on  $x^{p}$  as follows:

$$
T x_{i}^{p} = R x_{i}^{p} + a,\quad 1\leq i\leq N, \tag{14}
$$

where  $R$  is a  $3\times 3$  rotation matrix, and  $a\in \mathbb{R}^3$  is a translation vector. We would like the model to satisfy  $p_{\theta}(Tx(t + \tau)|Tx(t)) = p_{\theta}(x(t + \tau)|x(t))$  .We achieve translation equivariance by subtracting the average position of the atoms in the initial molecular state (Appendix A.2). Rotation equivariance is not encoded in the architecture but is handled by data augmentation: each training pair  $(x(t),x(t + \tau))$  from  $\mathcal{D}$  is acted upon by a random rotation matrix  $R$  to form  $(Rx(t),Rx(t + \tau))$  in each iteration.

# 5. Training objective

The model is trained in two stages. During likelihood training, the model is trained via maximum likelihood on pairs of states from the trajectories in the dataset. During acceptance training, the model is fine- tuned to maximise the probability of MH acceptance. Let  $k$  index training pairs, such that  $\{(x^{(k)}(t),x^{(k)}(t + \tau))\}_{k = 1}^{K}$  represents all pairs of states at times  $\mathcal{T}$  apart in  $\mathcal{D}$  . During likelihood training, we optimise:

$$
\begin{array}{r}\mathcal{L}_{\mathrm{lik}}(\theta)\coloneqq \frac{1}{K}\sum_{k = 1}^{K}\log p_{\theta}(x^{(k)}(t + \tau)|x^{(k)}(t)). \end{array} \tag{15}
$$

Once likelihood training is complete, we add a fine- tuning stage to optimise the MH acceptance probability. Let  $x^{(k)}(t)$  be sampled uniformly from  $\mathcal{D}$  . Then, we use the model to sample  $\begin{array}{r}\tilde{r}_{\theta}^{(k)}(t + \tau)\sim p_{\theta}(\cdot |x^{(k)}(t)) \end{array}$  using Equation (3). Note that the sample value depends on  $\theta$  through  $f_{\theta}$  .We use this to optimise the acceptance probability in Equation (7) with respect to  $\theta$  .Let  $r_{\theta}(X,\tilde{X})$  denote the model- dependent term in the acceptance ratio in Equation (7):

$$
r_{\theta}(X,\tilde{X})\coloneqq \frac{\mu_{\mathrm{aug}}(\tilde{X})p_{\theta}(X|\tilde{X}^{p})}{\mu_{\mathrm{aug}}(X)p_{\theta}(\tilde{X}|X^{p})}. \tag{16}
$$

The acceptance objective is given by:

$$
\begin{array}{r}\mathcal{L}_{\mathrm{acc}}(\theta)\coloneqq \frac{1}{K}\sum_{k = 1}^{K}\log r_{\theta}(x^{(k)}(t),\tilde{x}_{\theta}^{(k)}(t + \tau)). \end{array} \tag{17}
$$

![](images/710f7dcca517ea8674254a76cc2e459ef3c532b1ae0bb8349fd75299322f1019.jpg)  
Figure 3. Alanine dipeptide experiments. (a) Ramachandran plots for MD and Timewarp samples generated according to Algorithm 1. (b) Free energy comparison for the two dihedral angles  $\phi$  and  $\psi$ . (c) Ramachandran plots for the conditional distribution of MD compared with the Timewarp model. Red cross denotes initial state. (d) Time dependence of the  $\phi$  dihedral angle of MD and the Markov chain generated with the Timewarp model.

Training to maximise the acceptance probability can lead to the model proposing changes that are too small: if  $\tilde{x}_{\theta}^{(k)}(t + \tau) = x^{(k)}(t)$ , then all proposals will be accepted. To mitigate this, during acceptance training, we use an objective which is a weighted average of  $\mathcal{L}_{\mathrm{acc}}(\theta)$ ,  $\mathcal{L}_{\mathrm{lik}}(\theta)$  and a Monte Carlo estimate of the average differential entropy,

$$
\begin{array}{r}\mathcal{L}_{\mathrm{ent}}(\theta)\coloneqq -\frac{1}{K}\sum_{k = 1}^{K}\log p_{\theta}(\tilde{x}_{\theta}^{(k)}(t + \tau)|x^{(k)}(t)). \end{array} \tag{18}
$$

The weighting factors for each term are hyperparameters.

# 6. Experiments

We evaluate Timewarp on small peptide systems. To compare with MD, we focus on the slowest transitions between metastable states, as these are the most difficult to traverse. To find these, we use time- lagged independent component analysis (TICA) (Perez- Hernandez et al., 2013), a linear dimensionality reduction technique that maximises the autocorrelation of the transformed coordinates. The slowest components, TIC 0 and TIC 1, are of particular interest. To measure the speed- up achieved by Timewarp, we compute the effective sample size per second of wall- clock time (ESS/s) for the TICA components. The ESS/s is given by:

$$
\mathrm{ESS / s} = \frac{M_{\mathrm{eff}}}{t_{\mathrm{sampling}}} = \frac{M}{t_{\mathrm{sampling}}(1 + 2\sum_{\tau = 1}^{\infty}\rho_{\tau})}, \tag{19}
$$

where  $M$  is the chain length,  $M_{\mathrm{eff}}$  is the effective number of samples,  $t_{\mathrm{sampling}}$  is the sampling wall- clock time, and  $\rho_{\tau}$  is the autocorrelation for the lag time  $\tau$  (Neal, 1993). The speed- up factor is defined as the ESS/s achieved by Timewarp divided by the ESS/s achieved by MD. Additional