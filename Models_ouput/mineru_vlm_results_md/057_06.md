TABLE I: Description template, where  $\alpha_{b}^{b}b_{j}^{b}$  and "nbr" denote the GUI element and its neighbor,  $\alpha$  and  $\beta$  denote high- and low-confidence element.  

<table><tr><td>Action</td><td>Id</td><td>Condition</td><td>Template</td><td>Example</td></tr><tr><td rowspan="3">TAP</td><td>1</td><td>(objtext/caption ≠ NULL) ∧ (objconfid &amp;gt; α)</td><td>Tap [objtext] [objclass]</td><td>Tap “OK” button</td></tr><tr><td>2</td><td>(objtext/caption ≠ NULL) ∧ (β &amp;lt; objconfid &amp;lt; α)</td><td>Tap [objtext] [objclass] at [objposition]</td><td>Tap “menu” icon at top left corner</td></tr><tr><td>3</td><td>(objtext/caption == NULL) ∨ (objconfid &amp;lt; β)</td><td>Tap the [objclass] [nbrrelation] [nbrtext]</td><td>Tap the checkbox next to “Dark Mode”</td></tr><tr><td rowspan="2">SCROLL</td><td>4</td><td>objtext ≠ NULL</td><td>Scroll [direction] [offset] of the screen to [objtext]</td><td>Scroll down half of the screen to “Advanced Setting”</td></tr><tr><td>5</td><td>objtext == NULL</td><td>Scroll [direction] [offset] of the screen</td><td>Scroll up a quarter of the screen</td></tr><tr><td rowspan="2">INPUT</td><td>6</td><td>(objtext/caption ≠ NULL) ∧ (objconfid &amp;gt; α)</td><td>Input [text] in the [objtext] edittext</td><td>Input “100” in the “Amount” edittext</td></tr><tr><td>7</td><td>(objtext == NULL) ∨ (objconfid &amp;lt; α)</td><td>Input [text] in the edittext [nbrrelation] [nbrtext]</td><td>Input “John” in the edittext below “Name”</td></tr></table>

![](images/b218f1074b72333d0e5568ef3f3acc3a973130a34c97f019b57a262ef32a1d5c.jpg)  
Fig. 7: Example of GUI understanding.

and the "None" element at the bottom. Note that the "Audio cue settings" element is omitted due to large spacing, which is consistent with human viewing.

2) Subtitle Creation: The main instruction of interest is to create a clear and concise subtitle description based on  $\{action, object\}$ . The global GUI information is further used to complement the description by  $\{position, relationship\}$ . Based on the action obtained in Section II-B the attribute of object inferred in Section II-C and the corresponding GUI element information retrieved in Section II-D1 we propose description templates for  $TAP$ , SCROLL, INPUT, respectively. A summary of description templates can be seen in Table I

For  $TAP$  action, the goal of the description should be clear and concise, e.g., tap OK button. However, we find that this simple description may not articulate all  $TAP$  actions due to two reasons. First, the text and caption of object are prone to errors or undetected, as the OCR- obtained text and the caption- obtained annotation are not  $100\%$  accurate. Second, there may be multiple objects with the same text on the GUI. To resolve this, we set up an object confidence value  $obj_{confid}$  as:

$$
obj_{confid} = \left\{ \begin{array}{ll}OCR_{confid} & \mathrm{if~}obj_{text}~\mathrm{is~unique~in~GUI}\\ 0 & \mathrm{otherwise} \end{array} \right. \tag{2}
$$

where  $OCR_{confid}$  denotes the confidence predicted by OCR. Note that the confidence value of icon object is calculated likewise by captioning. The smaller the confidence value, the less intuitive the object is. Therefore, only the object with the highest confidence value  $(obj_{confid} > \alpha)$  will apply the simplest and most straightforward description (Template 1), otherwise, we add the context of absolute position to help locate the object (Template 2). For the object whose text is not detected or recognized with low confidence, we leverage the context of its neighbor to help locate the target object (Template 3), e.g., tap the checkbox next to "Dark Mode".

It is easy to describe a SCROLL action by its scrolling direction and offset (Template 5), e.g., scroll up a quarter of the screen. However, such an offset description is not precise and intuitive. To address this, if a new element with text appears by scrolling, we add this context to help describe where to scroll to (Template 4), e.g., scroll down half of the screen to "Advanced Setting".

The description of INPUT is similar to  $TAP$ . For the high- confidence object with text (Template 6), it generates: Input [text] in the  $[obj_{text}]$  edittext. Different from the  $TAP$  descriptions, we do not apply the context of absolute position to help locate the low- confidence object. This is because the objects are gathering at the top when the keyboard pops up, so the absolute positioning may not help. Instead, we use the relative position of neighbor to describe the input object of which text is not detected or recognized with low confidence (Template 7), e.g., Input "John" in the edittext below "Name".

After generating the natural language description for each action clip, we embed the description into the recording as subtitles as shown in Fig. 6 In detail, we create the subtitles by using the Wand image annotation library [42] and synchronize the subtitle display at the beginning of each action clip.