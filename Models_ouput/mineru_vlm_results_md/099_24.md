<table><tr><td>Model</td><td>Auto-J Rating</td><td>GPT-4 Win-rate</td><td>Rank Auto-J</td><td>Î”</td></tr><tr><td>XwinLM 70b V0.1</td><td>5.694</td><td>95.57</td><td>1</td><td>1</td></tr><tr><td>LLaMA2 Chat 70B</td><td>5.678</td><td>92.66</td><td>2</td><td>2</td></tr><tr><td>XwinLM 13b V0.1</td><td>5.647</td><td>91.76</td><td>3</td><td>3</td></tr><tr><td>OpenChat V3.1 13B</td><td>5.532</td><td>89.49</td><td>4</td><td>8</td></tr><tr><td>WizardLM 13B V1.2</td><td>5.547</td><td>89.17</td><td>5</td><td>6</td></tr><tr><td>Vicuna 83B v1.3</td><td>5.570</td><td>88.99</td><td>6</td><td>5</td></tr><tr><td>Humpback LLaMA2 70B</td><td>5.498</td><td>87.94</td><td>7</td><td>11</td></tr><tr><td>XwinLM 7b V0.1</td><td>5.584</td><td>87.83</td><td>8</td><td>4</td></tr><tr><td>OpenBudddy-LLaMA2-70B-v10.1</td><td>5.448</td><td>87.67</td><td>9</td><td>14</td></tr><tr><td>OpenChat V2-W 13B</td><td>5.533</td><td>87.13</td><td>10</td><td>7</td></tr><tr><td>OpenBuddy-LLaMA-65B-v8</td><td>5.458</td><td>86.53</td><td>11</td><td>13</td></tr><tr><td>WizardLM 13B V1.1</td><td>5.497</td><td>86.32</td><td>12</td><td>12</td></tr><tr><td>OpenChat V2 13B</td><td>5.519</td><td>84.97</td><td>13</td><td>9</td></tr><tr><td>Humpback LLaMA 65B</td><td>5.379</td><td>83.71</td><td>14</td><td>19</td></tr><tr><td>Vicuna 13B v1.3</td><td>5.388</td><td>82.11</td><td>15</td><td>18</td></tr><tr><td>OpenBuddy-LLaMA-30B-v7.1</td><td>5.391</td><td>81.55</td><td>16</td><td>17</td></tr><tr><td>LLaMA2 Chat 13B</td><td>5.518</td><td>81.09</td><td>17</td><td>10</td></tr><tr><td>OpenChat-13B</td><td>5.437</td><td>80.87</td><td>18</td><td>15</td></tr><tr><td>OpenBuddy-Falcon-40B-v9</td><td>5.373</td><td>80.70</td><td>19</td><td>20</td></tr><tr><td>UltraLM 13B</td><td>5.342</td><td>80.64</td><td>20</td><td>22</td></tr><tr><td>OpenChat8192-13B</td><td>5.429</td><td>79.54</td><td>21</td><td>16</td></tr><tr><td>OpenCoderPlus-15B</td><td>5.357</td><td>78.70</td><td>22</td><td>21</td></tr><tr><td>OpenBuddy-LLaMA2-13B-v11.1</td><td>5.340</td><td>77.49</td><td>23</td><td>23</td></tr><tr><td>Vicuna 7B v1.3</td><td>5.332</td><td>76.84</td><td>24</td><td>25</td></tr><tr><td>WizardLM 13B</td><td>5.247</td><td>75.31</td><td>25</td><td>32</td></tr><tr><td>JinaChat</td><td>5.319</td><td>74.13</td><td>26</td><td>26</td></tr><tr><td>airoboro65B</td><td>5.318</td><td>73.91</td><td>27</td><td>27</td></tr><tr><td>airoboro63B</td><td>5.289</td><td>73.29</td><td>28</td><td>30</td></tr><tr><td>Guanaco 65B</td><td>5.313</td><td>71.80</td><td>29</td><td>29</td></tr><tr><td>LLaMA2 Chat 7B</td><td>5.334</td><td>71.37</td><td>30</td><td>24</td></tr><tr><td>Vicuna 13B</td><td>5.314</td><td>70.43</td><td>31</td><td>28</td></tr><tr><td>OpenBuddy-Falcon-7b-v6</td><td>5.214</td><td>70.36</td><td>32</td><td>34</td></tr><tr><td>Baize-v2 13B</td><td>5.165</td><td>66.96</td><td>33</td><td>38</td></tr><tr><td>LLaMA 33B OASST RLHF</td><td>5.173</td><td>66.52</td><td>34</td><td>37</td></tr><tr><td>Minotaur 13B</td><td>5.210</td><td>66.02</td><td>35</td><td>36</td></tr><tr><td>Guanaco 33B</td><td>5.212</td><td>65.96</td><td>36</td><td>35</td></tr><tr><td>Nous Hermes 13B</td><td>5.271</td><td>65.47</td><td>37</td><td>31</td></tr><tr><td>Vicuna 7B</td><td>5.237</td><td>64.41</td><td>38</td><td>33</td></tr><tr><td>Baize-v2 7B</td><td>5.083</td><td>63.85</td><td>39</td><td>39</td></tr><tr><td>LLaMA 33B OASST SFT</td><td>4.985</td><td>54.97</td><td>40</td><td>41</td></tr><tr><td>Guanaco 13B</td><td>5.027</td><td>52.61</td><td>41</td><td>40</td></tr><tr><td>ChatGLM2 6B</td><td>4.846</td><td>47.13</td><td>42</td><td>46</td></tr><tr><td>Guanaco 7B</td><td>4.943</td><td>46.58</td><td>43</td><td>43</td></tr><tr><td>Falcon 40B Instruct</td><td>4.934</td><td>45.71</td><td>44</td><td>44</td></tr><tr><td>Alpaca Farm PPO Sim (GPT-4) 7B</td><td>4.978</td><td>44.10</td><td>45</td><td>42</td></tr><tr><td>Pythia 12B SFT</td><td>4.809</td><td>41.86</td><td>46</td><td>47</td></tr><tr><td>Alpaca Farm PPO Human 7B</td><td>4.907</td><td>41.24</td><td>47</td><td>45</td></tr><tr><td>Cohere Chat</td><td>4.524</td><td>29.57</td><td>48</td><td>51</td></tr><tr><td>Cohere</td><td>4.522</td><td>28.39</td><td>49</td><td>52</td></tr><tr><td>Alpaca 7B</td><td>4.658</td><td>26.46</td><td>50</td><td>48</td></tr><tr><td>Pythia 12B OASST SFT</td><td>4.620</td><td>25.96</td><td>51</td><td>49</td></tr><tr><td>Falcon 7B Instruct</td><td>4.537</td><td>23.60</td><td>52</td><td>50</td></tr><tr><td>Baichuan-13B-Chat</td><td>4.291</td><td>21.80</td><td>53</td><td>53</td></tr></table>

Table 24: Values and ranking by Auto-J and GPT-4 for open-source LLMs on AlpacaEval. Value of AUTO-J is the model's average rating on AlpacaEval dataset assigned by AUTO-J in single-response evaluation protocol, and value of GPT-4 is the model's win-rate against Davinci003 determined by GPT-4 on AlpacaEval dataset.  $\Delta = \mathsf{Rank}_{\mathsf{Au t o - J}} - \mathsf{Rank}_{\mathsf{GPT - 4}}$