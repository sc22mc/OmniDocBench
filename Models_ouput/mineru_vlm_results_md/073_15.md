which shows that X's payoff in the stationary state is  $u^{*}$  , regardless of Y's strategy  $y$

Below, we show that if X uses another strategy  $x\neq x^{*}\mathbf{1}$  , there always is Y's strategy such that  $v^{\mathrm{st}}>$ $v^{*}\Leftrightarrow u^{\mathrm{st}}< u^{*}$  . As X's non- equilibrium strategy, we assume the case  $x_{1}\neq x^{*}$  representatively. Then, Y's strategy  $y = y^{*}\mathbf{1} + \mathrm{d}y_{1}e_{1}$  with sufficiently small  $\mathrm{dy_1}$  satisfies

$$
\begin{array}{r}\pmb{p}^{\mathrm{st}} = \left( \begin{array}{llll}x_{1}(y^{*} + \mathrm{d}y_{1}) & x_{2}y^{*} & x_{3}y^{*} & x_{4}y^{*}\\ x_{1}(\tilde{y}^{*} - \mathrm{d}y_{1}) & x_{2}y^{*} & x_{3}\tilde{y}^{*} & x_{4}\tilde{y}^{*}\\ \tilde{x}_{1}(y^{*} + \mathrm{d}y_{1}) & \tilde{x}_{2}y^{*} & \tilde{x}_{3}y^{*} & \tilde{x}_{4}y^{*}\\ \tilde{x}_{1}(\tilde{y}^{*} - \mathrm{d}y_{1}) & \tilde{x}_{2}\tilde{y}^{*} & \tilde{x}_{3}\tilde{y}^{*} & \tilde{x}_{4}\tilde{y}^{*} \end{array} \right)\pmb{p}^{\mathrm{st}}. \end{array} \tag{A27}
$$

In this equation, we approximate  $\pmb{p}^{\mathrm{st}}\approx \pmb{p}^{\mathrm{st}(0)} + \pmb{p}^{\mathrm{st}(1)}$  , where  $\pmb{p}^{\mathrm{st}(k)}$  describes the  $O((\mathrm{dy}_1)^k)$  term in  $\pmb{p}^{\mathrm{st}}$  .We can derive these 0- th and 1- st order terms by comparing the left- hand and right- side of this equation. Here, the 0- th order term satisfies  $\pmb{p}^{\mathrm{st}(0)}\cdot \pmb {u} = \pmb{u}^{*}$  , which means that the term does not contribute to the deviation from the Nash equilibrium payoff. On the other hand, the 1- st order term gives

$$
\begin{array}{rl} & {\pmb{p}^{\mathrm{st}(1)} = p_1^{\mathrm{st}(0)}\mathrm{d}y_1(+x_1, - x_1, + \tilde{x}_1, - \tilde{x}_1)^\mathrm{T}}\\ & {\Rightarrow v^{\mathrm{st}(1)} = p_1^{\mathrm{st}(0)}\mathrm{d}y_1\underbrace{(v_1 - v_2 - v_3 + v_4)}_{= v\cdot \mathbf{1}_z\neq 0}(x_1 - x^*).} \end{array} \tag{A29}
$$

Here, we use  $\mathbf{1}_z\coloneqq (+1, - 1, - 1, + 1)$  . Thus, in the leading order,  $v^{\mathrm{st}(1)} > v^{*}\Leftrightarrow u^{\mathrm{st}(1)}< u^{*}$  holds by taking  $\mathrm{dy}_1 > 0$  if  $\pmb {v}\cdot \mathbf{1}_z(x_1 - x^*) > 0$  , while by taking  $\mathrm{dy}_1< 0$  if  $\pmb {v}\cdot \mathbf{1}_z(x_1 - x^*)< 0$  . In other words, X's minimax strategy is  $x = x^{*}\mathbf{1}$  .Similarly, we can prove that Y's minimax strategy is  $y = y^{*}\mathbf{1}$  .Thus, the Nash equilibrium is given by  $(x,y) = (x^{*}\mathbf{1},y^{*}\mathbf{1})$

# B Analysis of Learning Dynamics

# B.1 Simpler MMGA for Two-action Games

This section is concerned with the contents in Section 4.2 in the main manuscript.

Especially in two- action games, we can use the formulation of Assumption 1 in the main manuscript. By replacing the strategies  $(\mathbf{x},\mathbf{y})$  by  $(x,y)$  , we can formulation another simpler algorithm of MMGA as

# Algorithm A1 Discretized MMGA for two-action

Input:  $\eta$ $\gamma$  1: for  $t = 0,1,2,\dots$  do 2: for  $i = 1,2,\dots ,|S|$  do 3:  $x^{\prime}\leftarrow x$  4:  $x_{i}^{\prime}\leftarrow x_{i}^{\prime} + \gamma$  5:  $\Delta_{i}\leftarrow (1 - x_{i})\frac{v^{\mathrm{st}}(x^{\prime},y) - u^{\mathrm{st}}(x,y)}{\gamma}$  6: end for 7: for  $i = 1,2,\dots ,|S|$  do 8:  $x_{i}\leftarrow x_{i}(1 + \Delta_{i})$  9: end for 10:  $\mathbf{x}\leftarrow \mathrm{Norm}(\mathbf{x})$  11: end for

There is a major difference between the original and simpler MMGAs in lines 4 and 5. The equivalence