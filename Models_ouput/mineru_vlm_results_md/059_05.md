probability distribution over the generic vocabulary  $V_{g}$  at the decoding time step  $t$  is computed as,

$$
P_{v_g} = \mathrm{softmax}\big(\mathrm{FC}_1(\mathbf{o}_t)\big), \tag{4}
$$

where  $\mathrm{FC_1}$  is a fully connected layer.

To calculate the probability distribution over the history- aware vocabulary  $V_{h}$  , we adopt a maxpooling layer over the context- updated history memory  $\mathbf{C}_s$  a fully connected layer, and a softmax function as follows,

$$
P_{v_h} = \mathrm{softmax}\big(\mathrm{FC}_2(\mathrm{max - pooling}(\mathbf{C}_s))\big), \tag{5}
$$

where  $\mathrm{FC_2}$  is a fully connected layer.

The final word probability distribution at time step  $t$  is computed by using a switching mechanism between  $P_{v_g}$  and  $P_{v_h}$  as follows,

$$
P = \alpha_{v_g}*P_{v_g} + \alpha_{v_h}*P_{v_h}, \tag{6}
$$

where  $\alpha_{v_g}$  and  $\alpha_{v_h}$  is the switching probability of generating from generic vocabulary or copying from history conversations.  $\alpha_{v_g}$  and  $\alpha_{v_h}$  is calculated as follows,

$$
\begin{array}{rl} & {[\alpha_{v_g},\alpha_{v_h}] =}\\ & {\quad \mathrm{softmax}\big(\mathrm{FC}_3\big([o_j;\mathrm{max - pooling}(\mathbf{C}_s)]\big)\big),} \end{array}
$$

where  $\mathrm{FC_3}$  is a fully connected layer, and  $[;]$  is a concatenation operation over the last dimension.

# 3.4 Model Training

We train the model to maximize the generation probability of the target response, given the current conversation context and history conversations in an end- to- end manner. The loss function of HAHT is defined as,

$$
\mathcal{L} = -\sum_{t = 1}^{n_y}\log \big(P(y_j|X,H,y_{< t})\big), \tag{8}
$$

where  $X$  denotes the current conversation context,  $H$  denotes all history conversations,  $y_{< t}$  denotes tokens before time step  $t$  ,and  $n_{y}$  denotes the length of the ground truth response.

# 4Experimental Settings

In this section, we introduce the experimental dataset, evaluation metrics, baseline methods, and model settings.

<table><tr><td rowspan="2">Session number</td><td colspan="2">Train</td><td colspan="2">Valid</td><td colspan="2">Test</td></tr><tr><td>Conv.</td><td>Utter.</td><td>Conv.</td><td>Utter.</td><td>Conv.</td><td>Utter.</td></tr><tr><td>1*</td><td>8939</td><td>131,438</td><td>1000</td><td>7,801</td><td>1015</td><td>6,634</td></tr><tr><td>2</td><td>4000</td><td>46,420</td><td>500</td><td>5,897</td><td>501</td><td>5,939</td></tr><tr><td>3</td><td>4000</td><td>47,259</td><td>500</td><td>5,890</td><td>501</td><td>5,924</td></tr><tr><td>4</td><td>1001</td><td>11,870</td><td>500</td><td>5,904</td><td>501</td><td>5,940</td></tr><tr><td>5</td><td>-</td><td>-</td><td>500</td><td>5,964</td><td>501</td><td>5,945</td></tr><tr><td>Total</td><td>-</td><td>236,987</td><td>-</td><td>31,456</td><td>-</td><td>30,382</td></tr></table>

Table 1: The statistics of Facebook Multi- Session Chat (Facebook MSC) Dataset. Session number  $i$  indicates there are  $i - 1$  history conversation sessions that happen before the last conversation session. \* Session 1 does not contain history conversation sessions.

# 4.1 Experimental Dataset

The experiments are performed on a large dataset, i.e.,Facebook MULTI- SESSION CHAT (Facebook MSCXu et al.,2022).It is a crowdsourced dataset consisting of multi- session conversations, where the interlocutors learn about each other's interests and discuss the things they have understood from past sessions. The number of history conversations in Facebook MSC varies from 1 to 4. Session number  $i$  indicates there are  $i - 1$  history conversations happening before the last conversation session. The statistics of the Facebook MSC dataset are summarized in Table 1. As session 1 does not have history conversations, we evaluate our model on session 2- 5.

# 4.2 Evaluation Metrics

We conduct both automatic and human evaluations to demonstrate the effectiveness of the proposed model. For automatic evaluations, we leverage BLEU- 2,BLEU- 3 Papineni et al.,2002),and ROUGE- L (Lin and Och, 2004) to measure word overlaps between the generated response text and ground truth text.

Moreover, we also randomly sample 50 MSCs from the test set to conduct human evaluations. We present all the history conversation sessions, current conversation context, and the generated responses to three well- educated annotators. The annotators will evaluate the quality of the generated responses from the following three aspects:

Readability: measures whether the generated responses are natural and fluent. Context Relevancy: measures whether the generated responses are correlated with the current conversation context. History Relevancy: measures whether the generated responses are correlated with history con