[
    {
        "type": "text",
        "text": "REFERENCES   \nAmanda Askell, Yuntao Bai,Anna Chen,Dawn Drain,Deep Ganguli, Tom Henighan,Andy Jones, Nicholas Joseph, Ben Mann,Nova DasSarma, et al.A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861,2021.   \nAlan Baddeley. Working memory. Science,255(5044):556-559,1992.   \nMohammad Bavarian, Heewoo Jun,Nikolas Tezak, John Schulman, Christine McLeavey, Jerry Tworek,and Mark Chen. Efficient training of language models to fillin the middle. arXiv preprint arXiv:2207.14255,2022.   \nIz Beltagy,Mathew EPeters,and Arman Cohan. Longformer:The long-document transformer. arXiv preprint arXiv:2004.05150,2020.   \nAmanda Bertsch, Uri Alon, Graham Neubig,and Matthew Gormley. Unlimiformer: Long-range transformers with unlimited length input. Advances in Neural Information Processing Systems,36, 2023.   \nAydar Bulatov, Yury Kuratov,and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information ProcessingSystems,35:11079-11091,2022.   \nAydar Bulatov,Yuri Kuratov,andMikhail SBurtsev.Scaling transformerto1m tokens and beyond with rmt. arXiv preprint arXiv:2304.11062, 2023.   \nHoward Chen,Ramakanth Pasunuru,Jason Weston,andAsli Celikyilmaz.Walking down the memory maze: Beyond context limit through interactive reading.arXiv preprint arXiv:2310.05029,2023.   \nAlexis Chevalier,Alexander Wettig,Anirudh Ajith,and Danqi Chen.Adaptinglanguage models to compress contexts. arXiv preprint arXiv:2305.14788,2023.   \nRewon Child, Scott Gray,Alec Radford,and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509,2019.   \nKrzysztof Choromanski,ValeriiLikhosherstov,avidDohan,Xingyou Song,Andreea Gane,amas Sarlós,Peter Hawkins,Jared Davis,Afroz Mohiuddin,Lukasz Kaiser,David Belanger,Lucy J. Colwell,and Adrian Weller. Rethinking attention with performers.ArXiv,abs/2009.14794,2020.   \nGregoire Deletang,Anian Ruoss,Paul-Ambroise Duquenne,Elliot Catt,Tim Genewein,Christopher Mattern, Jordi Grau-Moya,Li Kevin Wenliang,Matthew Aitchison,Laurent Orseau,et al. Language modeling is compression. arXiv preprint arXiv:2309.10668,2023.   \nJiayu Ding,Shuming Ma,Li Dong,Xingxing Zhang,Shaohan Huang,Wenhui Wang,and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486,2023.   \nRandall W Engle,Stephen W Tuholski, James ELaughlin,and Andrew RA Conway.Working memory,short-termmemory,and general fluid intelligence:alatent-variable approach. Journal of experimental psychology:General,128(3):309,1999.   \nK Anders Ericsson,William GChase,and Steve Faloon.Acquisition of a memory skill. Science,208 (4448):1181-1182,1980.   \nRinon Gal, Yuval Alaluf,Yuval Atzmon,Or Patashnik,Amit HBermano,Gal Chechik,and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618,2022.   \nLeo Gao,Stella Biderman,Sid Black,Laurence Golding,Travis Hoppe,Charles Foster,Jason Phang, Horace He,Anish Thite,Noa Nabeshima,Shawn Presser,and Connor Leahy.The Pile:An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.0oo27,2020.   \nTao Ge,Hu Jing,Li Dong,Shaoguang Mao,Yan Xia, Xun Wang，Si-Qing Chen,and Furu Wei.Extensible prompts for language models on zero-shot language style customization. InA.Oh，T.Neumann,A.Globerson,K.Saenko,M.Hardt,and S.Levine(eds.)，Advances in Neural Information Processing Systems,volume 36, pp.35576-35591. Curran Associates,Inc.,2023. URL https://proceedings.neurips.cc/paper_files/paper/ 2023/file/6fcbfb3721c1781728b10c6685cc2f6c-Paper-Conference.pdf. ",
        "page_idx": 0
    }
]