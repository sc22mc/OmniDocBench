[
    {
        "type": "text",
        "text": "Edward JHu, Yelong Shen,Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,Shean Wang,Lu Wang, and Weizhu Chen.Lora:Low-rank adaptation of large language models.arXiv preprint arXiv:2106.09685,2021.   \nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang,and Lili Qiu. Llmlingua: Compressing prompts for accelerated inference of large language models.arXiv preprint arXiv:2310.05736, 2023a.   \nZhiying Jiang,Matthew Yang,Mikhail Tsirlin,Raphael Tang,Yiqin Dai,and Jimmy Lin.â€œlowresource\"text classification: A parameter-free classification method with compressors. In Findings of the Association for Computational Linguistics: ACL 2023,pp. 6810-6828,2023b.   \nMark A. Kramer. Nonlinear principal component analysis using autoassociative neural networks. Aiche Journal,37:233-243,1991.   \nPatrick Lewis,Ethan Perez,Aleksandra Piktus,Fabio Petroni, Vladimir Karpukhin,Naman Goyal, Heinrich Kuttler,MikeLewis,Wen-tau Yih,TimRocktaschel,etal. Retrieval-augmented generation for knowledge-intensive nlp tasks.Advances in Neural Information Processing Systems,33: 9459-9474,2020.   \nNelsonF.Liu,KevinLin,John Hewitt,Ashwin Paranjape,Michele Bevilacqua,Fabio Petroni,and Percy Liang. Lost in the middle: How language models use long contexts, 2023.   \nEleanor AMaguire,Elizabeth R Valentine,John MWilding,and Narinder Kapur.Routes to remembering: the brains behind superior memory. Nature neuroscience,6(1):90-95,2003.   \nJesse Mu, Xiang Lisa Li,and Noah Goodman. Learning to compress prompts with gist tokens. arXiv   \npreprint arXiv:2304.08467,2023.   \nOpenAI. Gpt-4 technical report. ArXiv,abs/2303.08774,2023.   \nLong Ouyang, Jeffrey Wu, Xu Jiang,Diogo Almeida, Carroll Wainwright,Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray,et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems,35: 27730-27744,2022.   \nKishore Papineni,Salim Roukos,Todd Ward,and Wei Jing Zhu.Bleu:a method for automatic evaluation of machine translation.10 2002. doi:10.3115/1073083.1073135.   \nGuangyue Peng, Tao Ge, Si-Qing Chen, Furu Wei, and Houfeng Wang. Semiparametric language models are scalable continual learners. arXiv preprint arXiv:23o3.01421,2023.   \nGuanghui Qin and Benjamin Van Durme. Nugget: Neural agglomerative embeddings of text. In Andreas Krause,Emma Brunskill,Kyunghyun Cho,Barbara Engelhardt,Sivan Sabato,and JonathanScarlett(eds.)Proceedingsofthe40th InternationalConferenceonachineLeaing, volume 202 of Proceedings of Machine Learning Research, pp.28337-28350. PMLR,23-29 Jul 2023.URL https://proceedings.mlr.press/v202/qin23a.html.   \nJack W Rae,Anna Potapenko,Siddhant M Jayakumar,and Timothy PLillicrap.Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507,2019.   \nCharlie Snell,Dan Klein,and Ruiqi Zhong.Learning by distilling context.arXiv preprint arXiv:2209.15189,2022.   \nWoomin Song,Seunghyuk Oh,Sangwoo Mo,Jaehyung Kim,Sukmin Yun,Jung-Woo Ha,andJinwoo Shin. Hierarchical context merging: Better long context understanding for pre-trained llms. In The Twelfth International Conference on Learning Representations, 2024.   \nHugo Touvron,ThibautLavril,Gautier Izacard,Xavier Martinet,Marie-Anne Lachaux,imothee Lacroix,BaptisteRoziere,Naman Goyal,Eric Hambro,Faisal Azhar,Aur'elienRodriguez,Armand Joulin,EdouardGraveandGuillaumeLample.Llama:Oenandefcientfoundationlanguage models. ArXiv,abs/2302.13971,2023a. ",
        "page_idx": 0
    }
]