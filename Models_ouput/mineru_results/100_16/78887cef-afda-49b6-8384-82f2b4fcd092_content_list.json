[
    {
        "type": "text",
        "text": "Algorithm 2: Best-First Aggregation and Greedy aggregation ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": ": $M$ $m$ $z _ { i = 1 } ^ { n ^ { m } }$ $\\overline { { z } } _ { 1 \\dots n }$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1- Best-First Aggregation ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "2foreach chain $m = 1 , 2 , . . . , M$ do   \n3 Compue the sum of edge weigtsfor $m$ -th chain as $\\begin{array} { r } { V ^ { m } = \\sum _ { i = m } ^ { n ^ { m } } V _ { i - 1 , i } } \\end{array}$   \n4 end   \n5 Get the best chain among $M$ chains by performing $m ^ { * } = a r g m a x _ { m } \\left\\{ V ^ { m } \\right\\}$   \n6 Assigntheagegate ehai sti seschsin $\\overline { { z } } _ { 1 \\dots n } : = \\left\\{ z _ { i = 1 } ^ { n ^ { m ^ { * } } } \\right\\}$   \n7- Greedy Aggregation   \n8 $\\overline { { z } } _ { 1 } : = z _ { 1 } ^ { m ^ { * } }$ where $m ^ { * } = a r g m a x _ { m } \\{ V _ { 1 } ^ { m } \\}$   \n9 for each aggregation step $i = 2 , . . . , n$ do   \n10 foreach chain $m = 1 , 2 , . . . , M$ do   \n11 Collect $J ^ { m } = \\big \\{ j , s i m \\left( \\overline { { z } } _ { i - 1 } , z _ { j } ^ { m } \\right) > 0 . 7 ; j \\in n ^ { m } \\big \\} .$   \n12 $\\operatorname { G e t } j ^ { * , m } = a r g m a x _ { j \\in J ^ { m } } \\left\\{ V _ { j , j + 1 } ^ { m } \\right\\}$   \n13 end   \n14 Get the best next reasoning step by performing: $\\overline { { z } } _ { i } = z _ { j * + 1 } ^ { m }$ where   \n$j * = a r g m a x _ { j \\in \\{ j ^ { * , m } \\} _ { m = 1 } ^ { M } } \\left\\{ V _ { j , j + 1 } ^ { m } \\right\\} .$   \n15end   \n16 Obtain the aggregated chain $\\overline { { z } } _ { 1 \\dots n }$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "DTHOUGHT STRUCTURES AGGREGATION ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "After completing the reasoning in Heterogeneous Tree Structures,the aggregation process of BoT first extracts the best reasoning chain from each tree and then combines them using either the Best-First or Greedy aggregation method into a single reasoning chain. More details of these two aggregation methods can be accessed in the source code examples/BoostingOfThought/BoT_aggregator.py. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As shown in the first block of the algorithml6 the Best-first aggregation is a straightforward approach foraggregation as it directly extracts the chain with the highest sum of edge weights.This method is fast and stable.It typically guarantees competitive performance as the subsequent experience is able to be generated by analyzing the best chain among obtained reasoning chains.However,it can only select existing chains without making effective adjustments. Greedy aggregation is more advanced as it combines reasoning steps from different chains to produce a new,better reasoning chain with the highest edge weights. The greedy aggregation procedure in algorithm $\\boxed { 1 6 }$ contains two steps.It first collects reasoning steps that are similar to the aggregated reasoning step $z _ { i - 1 }$ . Thus, the next aggregated reasoning step is selected from the next reasoning steps of this colected set by maximizing the edge weights.And,sim is the similarity function that uses LLMs to assess the percentage of identical words and mathematical numbers shared between two paragraphs. O.7 is an empirical threshold obtained from experiments. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "EINFLUENCEOFTHEBADFEEDBACK ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The feedback obtained by evaluating the aggregated reasoning chain with LLMs may include analysis of limited usefulness and completely incorrect conclusions and error reports.This issue typically arises due to the nature of LLMs, which are language models and do not inherently verify the accuracy of the generated text.Additionally,the capabilities ofLLMs,such as gpt-3.5-turbo,are constrained when used as validators for mathematical problems. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "A direct example is presented in Table7 The analysis report concludes that “The final result obtained in Step 3 is 8O,which is mathematically equal to 24.”Even worse,the experience further contains that “the reasoning chain is correct”and “No errors were found in the reasoning steps.. Using the prompt with this experience as the input in the first iteration, BoT is misled to generate wrong reasoning steps,and the corresponding aggregated chain can be seen at the beginning of Table $\\boxed { 8 }$ It is evident ",
        "page_idx": 0
    }
]