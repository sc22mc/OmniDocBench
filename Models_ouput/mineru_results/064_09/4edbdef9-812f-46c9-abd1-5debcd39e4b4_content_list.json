[
    {
        "type": "text",
        "text": "References   \nAlayrac,J.-B.,Donahue,J.,Luc,P.,Miech,A.,Bar,I Hasson,Y.,Lenc,K.,Mensch,A.,Millican,K.,Reynolds, M.,etal.Flamingo: a visual language model for few-shot learning.In NeurIPS,2022.   \nAllott, N. Relevance theory. In Perspectives on linguistic pragmatics,pp.57-98.2013.   \nBommasani,R., Hudson, D. A.， Adeli, E.,Altman, R., Arora,S.,von Arx,S.,Bernstein,M.S.,Bohg,J.,Bosselut,A.,Brunskill,E.,et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.   \nBrown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D., Dhariwal, P.,Neelakantan,A.， Shyam, P.， Sastry, G., Askell,A., etal.Language models are few-shot learners. In NeurIPS,2020.   \nChowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,ishra G.,Roberts,A.,Barham,P.,Chung,H. W.,Suton,C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311,2022.   \nConforti, C.，Berndt,J.，Pilehvar，M.T.，Giannitsarou, C., Toxvaerd,F.,and Collier, N. Stander:An expertannotated dataset for news stance detection and evidence retrieval. In ACL,2020.   \nCrowder, J.A.and Friess,S. Artificial psychology: The psychology of ai. People,2(3):4-5,2012.   \nDegrave,J.,Felici,F.,Buchli,J.,Neunert,M.,Tracey,B., Carpanese,F.,Ewalds,T., Hafner,R., Abdolmaleki, A., de Las Casas,D.,et al. Magnetic control of tokamak plasmas through deep reinforcement learning. Nature, 602(7897):414-419,2022.   \nDeng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei, L. Imagenet: A large-scale hierarchical image database. In CVPR,2009.   \nDosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn, D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,M., Heigold,G.,Gelly,S.,et al. An image is worth16x16 words: Transformers for image recognition at scale. In ICLR,2020.   \nFabbri，A.R.，Li,I.，She，T.，Li,S.，and Radev,D. Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model.In ACL, 2019.   \nFu,X.,Zhou,B.,Chandratreya,I., Vondrick,C.,and Roth, D.There's a time and place for reasoning beyond the image.In ACL,2022.   \nHe,K., Zhang,X.,Ren,S.,and Sun,J.Deep residual learning for image recognition. In CVPR,2016.   \nHe,K.,Fan,H.,Wu,Y.,Xie,S.,and Girshick,R.Momentum contrast for unsupervised visual representation learning. In CVPR,2020.   \nHorn,L. Towards a new taxonomy for pragmatic inference: Q-and r-based implicature.Meaning,Form, and Use in Context: Linguistic Applications,1984.   \nKenton,J. D.M.-W. C.and Toutanova,L.K. Bert: Pretraining of deep bidirectional transformers for language understanding. In NACCL, 2019.   \nLewis,M.,Liu,Y.,Goyal, N., Ghazvininejad,M.,Mohamed,A.,Levy,O.,Stoyanov,V.,and Zettlemoyer,L. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In ACL,2020.   \nLewkowycz, A., Andreassen, A. J.,Dohan, D., Dyer, E., Michalewski, H.,Ramasesh,V.V.,Slone,A.,Anil,C., Schlag,I.,Gutman-Solo,T.,et al. Solving quantitative reasoning problems with language models.In NeurIPS, 2022.   \nLiu, Z.,Lin, Y., Cao, Y.,Hu,H., Wei, Y.,Zhang, Z.,Lin, S.,and Guo,B. Swin transformer: Hierarchical vision transformer using shifted windows.In ICCV,2021.   \nMin,S.,Lyu,X.,Holtzman,A.,Artetxe,M.,Lewis,M., Hajishirzi,H.,and Zettlemoyer,L.Rethinking the role of demonstrations:What makesin-context learning work? arXiv preprint arXiv:2202.12837,2022.   \nOtt,M.,Edunov,S.,Baevski,A.,Fan,A.,Gross,.,Ng, N., Grangier,D.,and Auli,M.fairseq:A fast,extensible toolkit for sequence modeling. In ACL, 2019.   \nOuyang,L., Wu, J.,Jiang, X.,Almeida,D., Wainwright, C.L.，Mishkin,P.， Zhang，C.，Agarwal， S.， Slama, K.,Ray,A.,et al. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155,2022.   \nRadford, A.,Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,Askell, A.,Mishkin,P., Clark,J., et al.Learning transferable visual models from natural language supervision. In ICML,2021.   \nRamesh,A.,Pavlov,M.,Goh, G., Gray,S.,Voss,C.,Radford,A., Chen,M.,and Sutskever,I. Zero-shot text-toimage generation. In ICML,2021.   \nRasley,J.,Rajbhandari,S.,Ruwase,O.and He,Y.Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In SIGKDD, 2020. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    }
]