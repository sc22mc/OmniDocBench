[
    {
        "type": "table",
        "img_path": "images/6df27cc8452a5b32f178aa5197dbcd431fd5c5817f265a547a10fe6bdecfa00a.jpg",
        "table_caption": [
            "Table 4: Memory slots VS Original contexts ( ${ \\sim } 5 1 2$ tokens)on the PwC test set "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">System1 (k memory slots)</td><td rowspan=\"2\">System2 (original context)</td><td colspan=\"4\">Judgement (%)</td></tr><tr><td>win</td><td>lose</td><td>tie</td><td>on par(win+tie)</td></tr><tr><td rowspan=\"2\">Llama-7b (ICAE, k=128)</td><td>Alpaca</td><td>56.7</td><td>26.9</td><td>16.4</td><td>73.1</td></tr><tr><td>StableLM-7b GPT-4 (gold)</td><td>74.1</td><td>18.8</td><td>7.2</td><td>81.3</td></tr><tr><td rowspan=\"2\">Llama-2-7b-chat (ICAE, k=64)</td><td></td><td>3.4</td><td>69.4</td><td>27.2</td><td>30.6</td></tr><tr><td>LlaPa-2-7b-cbat</td><td>13.6</td><td>516</td><td>38</td><td>48</td></tr><tr><td rowspan=\"2\">Llama-2-7b-chat (ICAE, k=128)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LlaPa-7b-chat</td><td>19.6</td><td>45.</td><td>354</td><td>546</td></tr><tr><td rowspan=\"2\">Llama-2-7b-chat (ICAE, k=256)</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LlaPa-7b-chat</td><td>22.0</td><td>222</td><td>558</td><td>778</td></tr><tr><td rowspan=\"2\">Llama-2-13b-chat (ICAE,k=256)</td><td>Llama-2-13b-chat</td><td>21.9</td><td>20.8</td><td>57.3</td><td></td></tr><tr><td>GPT-4 (gold)</td><td>4.0</td><td>19.2</td><td>76.8</td><td>79.2 80.8</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/5c73ced34b4542d1e9d711045e44f4a2a056841f4e848e4bd17380aeba163250.jpg",
        "table_caption": [
            "Table 5: ICAE with diferent memory slot lengths and diferent pretraining setups.The last row is the comparison between 128-length ICAE's memory and 128-token summary produced by the GPT-4. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">ICAE (Llama-2-7b-chat)</td><td colspan=\"4\"> lose (gee Â±t)</td></tr><tr><td>win (%)</td><td></td><td></td><td>win/lose</td></tr><tr><td>k=128 (pretrained) VS k= 64 (pretrained)</td><td>57.6</td><td>19.5</td><td>22.9</td><td>3.0</td></tr><tr><td>k=64(pretrained)VSk=32(pretrained)</td><td>44.7</td><td>21.8</td><td>33.5</td><td>2.1</td></tr><tr><td>k = 64(pretrained) VSk =128 (no pretraining)</td><td>33.1</td><td>28.0</td><td>38.9</td><td>1.2</td></tr><tr><td>k =128 (pretrained) VS k = 128 (no pretraining)</td><td>60.4</td><td>9.5</td><td>30.1</td><td>6.4</td></tr><tr><td>k =128 (pretrained) VS k = 128 (pretrained only with AE)</td><td>36.4</td><td>28.5</td><td>35.1</td><td>1.3</td></tr><tr><td>k =128 (pretrained)VS k =128 (pretrained only with LM)</td><td>35.1</td><td>24.9</td><td>40.0</td><td>1.4</td></tr><tr><td>k =128 (pretrained) VS 128-token summary (by GPT-4)</td><td>34.1</td><td>17.6</td><td>48.3</td><td>1.9</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "non-pretrained counterpart, emphasizing the importance of pretraining.By comparing the outputs generated via the pretrained and non-pretrained ICAE,we find the pretrained ICAE suffers less from hallucination than the non-pretrained counterpart (see the examples in Table 9 in Appendix D).We assume the pretraining of ICAE improves the LLM's working memory as it shares some analogies with humans enhancing their memory capacity via extensive memory training which improves the brain's memory encoding capabilities. We also examine pretraining objectives and find combining3 AE andLMyields better results than using AEorLM individually (the 4th row in Table 5). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The last row of Table5compares ICAE's 128-length memory slots with a summary4 within 128 tokens( $\\mathord { \\sim } 1 0 0$ words). Memory slots significantly outperform summaries under the same context length,with ${ \\sim } 2 \\times$ win/lose ratio, proving to be more compact and informative than natural language. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3ANALYSIS ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3.1SCALABILITY ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As discussed above, ICAE should achieve better compression performance with a more powerful targetLLM.To verify this assumption, we compare the ICAE's performance on three target LLMs: Llama-7b,Llama-2-7b and Llama-2-13b in Table 6, which align well with our expectations-more powerful target LLMs can achieve better context compression ratios. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.3.2LATENCY ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We conducted an empirical test to evaluate the impact of ICAE's $4 \\times$ context compression on inference effciency.For thisefficiency test,wefix thecontext (i.e.,input)length toeither512or2048and the generation length to 128. Table 7 shows that context compression by ICAE is helpful to improve LLM (i.e.,Llama-7b) inference efficiency,achievingover $2 \\times$ speedup. Its acceleration becomes ",
        "page_idx": 0
    }
]