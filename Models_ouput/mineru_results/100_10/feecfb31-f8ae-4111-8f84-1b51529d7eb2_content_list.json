[
    {
        "type": "text",
        "text": "with an experience-driven iteration processIteratively exploringand self-evaluating the generated simplistic trees of thoughts enables a simple initial promptto be gradually enhanced by an ensemble of trial-and-error reasoning experiences,resulting in accurate solutions. Our extensive experiments demonstrated that BoT is capable of achieving state-of-the-art on multiple benchmark datasets while outperforming the alternative leading approach in Game of 24, which is a challenging mathematical reasoningtask. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "REFERENCES ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Maciej Besta,NilsBlach，Ales Kubicek，Robert Gerstenberger，Lukas Gianinazzi,Joanna Gajda,Tomasz Lehmann,Michal Podstawski， Hubert Niewiadomski, Piotr Nyczyk,et al. Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint arXiv:2308.09687,2023.   \nTomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JredDKaplan,Prafulla hariwal, Arvind Neelakantan,Pranav Shyam, Girish SastryAmanda Askel, et al. Language models are few-shot learners. Advances in neural information processing systems,33:1877-1901,2020.   \nTianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proc.the 22nd acm sigkdd international conference on knowledge discovery and data mining, pp.785-794,2016.   \nKarl Cobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser, MatthiasPlappert,Jerry Tworek,JacobHilton,ReiichiroNakano,etal.Trainingverifiers tosolve math word problems. arXiv preprint arXiv:2110.14168,2021.   \nShizhe Diao,Pengcheng Wang, Yong Lin,and Tong Zhang.Active prompting with chain-of-thought forlarge language models. arXiv preprint arXiv:2302.12246,2023.   \nYoav Freund,Robert E Schapire,et al.Experiments with a new boosting algorithm.In icml, volume96,pp.148-156.Citeseer,1996.   \nYaoFu,Hao Peng,AshishSabharwal,Peter Clark,and TusharKhot.Complexity-based promptingfor multi-step reasoning. In Proc.The Eleventh International Conference on Learning Representations, 2022.   \nDan Hendrycks, CollinBurns, Steven Basart,Andy Zou,Mantas Mazeika,Dawn Song,and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a.   \nDan Hendrycks,CollinBurns,SauravKadavath, Akul Arora,Steven Basart,Eric Tang,Dawn Song, andJacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874,2021b.   \nBairu Hou,Joe O'connor,Jacob Andreas,Shiyu Chang,and Yang Zhang. Promptboosting: Black-box textclassification withtenforwardpasses.InInterational ConferenceonMachineLearningpp. 13309-13324.PMLR,2023.   \nGuolin Ke,Qi Meng,Thomas Finley,Taifeng Wang,Wei Chen,Weidong Ma,Qiwei Ye,and Tie-Yan Liu.Lightgbm: Ahighly efficient gradient boosting decision tree.Advances in neural information processing systems,30,2017.   \nTakeshi Kojima, Shixiang Shane Gu,Machel Reid, Yutaka Matsuo,and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35: 22199-22213,2022.   \nAitor Lewkowycz,AndersAndreassen,David Dohan,EthanDyer,Henryk ichalewski, Vinay Ramasesh,Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.   \nWang Ling,Dani Yogatama, Chris Dyer, and Phil Blunsom.Program induction by rationale generation:Learning_to solve and explain algebraic word problems.arXiv preprint arXiv:1705.04146,2017. ",
        "page_idx": 0
    }
]