[
    {
        "type": "text",
        "text": "4.2. Comparative Results ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Location Reasoning. We compare the results of our QRCLIP with other methods for location reasoning in Tab.1. QR-CLIP, achieves accuracy of $1 9 . 3 1 \\%$ Accuracy $( \\mathbb { R } ^ { @ 1 ) }$ Meanwhile,itsExample-F1 score for thehierarchical labels is $5 0 . 9 6 \\%$ .All the results clearly show that our method outperforms other methods. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "(1) Compared with ResNet-50 (He et al.,2016) and SwinT (Liu et al., 2021), vanilla CLIP achieves $7 . 9 3 \\%$ and $4 . 4 1 \\%$ absolute improvement in location prediction accuracy (ID: 1,2,3). It indicates that compared with the vision model only trained on ImageNet (Deng et al., 2009), CLIP already possesses some knowledge for reasoning.Meanwhile, our QR-CLIP achieves a more significant advantage with $1 6 . 1 3 \\%$ and $1 2 . 6 1 \\%$ absolute improvements in terms of accuracy (ID: 1,2,6). These results show that traditional image classification methods cannot accomplish inference of the abstract information behind the images. While the CLIP model trained on large-scale internet data have the ability to identify locations based on image data,and QR-CLIP significantly enhances this capability. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "(2) Besides,compared to $\\mathrm { C L I P \\dagger }$ and the state-of-the-art method ${ \\mathrm { C L I P } } { + } { \\mathrm { S e g } }$ ，QR-CLIP improves the accuracy by $3 . 5 9 \\%$ and $2 . 8 5 \\%$ absolute improvement, the F1-Score has increased by $3 . 8 8 \\%$ and $3 . 0 7 \\%$ ,respectively (ID:4,5,6). Other evaluation metrics also improved. The results show that QR-CLIP can effectively utilize open-world knowledge to establish a closer connection between image and location information through fine-tuning CLIP.However,we also find that the improvement in Example-F1 is not as obvious. We argue that this is because the mechanism of Example-F1: take the image of Fig.1 as an example-the picture show many Arabia elements (turbanand Arabic). Itisnot difficult for many models to recognize that this image was captured in the Middle East and to predict its hierarchical label as $\\{ \\langle A s i a ^ { , } \\}$ .However,they failedwhenasked to predict the entire label $\\{$ ‘Riyadh,Saudi Arabia,Asia\"}.Therefore,the discrepancy in other metrics may be more noticeable. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Time Reasoning. Tab.1 also presents the performance of our method and existing techniques for time reasoning. The Accuracy $( \\mathbb { R } ^ { \\ @ 1 ) }$ of QR-CLIPis $3 . 5 3 \\%$ ,andExample-F1is $4 7 . 8 9 \\%$ ; compared to the CLIP model, the two metrics have been absolutely improved by $3 . 0 7 \\%$ and $7 . 9 9 \\%$ ,respectively (ID: 9,12). Compared with $\\mathrm { C L I P \\dagger }$ and ${ \\mathrm { C L I P } } { + } { \\mathrm { S e g } }$ ,which are also based on CLIP fine-tuning,our method obtains $2 . 5 3 \\%$ and $2 . 6 1 \\%$ improvement in the accuracy of prediction time, respectively. Compared with traditional image classification methods, QR-CLIP has achieved absolute advantages in all metrics (ID:7,8,12). In addition, due to the lack of timerelated information in the image,the prediction accuracy of fine-tuning CLIP methods for image time can only reach about $1 \\%$ ,which is significantly lower than the accuracy of ",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/c3a89f556ed4142a9f6a8c89fb9691365a5336428fe28a1466b37f908d4df849.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"4\">ID Method Accuracy (~ Rank@1) Rank@5|Example-F1</td></tr><tr><td colspan=\"4\">Location Reasoning</td></tr><tr><td colspan=\"4\">13 CLIP+[CLS*]}(n=2)</td></tr><tr><td></td><td>9.69%</td><td>27.17% 26.25%</td><td>44.37% 43.23%</td></tr><tr><td>14 CLIP+[CLS*]}(n=4)</td><td>9.53%</td><td></td><td></td></tr><tr><td>15 CLIP+[CLS*](n=6)</td><td>9.21%</td><td>27.05%</td><td>43.69%</td></tr><tr><td colspan=\"4\"></td></tr><tr><td></td><td>16 CLIP+[CLs]}(n=2)</td><td>16.84%</td><td>37.47%</td><td>49.22%</td></tr><tr><td>17 CLIP+[CLs]}(n=4)</td><td></td><td>17.11%</td><td>37.60%</td><td>49.51%</td></tr><tr><td></td><td>18 CLIP+[CLs]}(n=6)</td><td>17.25%</td><td>37.80%</td><td>49.98%</td></tr><tr><td>19</td><td>CLIP+[CLs]}(n=8)</td><td>17.03%</td><td>37.62%</td><td>48.93%</td></tr><tr><td colspan=\"4\">TimeReasoning</td><td></td></tr><tr><td></td><td>20 CLIP+[CLS*](n=2)</td><td>0.98%</td><td>3.03%</td><td>42.18%</td></tr><tr><td>21</td><td>CLIP+[CLS*](n=4)</td><td>1.03%</td><td>2.99%</td><td>43.98%</td></tr><tr><td></td><td>22 CLIP+[CLs*1}(n=6)</td><td>1.08%</td><td>3.15%</td><td>43.62%</td></tr><tr><td></td><td>23CLIP+[CLs]}(n=2)</td><td>1.84%</td><td>5.14%</td><td>45.57%</td></tr><tr><td>24</td><td>CLIP+[CLs]}(n=4)</td><td>1.92%</td><td>5.21%</td><td>45.63%</td></tr><tr><td></td><td>25 CLIP+[CLS](n=6)</td><td>2.00%</td><td>5.37%</td><td>45.60%</td></tr><tr><td></td><td></td><td>1.53%</td><td>5.06%</td><td>45.15%</td></tr><tr><td></td><td>26 CLIP+[CLs](n=8)</td><td></td><td></td><td></td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Table2.Performance ofadditional[CLS]inQR-CLIPwith different number and prediction methods.Whereas $[ \\boldsymbol { \\mathrm { C L S } } ^ { * } ] _ { i } ^ { \\nu }$ refers to fusingall additional [CLS]by MLPs and then calculating the similarity with locationand time labels, $[ \\mathrm { C L S } ] _ { i } ^ { \\nu }$ refers to calculating the similarity betweeneachadditional[CLS]with labels separately,and thenusing the ( $[ \\mathrm { C L S } ] _ { i } ^ { \\nu }$ -label) pairwith thegreatest similarityasthe prediction. $n$ represents the number of [CLS]. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "location prediction (ID: 10,11). This is not surprising,also take the image on Fig.1 as sample:even for humans, it is difficult to determine that $\\{ { } ^ { \\cdot } O 3 - O I - 2 O 2 3 { } ^ { \\circ } \\}$ is the time when this photograph was taken, if they are unfamiliar with Cristiano Ronaldo or some specific knowledge. Nevertheless, the method proposed in this paper is still effective (that our model achieves $+ 2 5 3 . 0 0 \\%$ relative lift) for predicting time and significantly closes the gap with location prediction. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.3.Ablation Study ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Analysis on Additional [CLS].Following the network design process,all experiments of this part were conducted on the setting with only the step 1 in Quantity module (Sec 3.2). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As shown in Tab.2,both of different [CLS] aggregation methodsand different numbers of[CLS] can affect network performance. Comparing $[ \\mathrm { C L S } _ { i } ^ { * } ]$ and $[ \\mathrm { C L S } ] _ { i } ^ { \\nu }$ with the same number (i.e., $n = 2$ ）of[CLS],thelatterhas $7 . 1 5 \\%$ and $0 . 8 6 \\%$ higher location and time prediction accuracy (ID:13,16,20,23). Besides, the performance of $[ \\mathrm { C L S } _ { i } ^ { * } ]$ is not significantly affected by the number of[CLS](ID: 13-15,20-22). We argue that using MLP to aggregate the embeddings may destroy CLIP's original representation. It isbetter to separately calculate the similarities across each $[ \\mathrm { C L S } ] _ { i } ^ { \\nu }$ with the location and time labels and then select the one with the most significant value as the prediction. Thenwe analyze how different numbersof[CLS］affect the model performance.When $n$ was increased to 8, no significant performance difference was observed, so we finally chose $n = 6$ in the following experiments (ID: 17-19,24-26). The results indicate that the additional [CLS] effectively ",
        "page_idx": 0
    }
]