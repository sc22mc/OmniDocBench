[
    {
        "type": "table",
        "img_path": "images/c2897db287ab88b6c2f10b78851c015424771c754220989ab68e56fc0c6a1944.jpg",
        "table_caption": [
            "Table2:Top half:Average GPT-4Rating onthe Best-of- $N$ (BoN) responses selectedby different rating models.Bottom half: Correlations between different models and GPT-4onall selected Bestof $. N$ responses by different rating models, $\\dagger$ means p-value ${ > } 0 . 0 5$ .L2Chat:LLaMA-2-Chat-13B. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>BaseLLM</td><td>BoN</td><td>Open-Assistant</td><td>SteamSHP</td><td>ChatGPT</td><td>L2Chat</td><td>Vicuna</td><td>WizardLM</td><td>AUTO-J</td></tr><tr><td rowspan=\"4\">Selection</td><td></td><td>8 16</td><td>8.17 8.28</td><td>8.02 8.01</td><td>8.20</td><td>8.13</td><td>8.09</td><td>7.93 7.89</td><td>8.21 8.33</td></tr><tr><td>LLaMA-2-Chat-7B</td><td>32</td><td>8.25</td><td>7.84</td><td>8.14 8.14</td><td>8.19 8.16</td><td>8.03 8.05</td><td>7.94</td><td>8.34</td></tr><tr><td rowspan=\"2\"></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>8</td><td>7.51</td><td>7.47</td><td>7.28</td><td>7.07</td><td>7.19</td><td>6.32</td><td>7.49</td></tr><tr><td rowspan=\"3\"></td><td rowspan=\"3\">Vicuna-7B-v1.5</td><td>1</td><td>769</td><td>774</td><td>7.29</td><td>707</td><td>7.53</td><td>646</td><td>774</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>0.36</td><td>0.13</td><td>0.06</td><td>0.16</td><td>-0.05</td><td>0.41</td><td>0.57</td></tr><tr><td rowspan=\"2\">Correlation</td><td rowspan=\"2\" colspan=\"2\">Spearman</td><td>0.42</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>一</td><td>0.13</td><td>0.06</td><td>0.24</td><td>-0.01</td><td>0.35</td><td>0.55</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Based on the 1,993 query-response pairs with GPT-4 rating in the above best-of- $N$ experiment, we calculate the response-level Spearman and Pearson correlations between model's rating and GPT-4 ratings.Results in Tab. $\\bigstar$ show a better correlation between AUTO-Jand GPT-4 than all baselines. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "6.4ANALYSIS AND CASE STUDIES ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "System-level Ranking Besides response-level evaluation,and we also investigate the potential of AUTO-J on the system level,which is useful when we benchmark existing LLMs with leaderboard.We use the AlpacaEval leaderboard as it has archived complete outputs for each submitted model. We use AUTO-Jin single-response evaluation protocol and calculate average ratings on the dataset for all opensource LLMs on the leaderboard. 3 The Spearman and Pearson correlations with GPT-4's ranking on the leaderboard are O.97 and 0.96 respectively (Fig.[5),and we show detailed ranking in Tab.[24] This extremely strong correlation indicates that AUTO-J can also serve asa good system-level judge for ranking open-source LLMs. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Ablation Studies(1） We train a model that outputs only the final decision using the same pairwise training data for AUTo-J. Its agreement rate with human on Eval-P is 55.0 (AUTo-J gets 54.8,in Tab..We conclude that our model does not sacrifice the pairwise comparison performance for supporting multiple evaluation protocols and generating supporting explanations. (2) Using the same pairwise training data, we train a standard reward model to output ascalar rating for each query-response pair (its agreement rate on Eval-P is 54.5).We conduct best-of-32 response selection experiments. As shown in Tab. $\\textcircled { 3 }$ despite not being directly optimized for a scalar output, AUTO-J achieves comparable performance to reward model. It also demonstrates highercorrelation with GPT-4 ratings than the reward model trained solely for that purpose. ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/fedac7443cbf846ec30219d6ac70c5b1d424c386ae37cc03688d035b160bc493.jpg",
        "image_caption": [
            "Figure 5: System-level correlation on AlpacaEval. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Case StudiesWe show a pairwise comparison case from the test set (Eval-P) in Tab.4(complete version in Tab.25and $\\boxed { 2 6 }$ .This example shows only AUTO-J (and GPT-4) emphasize the advantages of the second response in terms of tone and interactivity for a family email,and make the correct choice. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We show a single-response evaluation case from the test set (Eval-C) in Tab. $\\boxed { 5 }$ (complete version in Tab.27) shows that the critique given by AUTO-J is more aware of the user's status as a novice in cooking, and pinpoint more essential concerns on this. ",
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/6fd10630801868d03cf35aef4ae7a00d7699fefd41fbc5aeedefef997d8b3128.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Base LLM</td><td>AUTO-J</td><td>ScalarRM</td></tr><tr><td>L2Chat7B Vicuna7B</td><td>8.34</td><td>8.42 7.94</td></tr><tr><td>Correlation with GPT-4</td><td>7.97</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>Pearson</td><td>0.57</td><td>0.39</td></tr><tr><td>Spearman</td><td>0.55</td><td>0.40</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The Best-of- $N$ selection case from the test set (Eval-R) in Tab.28   \nshows the usefulness of its rating in single-response evaluation.With   \nmore candidate responses given by the base LLM(Vicuna-7B-v1.5), .UTO-J is able to select a better response measured by both GPT-4 rating and human observation. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Table 3:Best-of-32 response selection for AUTO-J and a standard RM,and their correlation with GPT-4 ratings. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    }
]