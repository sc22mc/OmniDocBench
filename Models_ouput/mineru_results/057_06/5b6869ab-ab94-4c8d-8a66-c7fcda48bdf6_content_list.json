[
    {
        "type": "table",
        "img_path": "images/11aa57ff39df8023c0021caf47142fbbf89b401a14e4e1ac0fef152685239075.jpg",
        "table_caption": [
            "TABLE I: Description template,where“obj”and“nbr”denote the GUI element and its neighbor, $\\alpha$ and $\\beta$ denote high- and low-confidence element. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Action</td><td>Id</td><td>Condition</td><td>Template</td><td>Example</td></tr><tr><td rowspan=\"3\">TAP</td><td>1</td><td>(objrextcaption≠NULL)  (objconfid α)</td><td>Tap [objiex] [objclass]</td><td>Tap“OK”button</td></tr><tr><td>2</td><td>(objrexcaption / NUL)）(β&lt;objconfida)</td><td>Tap[objiex][objclassat[objposition]</td><td>Tap&quot;menu&quot; icn at top left</td></tr><tr><td>3</td><td>(objiexticaptin ==NULL) V (objconfd &lt;β)</td><td>Tapthe[obicass][nbrelationl[nbren]</td><td>Tap the checkbox nexto -Dak</td></tr><tr><td rowspan=\"2\">SCROLL</td><td>4</td><td>objtextNULL</td><td>Scroldiecofefeeee</td><td>Soroldawn half he sare</td></tr><tr><td>5</td><td>objtext == NULL</td><td>Scroll [direction] [offset]of the screen</td><td>Scroll up a quarter of the screen</td></tr><tr><td rowspan=\"2\">INPUT</td><td>67</td><td>(</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/c5031269e5ab3f9cd67dbe415aed2beed05ee3457778d371f877018585f82958.jpg",
        "image_caption": [
            "Fig.7: Example of GUI understanding. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "and the“None”element at the bottom.Note that the“Audio cue settings”element is omitted due to large spacing,which is consistent with human viewing. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "2）Subtitle Creation: The main instruction of interest is to create a clear and concise subtitle description based on $\\{ a c t i o n , o b j e c t \\}$ .The global GUI information is further used to complement the description by $\\left\\{ p o s i t i o n , r e l a t i o n s h i p \\right\\}$ Based on the action obtained in Section $\\| \\mathbf { I - B } \\| _ { \\ell }$ the attribute of object inferred in Section $\\lVert \\mathbf { I } \\mathbf { I } - \\mathbf { C } \\rVert$ and the corresponding GUI element information retrieved in Section $\\left| \\mathrm { I - D 1 } \\right|$ we propose description templates for TAP,SCROLL,INPUT,respectively. A summary of description templates can be seen in Table[ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "For $T A P$ action,the goal of the description should be clear and concise, e.g., tap“OK” button.However, we find that this simple description maynot articulate all $T A P$ actions due to two reasons.First, the text and caption of object are prone to errors or undetected,as the OCR-obtained text and the captionobtained annotation are not $100 \\%$ accurate. Second, there may be multiple objects with the same text on the GUI. To resolve this, we set up an object confidence value objconfid as: ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/728951418a502e59e3cd3247b8628d5de046442723c195e82d9dace5f3aa9467.jpg",
        "text": "$$\no b j _ { c o n f i d } = \\left\\{ \\begin{array} { l l } { { O C R _ { c o n f i d } } } & { { \\mathrm { i f ~ } o b j _ { t e x t } { \\mathrm { ~ i s ~ u n i q u e ~ i n ~ G U I } } } } \\\\ { { 0 } } & { { \\mathrm { o t h e r w i s e } } } \\end{array} \\right.\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "where $O C R _ { c o n f i d }$ denotes the confidence predicted by OCR. Note that the confidence value of icon object is calculated likewise by captioning.The smaller the confidence value, the less intuitive the object is.Therefore,only the object with the highest confidence value $( o b j _ { c o n f i d } > \\alpha )$ will apply the simplest and most straightforward description (Template 1),otherwise, we add the context of absolute position to help locate the object (Template 2).For the object whose text is not detected orrecognized with low confidence,we leverage the context of its neighbor to help locate the target object (Template 3), e.g., tap the checkbox next to “Dark Mode”. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "It is easy to describe a SCROLL action by its scrolling direction and offset (Template 5),e.g., scroll up a quarter of the screen.However, such an offset description is not precise and intuitive.To address this,if a new element with text appears by scrolling，we add this context to help describe where to scroll to (Template 4),e.g.,scroll down half of the screen to \"Advanced Setting”. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The description of INPUT issimilar to $T A P$ For the high-confidence object with text (Template 6),it generates: Input [text] in the $[ o b j _ { t e x t } ]$ edittext.Different from the $T A P$ descriptions,we do not apply the context of absolute position to help locate the low-confidence object.This is because the objects are gathering at the top when the keyboard pops up, so the absolute positioning may not help.Instead,we use the relative position of neighbor to describe the input object of which text is not detected or recognized with low confidence (Template7),e.g.,Input“John”in the edittextbelow“Name”. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "After generating the natural language description for each action clip,we embed the description into the recording as subtitles as shown inFig. In detail, we create the subtitles by using the Wand image annotation library [42l and synchronize the subtitle display at the beginning of each action clip. ",
        "page_idx": 0
    }
]