[
    {
        "type": "image",
        "img_path": "images/fcf888513a2d6f3fb67c513d457716adc909b151bbbd7bf281437a85bcdef472.jpg",
        "image_caption": [
            "Figure 2.Network architecture of RecNet "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "and $a ^ { ( 3 ) }$ ,respectively. In line 9, we compute an event stack from the consecutive coded-aperture images as ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/0b4f91f73fbe798bf1f7d421dfbbaeb2a0d16610fbb07337bc77928555455233.jpg",
        "text": "$$\nE _ { x , y } ^ { ( n , n + 1 ) } = Q \\Bigg ( \\frac { \\log ( I _ { x , y } ^ { ( n + 1 ) } + \\epsilon ) - \\log ( I _ { x , y } ^ { ( n ) } + \\epsilon ) } { \\tau + n _ { x , y } } \\Bigg )\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "where $Q ( x ) : = \\mathrm { s i g n } ( x ) \\mathrm { f l o o r } ( | x | )$ is a quantization operator, and $\\epsilon = 0 . 0 1$ W.r.t. the intensity range of $[ 0 , 1 ]$ for $I ^ { ( n ) }$ and I(n+1). To account for the randomness of the sensor, a zeromeans Gaussian noise $n$ with $\\sigma = 0 . 0 2 1$ is added to the contrast threshold $\\tau$ . Equation (12) follows a widely used event simulator [36],but we implement it as being differentiable.Following previous studies [14,28,42],we also add a zero-means Gaussian noise ( $\\sigma = 0 . 0 0 5$ W.r.t. the normalized intensity range $[ 0 , 1 ] )$ to the image frame $\\bar { I }$ to account for the measurement noise. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "A challenging issue with Eq.(12) is how to determine the contrast threshold $\\tau$ .When the imaging process is computationally simulated, a smaller $\\tau$ isbetter. This is because a smaller $\\tau$ leads to a more fine-grained observation (a larger number of events),which in turn resultsin more accurate light-field reconstruction. However, it is difficult to determine a specific $\\tau$ thatis compatible with a real event camera (e.g.,a DAVIS 346 camera) the $\\tau$ of which cannot be controlled in an explicit (direct) manner. With our experimental configuration,we empirically estimate $\\tau \\simeq 0 . 1 5$ ,but it depends on the configuration. To address this issue,we treat $\\tau$ asbeing variable,which makes themodel of Eq.(12) more flexible. Specifically, we randomly draw $\\tau$ from [0.075,0.3] for each batch during training,which enables the algorithm to be independent of a specific $\\tau$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "RecNet. Since network architectures are not the main focus of this paper, we used a plain network architecture consisting of a sequence of 2-D convolutional layers. The dataobtained from the camera $( \\bar { I } , E ^ { ( 1 , 2 ) } , E ^ { ( 2 , 3 ) }$ ,and $E ^ { ( 3 , 4 ) }$ stacked along the channel dimension） are fed to RecNet; the output from RecNet is a tensor with 64 channels corresponding to 64 views of the reconstructed light field (see Fig.2 for more details). Similar architectures were used in previous works [14,42],achieving a good balance between the reconstruction quality and computational cost. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Our plain architecture can be used for other imaging methods with minimal modifications (only by changing the number of channels for the input layer),which makes the comparison easier. Exploration for better network architectures isleftas future work. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Training and loss function. AcqNet and RecNet are implemented using PyTorch 2.O and jointly trained on the BasicLFSR dataset [23]. We extract 29,327 training samples, each with $6 4 \\times 6 4$ pixelsand $8 \\times 8$ views,from144light fields designated for training. The disparities among the neighboring viewpoints are mostly limited within $[ - 3 , 3 ]$ pixels.This issuitable for our method because the $8 \\times 8$ viewpoints are arranged on the small aperture plane of the camera,which results in limited disparities among the viewpoints.We use the built-in Adam optimizer with default parameters and train the entire network over 6Oo epochs with abatch size of 16.The scale parameter $s$ is initialized as 1 and multiplied by1.O2 for each epoch. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Asmentioned earlier,we use the reconstruction loss between the original and reconstructed light fields. However,this is insuficient in some cases. The learned patb many events spreading over all the pixels. These coding terns sometimes have significantly different brightness (the patterns are uselesswith a real event camera because such alarge number of events cannot be recorded correctly due to the limited throughput of the camera. To avoid this problem,we optionally add the second term to the loss function to suppress the number of events in each batch $( N _ { \\mathrm { e v e n t } } )$ belowa pre-defined threshold $\\mathbf { \\eta } ^ { ( \\theta ) }$ ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/d65b6222e79345bcc94120fbe79bf9f9a6b34566a6fd153343c07dee924e0db8.jpg",
        "text": "$$\n\\mathrm { L o s s } = \\mathrm { M S E } ( L , \\hat { L } ) + \\lambda \\cdot \\operatorname* { m a x } ( N _ { \\mathrm { e v e n t } } - \\theta , 0 )\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "where $\\lambda$ is anon-negative weight. The threshold $\\theta$ can be computed from the camera's throughput (events per sec.). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.4.Hardware ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Our hardware setup is shown in Fig. 3. The optical system consists of a Nikon Rayfact lens $2 5 \\ : \\mathrm { m m } \\ : \\mathrm { F } 1 . 4 \\ : \\mathrm { S F } 2 5 1 4 \\mathrm { M C } )$ set of relay optics,beam splitter,andLCoS display (Forth Dimension Displays,SXGA-3DM, $1 2 8 0 \\times 1 0 2 4$ pixels). We use an iniVation DAVIS 346 monochrome camera that can acquire both events and image frames with $3 4 6 \\times 2 6 0$ pixels. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We use the central portion of the LCoS display ( $1 0 2 4 \\times$ 1024 pixels) as the effective aperture area,and divide it into $8 \\times 8$ regions (each with $1 2 8 \\times 1 2 8$ pixels） to display the coding patterns (each with $8 \\times 8$ elements). Four coding patterns, $a ^ { ( 1 ) }$ ， $a ^ { ( 2 ) }$ ， $a ^ { ( 3 ) }$ ,and $a ^ { ( 4 ) }$ ,are repeatedly displayed, each with a5.O msec duration.According to the hardware's document, the total time for each coding pattern $( T _ { \\mathrm { c } } )$ takes 5.434 msec including the overhead time. The exposure time for an image frame $\\bar { I }$ is set to $4 T _ { \\mathrm { c } }$ $( \\simeq 2 2 { \\mathrm { ~ m s e c } } ^ { 3 }$ ）tocover a ",
        "page_idx": 0
    }
]