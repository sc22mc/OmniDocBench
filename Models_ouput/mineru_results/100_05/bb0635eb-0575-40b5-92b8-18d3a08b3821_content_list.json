[
    {
        "type": "text",
        "text": "Simple Prompt.For any task，in iteration $ { \\boldsymbol { { t } } } \\ = \\ 0$ ，we create a simple initial prompt $\\mathbb { I } ^ { 0 } \\left( \\bar { S } , X , Q , \\mathbf { F } ^ { \\bar { 0 } } , \\{ G _ { i } \\} \\right)$ ，where $S$ represents task-agnostic descriptions while the terms $X$ and $Q$ respectively denote the task information and the question. The experience part of the prompt is denoted as $\\mathbf { \\hat { F } } ^ { 0 }$ ,which should be empty at the beginning. $\\left\\{ { G } _ { i } \\right\\}$ is a placeholder that is waiting to be filled during building thought structures. In other words, when generating the next thought $z _ { i }$ ， $\\left\\{ { G } _ { i } \\right\\}$ will be substituted with the preceding chain of thoughts $z _ { 1 \\ldots , i - 1 }$ ： ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Thought Structures Generation. After collecting experience $\\mathbf { F } ^ { t - 1 }$ ,the prompt in the iteration $t$ can be $\\mathbb { I } ^ { t } \\left( S , X , Q , \\mathbf { F } ^ { 1 , \\ldots , t - 1 } , \\{ G _ { i } \\} \\right)$ Based on this prompt, BoT generates $M$ thought structures in parallel. BoT is inherently capable of embracing any thought structure,such as the chain $| \\mathrm { W e i } |$ $\\boxed { \\mathrm { e t ~ a l . } } ( \\boxed { 2 0 2 2 } )$ or tree $\\boxed { \\mathrm { Y a o ~ e t ~ a l . } } \\boxed { 2 0 2 4 }$ structure. Considering the exploration of reasoning steps and experimental results, we investigate the tree thought structure. However, BoT introduces two novel modifications to make it better suited for the boosting framework. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "·Weighted Binary Tree.With a simple prompt in each round,BoT builds the weak thoughts structured in low complexity as they can be further revised in the boosting mechanism. Thus,each thought structure of BoT is a shallow weighted binary tree. For simplicity, we retain the notation $z _ { 1 \\dots i - 1 }$ to represent the thoughts from the root to the parent of node $i$ In addition to providing each node $i$ with one thought $z _ { i }$ and its thought evaluation score $V _ { i } \\sim$ $p _ { \\theta } \\left( z _ { 1 \\ldots i } , \\mathbb { I } _ { a } , X , Q \\right)$ ,we incorporate the edge score $V _ { i - 1 , i } \\sim p _ { \\theta } \\left( z _ { i - 1 } , z _ { i } , \\mathbb { I } _ { e } , X , Q \\right)$ between a child node and its parent node,where $\\mathbb { I } _ { a }$ and $\\mathbb { I } _ { e }$ refer to the instructional descriptions for thought and edge evaluations. $V _ { i - 1 , i }$ represents the LLMs’confidence level in generating this reasoning step.Thus,the next thought generation of BoT in this tree structure is formalized as $p _ { \\theta } \\left( \\bar { z } _ { i } | \\left( V _ { i - 1 , i } , V _ { i } , \\mathbb { I } ^ { t } , X , Q \\right) \\right)$ ： ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "·Tree Heterogeneity. Unlike To $\\Gamma [ \\overline { { \\mathrm { Y a o ~ e t ~ a l . } } } ] ( \\mathbb { Z } 0 2 4 )$ ,which seeks to search for a solution in one large and complex tree,BoT aims to build highly heterogeneous tree thought structures. As a result, complete reasoning chains with various logical in trees of BoT are subsequently assessed as experience. Therefore,to increase heterogeneity, thought structure generation embraces different tree growth strategies, such as level-wise growth and leaf-wise growth. The former emphasizes exploration but less exploitationChen & Guestrin(2016), while the latter does the opposite $\\boxed { \\mathrm { K e ~ e t ~ a l . } } \\textcircled { 2 0 1 7 }$ . Thus, the leaf-wise strategy tends to continue reasoning from the current best thought to reach a better final thought as compared to level-wise growth,but it also tends to get monotonous reasoning chains.Besides,different temperature and Top-p settings of LLMs are applied. Finally, we use a small max_depth value in BoTand label a node as a leaf when its $V _ { i - 1 , i }$ and $V _ { i }$ values are outside the specified range [0.3,0.8]. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Thought Structures Aggregation. Upon obtaining $M$ thought structures,BoT aggregates them into one thought chain denoted as $\\overline { { z } } _ { 1 \\dots n }$ .To achieve this,for each thought structure with index $m$ ,BoTfirst selects the chain with the highest evaluation score asz1nm $z _ { 1 _ { \\pm } . . n _ { m } } ^ { m } : = \\arg \\operatorname* { m a x } _ { z _ { 1 _ { \\pm } . . . n } \\in { \\cal Z } ^ { m } } \\sum _ { i = 1 } ^ { n } V _ { i } + V _ { i - 1 , i }$ where ${ \\boldsymbol { Z } } ^ { m }$ denotes the set of all thought chains of $m$ -th tree.Subsequently,two strategies exist to obtain $\\overline { { z } } _ { 1 \\ldots n }$ ： ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "· Best-First Aggregation. BoTrelies on arg $\\begin{array} { r } { \\operatorname* { m a x } _ { z _ { 1 } . . . n \\in \\{ Z ^ { m } \\} _ { m = 1 } ^ { M } } \\sum _ { i = 1 } ^ { n } V _ { i } + V _ { i - 1 , i } } \\end{array}$ to choose the best one as $\\overline { { z } } _ { 1 \\ldots n }$ from $M$ thought structures.This algorithm is fast but may lead to an unreasonable chain that is hard to guide the following refinement.   \n· Greedy Aggregation.BoT is allowed to perform a greedysearch on $\\{ Z ^ { m } \\} _ { m = 1 } ^ { M }$ to assemble   \na new thought chain that may not exist and is globally optimal. Starting from the initial thought,generallyte rotnode ofthetree, B oTobtains ${ \\overline { { z } } } _ { 1 } = { \\mathrm { a r g } } { \\mathrm { m a x } } _ { z _ { j } \\in \\left\\{ z _ { 1 } ^ { m } \\right\\} _ { m = 1 } ^ { M } } V _ { j } +$ $V _ { j - 1 , j }$ . Subsequently, to obtain $\\overline { { z } } _ { i }$ for $\\overline { { z } } _ { i - 1 }$ ,BoT searches all thoughts where the previous   \nstepisZi-1in{zmm ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Thought Chain Analysis.To gain insights into what should be adjusted to enhance the prompt to generate better thoughts, BoT utilizes the self-evaluation ability $\\boxed { \\mathrm { W e n g ~ e t ~ a l . } } \\textcircled { 2 0 2 3 }$ of LLMs to assess $\\overline { { z } } _ { 1 \\dots n }$ . Specifically,with the prompt $\\mathbb { I } _ { f } ^ { t } \\left( \\overline { { z } } _ { 1 \\ldots n } , X , Q \\right)$ as input, LLM outputs a feedback paragraph containing issues report of this thought chain $\\overline { { z } } _ { 1 \\dots n }$ and detailed advice.This feedback will beadded t0 $\\mathbf { F } ^ { 1 , \\ldots , t - 1 }$ asanewexperiencein thought generation,resulting $\\mathbf { F } ^ { 1 , \\hdots , t }$ ",
        "page_idx": 0
    }
]