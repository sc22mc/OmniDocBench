[
    {
        "type": "text",
        "text": "OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Linke Ouyang1\\*Yuan $\\mathrm { Q u ^ { 1 * } }$ Hongbin Zhoul\\*Jiawei Zhul\\*Rui Zhangl\\*Qunshu Lin2\\* Bin Wangl\\*tZhiyuan Zhaol Man Jiang1 Xiaomeng Zhaol Jin $\\mathrm { S h i ^ { 1 } }$ Fan ${ \\mathbf { W } } { \\mathbf { u } } ^ { 1 }$ Pei Chu1 Minghao Liu3 Zhenxiang $\\mathrm { L i ^ { 1 } }$ Chao ${ \\mathrm { X u } } ^ { 1 }$ Bo Zhang1 Botian $\\mathrm { S h i ^ { 1 } }$ Zhongying $\\mathrm { T u ^ { 1 } }$ Conghui $\\mathrm { H e ^ { 1 \\ddag } }$ 1Shanghai AI Laboratory²Abaka AI 32077AI ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Abstract ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Document content extraction is a critical task in computer vision, underpinning the data needs of large language models (LLMs) and retrieval-augmented generation (RAG) systems. Despite recent progress，current document parsing methods have not been fairly and comprehensively evaluated due to the narrow coverage of document types and the simplified, unrealistic evaluation procedures in existing benchmarks. To address these gaps, we introduce OmniDocBench,a novel benchmark featuring high-quality annotations across nine document sources, including academic papers, textbooks, and more challenging cases such as handwritten notes and densely typeset newspapers. OmniDocBench supports flexible,multi-level evaluations-ranging from an end-to-end assessment to the task-specific and attribute-based analysis-using 19 layout categoriesand 15 attribute labels. We conduct a thorough evaluation of both pipeline-based methods and endto-end vision-language models,revealing their strengths and weaknesses across different document types. OmniDocBench sets a new standard for the fair, diverse, and fine-grained evaluation in document parsing. Dataset and code are available at https : / /github. com/ opendatalab/OmniDocBench. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1. Introduction ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As large language models [1,28, 39, 44] increasingly rely onhigh-quality,knowledge-rich data,the importance ofaccurate document parsing has grown substantially. Document parsing,a core task in computer vision and document intelligence,aims to extract structured,machine-readable content from unstructured documents such as PDFs. This task is particularly critical for ingesting academic papers, technical reports, textbooks,and other rich textual sources into large language models,thereby enhancing their factual accuracy and knowledge grounding[19,42,45, 47,52]. Moreover,with the emergence of retrieval-augmented generation (RAG) systems [12,22],which retrieve and generate answers conditionally with external documents,the demand forprecise document understanding has further intensified. ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/b6daa2a8736184fcdb584c4f354cdac57e87f82567f5be9c124877739a1c3274.jpg",
        "image_caption": [
            "Figure 1. Results of End-to-End Text Recognition on OmniDocBench across 9 PDF page types. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "To address this challenging task,two main paradigms have emerged:1） Pipeline-based approaches that decompose the task into layout analysis,OCR，formula/table recognition,and reading order estimation [34,42];and 2) End-to-end vision-language models (VLMs） that directly output structured representations (e.g.,Markdown) [3, 7,8, 29,45,46, 48]. Although both approaches have demonstrated promising results,conducting a broad comparison of their effectiveness remains challenging due to the absence of a comprehensive and unified evaluation benchmark. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "As shown in Table1,for pipeline-based document parsing systems, dedicated benchmarks [10,26,54] have been ",
        "page_idx": 0
    }
]