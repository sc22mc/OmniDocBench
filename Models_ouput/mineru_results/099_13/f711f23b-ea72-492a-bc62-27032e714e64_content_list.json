[
    {
        "type": "text",
        "text": "Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang,and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-fedback generation. Blog post, May 2023. URLhttps://kaistai.github.io/SelFee/   \nWeizhe Yuan, Graham Neubig,and Pengfei Liu. Bartscore: Evaluating generated text as text generation.Advances in Neural Information Processing Systems,34:27263-27277,2021.   \nTianyi Zhang, Varsha Kishore,Felix Wu,Kilian Q Weinberger,and YoavArtzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations,2019.   \nXinghuaZhang,BowenYu,Haiyang Yu,YangyuLv,TingwenLiu,FeiHuang,HongboXu,andYongbinLi.Wider and deeper llm networks are fairer llm evaluators.arXiv preprint arXiv:2308.01862, 2023.   \nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, ZiLin,ZhuohanLi,DachengLi, Eric Xing,et al.Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685,2023.   \nChunting Zhou,PengfeiLiu,Puxin Xu,Srini Iyer,Jiao Sun,YuningMao,XuezheMa,AviaEfrat, Ping Yu,Lili Yu,etal.Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206,2023.   \nDanielMZiegler,NisanStiennon,JeffreyWu,omBBrown,AlecRadford,DarioAmodeiPaul Christiano,and Geofrey Irving.Fine-tuning language models from human preferences.arXiv ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "preprint arXiv:1909.08593,2019. ",
        "page_idx": 0
    }
]