[
    {
        "type": "text",
        "text": "gameplay becomes extreme in learning between those who can use equally sophisticated (i.e., multi-memory) strategies.We also founda novel problem that Nash equilibrium is diffcult to reach in multi-memory zerosumgames.Here,notethatconvergencetoNash equilibrium,eitherasalast-iterate[32,33,34,35,36,37,38] or as anaverageoftrajectories[39,40,41],isafrequentlydiscussedtopic.Ingeneral,heterocliniccylesfail to converge even on average.What algorithm can converge to Nash equilibrium in multi-memory zero-sum games would be interesting future work. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "References ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "[1] Drew Fudenberg and Jean Tirole. Game theory. MIT press,1991.   \n[2]JohnFNash Jr.Equilibrium points in n-person games.Proceedings of the National Academy of Sciences, 36(1):48-49,1950.   \n[3]John G Cross. A stochastic learning model of economic behavior.The Quarterly Journal of Economics, 87(2):239-266,1973.   \n[4] Tilman Borgers and Rajiv Sarin. Learning through reinforcement and replicator dynamics. Journal of Economic Theory,77(1):1-14,1997.   \n[5]Josef Hofbauer,Karl Sigmund,etal.Euolutionary games and population dynamics.Cambridgeuniversitypress,1998.   \n[6]Satinder Singh,Michael JKearns,and Yishay Mansour.Nash convergence of gradient dynamics in general-sum games.In UAI, pages 541-548,2000.   \n[7]Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent.In ICML, pages928-936,2003.   \n[8]Michael Bowlingand Manuela Veloso.Multiagent learning using a variable learning rate.Artificial Intelligence,136(2):215-250,2002.   \n[9]Michael Bowling.Convergence and no-regret in multiagent learning.In NeurIPS,pages 209-216,2004.   \n[10] Christopher JCH Watkins and Peter Dayan. Q-learning.Machine learning,8(3):279-292,1992.   \n[11]Michael Kaisers and Karl Tuyls.Frequency adjusted multi-agent q-learning.In AAMAS,pages 309-316, 2010.   \n[12] Sherief Abdallah and Michael Kaisers.Addressing the policy-bias of q-learning by repeating updates. In AAMAS, pages1045-1052, 2013.   \n[13] Panayotis Mertikopoulos and Wiliam H Sandholm. Learning in games via reinforcement and regularization.Mathematics of Operations Research,41(4):1297-1324,2016.   \n[14]Panayotis Mertikopoulos,Christos Papadimitriou,and Georgios Piliouras.Cycles in adversarial regularizedlearning.In SODA,pages2703-2717,2018.   \n[15] Karl Tuyls and Ann Nowé.Evolutionary game theory and multi-agent reinforcement learning.The Knowledge Engineering Review, 20(1):63-90,2005.   \n[16]Karl Tuyls,Pieter Jan'T Hoen,and Bram Vanschoenwinkel.An evolutionary dynamical analysis of multi-agent learning in iterated games.Autonomous Agents and Multi-Agent Systems,12(1):115-153, 2006.   \n[17] Daan Bloembergen,Karl Tuyls,Daniel Hennes,and Michael Kaisers.Evolutionary dynamicsof multiagent learning:A survey. Journal of Artificial Intelligence Research,53:659-697,2015.   \n[18] Wolfram Barfuss. Towards a unified treatment of the dynamics of colective learning. In Challenges and Opportunities for Multi-Agent Reinforcement Learning， AAAI Spring Symposium, 2020. ",
        "page_idx": 0
    }
]