[
    {
        "type": "text",
        "text": "which shows that X's payoff in the stationary state is $u ^ { * }$ , regardless of Y's strategy $\\textbf {  { y } }$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Below，we show that if X uses another strategy $\\boldsymbol { x } \\neq \\boldsymbol { x } ^ { * } \\mathbf { 1 }$ ,there always is Y's strategy such that $v ^ { \\mathrm { s t } } >$ $v ^ { * } \\Leftrightarrow u ^ { \\mathrm { s t } } < u ^ { * }$ .As X's non-equilibrium strategy，we assume the case $x _ { 1 } \\neq x ^ { * }$ representatively. Then,Y's strategy $\\pmb { y } = y ^ { * } \\mathbf { 1 } + \\mathrm { d } y _ { 1 } \\pmb { e } _ { 1 }$ with sufficiently small $\\mathrm { d } y _ { 1 }$ satisfies ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/49c21ab1b4dfa5262c7de4e649d020d31a68b4abd405f8382d0737924dca232d.jpg",
        "text": "$$\n\\begin{array} { r } { p ^ { \\mathrm { s t } } = \\left( \\begin{array} { l l l l } { x _ { 1 } ( y ^ { * } + \\mathrm { d } y _ { 1 } ) } & { x _ { 2 } y ^ { * } } & { x _ { 3 } y ^ { * } } & { x _ { 4 } y ^ { * } } \\\\ { x _ { 1 } ( \\tilde { y } ^ { * } - \\mathrm { d } y _ { 1 } ) } & { x _ { 2 } \\tilde { y } ^ { * } } & { x _ { 3 } \\tilde { y } ^ { * } } & { x _ { 4 } \\tilde { y } ^ { * } } \\\\ { \\tilde { x } _ { 1 } ( y ^ { * } + \\mathrm { d } y _ { 1 } ) } & { \\tilde { x } _ { 2 } y ^ { * } } & { \\tilde { x } _ { 3 } y ^ { * } } & { \\tilde { x } _ { 4 } y ^ { * } } \\\\ { \\tilde { x } _ { 1 } ( \\tilde { y } ^ { * } - \\mathrm { d } y _ { 1 } ) } & { \\tilde { x } _ { 2 } \\tilde { y } ^ { * } } & { \\tilde { x } _ { 3 } \\tilde { y } ^ { * } } & { \\tilde { x } _ { 4 } \\tilde { y } ^ { * } } \\end{array} \\right) p ^ { \\mathrm { s t } } . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this equation, we approximate $\\pmb { p } ^ { \\mathrm { s t } } \\simeq \\pmb { p } ^ { \\mathrm { s t } ( 0 ) } + \\pmb { p } ^ { \\mathrm { s t } ( 1 ) }$ ，where $ { \\boldsymbol { p } } ^ { \\mathrm { s t } ( k ) }$ describes the $O ( ( \\mathrm { d } y _ { 1 } ) ^ { k } )$ term in $p ^ { \\mathrm { s t } }$ We can derive these O-thand 1-st orderterms bycomparing the left-hand and right-side of this equation.Here, the $0$ -th order termsatisfies $\\pmb { p } ^ { \\mathrm { s t } ( 0 ) } \\cdot \\pmb { u } = \\pmb { u } ^ { * }$ ,which means that the term does not contribute to the deviation from the Nash equilibrium payoff. On the other hand, the 1-st order term gives ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/4ef5d111fe5aea6663153fbc18855ecf44f1bfe0a5fded341955119cd8d5ffed.jpg",
        "text": "$$\n\\begin{array} { r l } & { p ^ { \\mathrm { s t } ( 1 ) } = p _ { 1 } ^ { \\mathrm { s t } ( 0 ) } \\mathrm { d } y _ { 1 } ( + x _ { 1 } , - x _ { 1 } , + \\tilde { x } _ { 1 } , - \\tilde { x } _ { 1 } ) ^ { \\mathrm { T } } } \\\\ & { \\Rightarrow v ^ { \\mathrm { s t } ( 1 ) } = p _ { 1 } ^ { \\mathrm { s t } ( 0 ) } \\mathrm { d } y _ { 1 } \\underbrace { \\bigl ( v _ { 1 } - v _ { 2 } - v _ { 3 } + v _ { 4 } \\bigr ) } _ { = v \\cdot \\mathbf { 1 } _ { z } \\ne 0 } ( x _ { 1 } - x ^ { * } ) . } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Here, we use $\\mathbf { 1 } _ { z } : = ( + 1 , - 1 , - 1 , + 1 )$ .Thus, in the leading order, $v ^ { \\mathrm { s t ( 1 ) } } > v ^ { * } \\Leftrightarrow u ^ { \\mathrm { s t ( 1 ) } } < u ^ { * }$ holds by taking $\\mathrm { d } y _ { 1 } > 0$ if $v \\cdot { \\bf 1 } _ { z } ( x _ { 1 } - x ^ { * } ) > 0$ ,while by taking $\\mathrm { d } y _ { 1 } < 0$ if $v \\cdot { \\bf 1 } _ { z } ( x _ { 1 } - x ^ { * } ) < 0$ . In other words, X’s minimax strategy is $x = x ^ { * } \\mathbf { 1 }$ .Similarly，we can prove that Y's minimax strategy is $\\boldsymbol { y } = \\boldsymbol { y } ^ { * } \\mathbf { 1 }$ .Thus,the Nash equilibrium is given by $( \\pmb { x } , \\pmb { y } ) = ( \\pmb { x } ^ { * } \\mathbf { 1 } , \\pmb { y } ^ { * } \\mathbf { 1 } )$ □ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "BAnalysis of Learning Dynamics ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "B.1Simpler MMGA for Two-action Games ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "This section is concerned with the contents in Section 4.2 in the main manuscript. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Especially in two-action games,we can use the formulation of Assumption 1 in the main manuscript.By replacing the strategies $\\displaystyle ( \\mathbf { x } , \\mathbf { y } )$ by $( x , y )$ ,we can formulation another simpler algorithm of MMGA as ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Algorithm A1 DiscretizedMMGA for two-action ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Input: $\\eta$ ， $\\gamma$   \n1:for $t = 0 , 1 , 2 , \\cdots$ do   \n2: for $i = 1 , 2 , \\dots , | S |$ do   \n3: $x ^ { \\prime } \\gets x$   \n4: x←x²+γ   \n5: $\\Delta _ { i }  ( 1 - x _ { i } ) \\frac { u ^ { \\mathrm { s t } } ( { \\pmb x } ^ { \\prime } , { \\pmb y } ) - u ^ { \\mathrm { s t } } ( { \\pmb x } , { \\pmb y } ) } { \\gamma }$   \n6: end for   \n7: for $i = 1 , 2 , \\dots , | S |$ do   \n8: $x _ { i } \\gets x _ { i } ( 1 + \\eta \\Delta _ { i } )$   \n9: end for   \n10: $\\mathbf { x } \\gets \\mathrm { N o r m } ( \\mathbf { x } )$   \n11:end for ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "There is a major diference between the original and simpler MMGAs in lines 4and 5.The equivalence ",
        "page_idx": 0
    }
]