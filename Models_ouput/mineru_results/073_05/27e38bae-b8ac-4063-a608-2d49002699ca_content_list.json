[
    {
        "type": "text",
        "text": "Algorithm 2 (Discretized MMGA） takes not only its learning rate $\\eta$ but a small value $\\gamma$ in measuring an approximate gradient as inputs.In each time step,each player measures the gradients of its payof for each variable ofits strategy (lines 2-6).Then,the player updates its strategy by the gradients (lines 7-10). Here,note that the strategy update is weighted by the probability $x ^ { a | s }$ (line 8） in order to correspond to Algorithm 1.Here,each of lines 3-5 and line 8can beupdated in parallel with respect to $a$ and $s$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4Theoretical Analysis ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.1Continuous-Time Equivalence of Algorithms ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The following theorems provide a unified understanding of diferent algorithms.Theorem 1 and 2 are concerned with continualizationof the twodiscrete algorithms.Surprisingly，Theorem 3 proves thecorrespondence between these different continualized algorithms by Theorem 1 and 2. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Theorem 1 (Coutinualized MMRD). Let $\\pmb { p } ^ { a | s }$ be the expected distribution when $X$ chooses a under state s; ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/e7bdebe84668eff773a0789d43970d4f35d90a334ee9019ebf3d35ccb8d248ae.jpg",
        "text": "$$\np _ { i ^ { \\prime } } ^ { a | s } : = \\left\\{ \\begin{array} { l l } { y ^ { b | s } } & { ( s _ { i ^ { \\prime } } = a b s ^ { - } ) } \\\\ { 0 } & { ( \\mathrm { o t h e r w i s e } ) } \\end{array} \\right. .\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In the limit of $\\eta  0$ ，Algorithm $\\mathit { 1 }$ is continualized as dynamics ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/9c9e45a0651b20db19abe218e31ff85a23d820471eab6fec89be888b86ca8a99.jpg",
        "text": "$$\n\\begin{array} { l } { { \\displaystyle { \\dot { x } } ^ { a | s _ { i } } ( { \\bf x , y } ) = p _ { i } ^ { \\mathrm { s t } } x ^ { a | s _ { i } } \\left( \\pi ( p ^ { a | s _ { i } } , { \\bf x , y } ) - \\bar { \\pi } ^ { s _ { i } } ( { \\bf x , y } ) \\right) , } } \\\\ { { \\displaystyle { \\bar { \\pi } } ^ { s _ { i } } ( { \\bf x , y } ) = \\sum _ { a } x ^ { a | s _ { i } } \\pi ( p ^ { a | s _ { i } } , { \\bf x , y } ) , } } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "for all $a \\in { \\mathcal { A } }$ and $s \\in S$ Here, $\\bar { \\pi } ^ { s _ { i } }$ is the expected payoff under state $s _ { i }$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Theorem 2 (Continualized MMGA). In the limit of $\\gamma  0$ and $\\eta  0$ ，Algorithm $\\mathcal { L }$ is continualized as dynamics ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/852fd27bf76beadb0e3c31a70f7742f962e59244c31a99a804c13b638f3c8f46.jpg",
        "text": "$$\n{ \\dot { x } } ^ { a | s } ( \\mathbf { x } , \\mathbf { y } ) = x ^ { a | s } { \\frac { \\partial } { \\partial x ^ { a | s } } } u ^ { \\mathrm { s t } } ( \\mathrm { N o r m } ( \\mathbf { x } ) , \\mathbf { y } ) ,\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "forall $a \\in { \\mathcal { A } }$ and $s \\in S$ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "See Appendix A.1 and A.2 for the proof of Theorems 1 and 2. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Theorem 3 (Equivalence between the algorithms). The dynamics Eqs. (8) and (10) are equivalent. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Proof Sketch.Let $\\mathbf { x } ^ { \\prime }$ be the strategy given by $x ^ { a | s }  x ^ { a | s } + \\gamma$ in $\\mathbf { x }$ for $a \\in { \\mathcal { A } }$ and $s \\in S$ . Then, we consider the changes of the Markov transition matrix $\\mathrm { d } M : = M ( \\mathrm { N o r m } ( \\mathbf { x } ^ { \\prime } ) , \\mathbf { y } ) - M ( \\mathbf { x } , \\mathbf { y } )$ and the stationary distribution $\\mathrm { d } p ^ { \\mathrm { s t } } : = p ^ { \\mathrm { s t } } ( \\mathrm { N o r m } ( \\mathbf { x } ^ { \\prime } ) , \\mathbf { y } ) - p ^ { \\mathrm { s t } } ( \\mathbf { x } , \\mathbf { y } )$ .By considering this changes in the stationary condition $p ^ { \\mathrm { s t } } = M p ^ { \\mathrm { s t } }$ ， we get $\\mathrm { d } p ^ { \\mathrm { s t } } = ( { \\cal E } - { \\cal M } ) ^ { - 1 } \\mathrm { d } { \\cal M } p ^ { \\mathrm { s t } }$ in $O ( \\gamma )$ . The right-hand (resp. left-hand) side of this equation corresponds to the continualized MMRD (resp. MMGA). See Appendix A.3 for the full proof. □ ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "For games with a general number of actions,the study [7] has proposed a gradient ascent algorithm in relation toreplicator dynamics.In light of this study,Theorem 3extends the relation to the multi-memory games.This extension is neither simple nor trivial. The relation between replicator dynamics and gradient ascent has been proved by directly calculating $u ^ { \\mathrm { s t } } = p ^ { \\mathrm { s t } } \\cdot u$ [17]. In multi-memory games, however, $u ^ { \\mathrm { s t } } = p ^ { \\mathrm { s t } } { \\cdot } u$ is too hard to calculate.Thus,as seen in the proof sketch,we proved the relation by consideringa slight change in the stationary condition $p ^ { \\mathrm { s t } } = M p ^ { \\mathrm { s t } }$ ,technically avoiding such a hard direct calculation. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.2Learning Dynamics Near Nash Equilibrium ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Below,let us discuss the learning dynamics in multi-memory games,especially divergence from Nash equilibrium in zero-sum payoff matrices.In order to obtaina phenomenological insight into the learning dynamics simply,we assume one-memory two-action zero-sum games in Assumption 1. ",
        "page_idx": 0
    }
]