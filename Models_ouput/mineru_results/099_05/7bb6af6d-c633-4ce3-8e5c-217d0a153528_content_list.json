[
    {
        "type": "table",
        "img_path": "images/7083af11c66740fb470101b4840078e616f09af5e70ec593f228abc68f909616.jpg",
        "table_caption": [
            "Table 1: Agreement rates for pairwise comparison on different scenario groups and overallresults. Results with underline are the best among all models and results in bold are the second-best. The mapping from abbreviations to names of scenario groups are: $\\mathrm { S u m m }  \\colon$ Summarization, Crea $\\textrm { W } $ CreativeWriting,Func $\\textrm { W } $ Functional Writing,and Comm General Communication. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Summ</td><td>Exam</td><td>Code</td><td>Rewriting</td><td>CreaW</td><td>FuncW</td><td>Comm</td><td>NLP</td><td>Overall</td></tr><tr><td colspan=\"10\">Closed-source Models</td></tr><tr><td>ChatGPT</td><td>33.3</td><td>40.3</td><td>36.6</td><td>31.6</td><td>48.2</td><td>40.4</td><td>47.6</td><td>45.8</td><td>42.7</td></tr><tr><td>Claude-2</td><td>30.6</td><td>36.1</td><td>41.7</td><td>34.2</td><td>48.1</td><td>42.5</td><td>40.6</td><td>48.5</td><td>42.4</td></tr><tr><td>GPT-4</td><td>59.7</td><td>51.4</td><td>69.2</td><td>58.3</td><td>66.7</td><td>60.4</td><td>58.3</td><td>65.2</td><td>61.9</td></tr><tr><td colspan=\"10\">Open-source Models</td></tr><tr><td>SteamSHP</td><td>33.3</td><td>29.2</td><td>26.7</td><td>33.3</td><td>40.7</td><td>31.3</td><td>51.4</td><td>51.9</td><td>40.6</td></tr><tr><td>PandaLM</td><td>29.2</td><td>33.3</td><td>31.7</td><td>23.3</td><td>43.5</td><td>32.9</td><td>44.8</td><td>48.9</td><td>38.9</td></tr><tr><td>LLaMA-2-Chat-13B</td><td>20.8</td><td>27.8</td><td>19.2</td><td>20.0</td><td>31.5</td><td>27.5</td><td>35.8</td><td>31.8</td><td>29.0</td></tr><tr><td>Vicuna-13B-v1.5</td><td>30.6</td><td>23.6</td><td>35.0</td><td>28.3</td><td>36.1</td><td>37.5</td><td>45.5</td><td>39.8</td><td>37.3</td></tr><tr><td>WizardLM-13B-v1.2</td><td>22.2</td><td>20.8</td><td>32.5</td><td>19.2</td><td>28.7</td><td>25.4</td><td>29.2</td><td>33.0</td><td>27.8</td></tr><tr><td>LLaMA-2-chat-70B</td><td>34.7</td><td>33.3</td><td>36.7</td><td>35.8</td><td>51.4</td><td>54.2</td><td>47.2</td><td>47.7</td><td>45.9</td></tr><tr><td>AUTO-J</td><td>45.8</td><td>38.9</td><td>59.2</td><td>47.5</td><td>54.6</td><td>57.1</td><td>58.0</td><td>57.6</td><td>54.8</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "annotated with a reformatted judgment (or discarded),or we have collected 1OO samples for this scenario.The final size of pairwise training data is 3,436,and the detailed statistics are in Tab.[21 ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Single-response: For single-response, we pick 96O query-response pairs from Chatbot Arena Conversations with a balanced sampling on different scenarios.In preliminary experiments,directly incorporating the scenario criteria as the system message (as in pairwise evaluation)impairs GPT-4's performance on single-response assessment, overly constraining its generated output to the scenariospecific criteria.Therefore,we adopt a“divide-and-conquer”strategy:We collect two pieces of critiques from GPT-4 for a single response with and without scenario criteria as a system message, andthen in the third inference,we getthe final evaluation judgment by asking GPT-4 tocombine these two critiques into a more comprehensive critique and give a final rating. The user message prompt and the prompt for combining critiques are in Tab.12and13 and the detailed statistics are shown in Tab. $\\left| 2 2 \\right|$ Tab. $\\boxed { 2 0 }$ shows an example from the“planning” scenario. We find that critiques generated with and without scenario criteria exhibit distinct stylistic differences: The former is longer and closely adheres to the given criteria, whereas the latter is more concise yet capable of incorporating details not covered by the criteria.Finally,combining the above two critiques,a comprehensive critique simultaneously contains general criteria for this scenario and specific details for this sample. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Input format: Besides the collected evaluation judgments,we also need to determine the input format for AUTo-J.In early-stage experiments,we attempted to include the scenario criteria as the system message inthe input.However,models trained inthis manner performed poorly,often simply paraphrasing the scenario criteria.Therefore,we adopt a technique akin to Context Distillation $\\mathrm { ( } \\varlimsup \\mathrm { ) }$ $\\boxed { \\mathrm { e t ~ a l . } } \\boxed { 2 0 2 2 \\mathrm { b } }$ and Ghost Attention (Touvron et al.2O23b), where we omit the inclusion of scenario criteria in the input for the training data, allowing the model to learn them from the output end implicitly.This design significantly enhances the generality of AUTo-J.The final input formats for pairwise comparison and single-response evaluation are in Tab.17and Tab.[18 respectively. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4TRAINING AUTO-J ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Byintegrating data from both pairwise and single-response evaluations,we train our model to seamlessly toggle between diverse evaluation protocols simply by applying the corresponding prompts. To lessen the positional bias $\\pmb { \\mathrm { \\overbrace { W a n g \\ e t { \\ a l . } } } } \\mathbf { \\overbrace { 2 0 2 3 a } } )$ in pairwise comparison,we applya simple data augmentation trick. For each pairwise training sample,we swap the order of two responses in the input and alternate the“Response 1\"and \"Response $2 ^ { \\circ }$ in the evaluation judgment. Since this doubles the pairwise data,we balanced the dataset by duplicating each single-response samples as well. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We train AUTo-J from LLaMA-2-13B-chat (Touvron et al.2023b) with the DeepSpeed (Rasley et al. $\\boxed { 2 0 2 0 }$ library, Zero Redundancy Optimizer (ZeRO) (Rajbhandari et al.|2020 Ren et al. 2021 Stage 3, gradient-checkpointing $\\boxed { \\overline { { \\mathbf { C h e n ~ e t ~ a l . } } } \\boxed { 2 0 1 6 } }$ and FlashAttention (Dao et al.2022|Dao|2023 on 8 NVIDIA A100 GPUs.We use the bfloat16 (BF16) and tfloat32 (TF32) mix computation precision ",
        "page_idx": 0
    }
]