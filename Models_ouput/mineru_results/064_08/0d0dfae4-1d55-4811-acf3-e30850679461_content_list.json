[
    {
        "type": "image",
        "img_path": "images/91fd5a6c5195157ef23602a3ca501590f3d9dcc0779cb30978cde50cba0646f0.jpg",
        "image_caption": [
            "Figure 3.We show the visualizations of 5 procedures of QR-CLIP.For each proces,the reader can refer to Fig.2 "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.5.Limitation and Future Work ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Weare still in the early stages of investigating how to best use CLIP and the QR principle to explore open-world knowledge to support location and time reasoning. And the modules and techniques developed are simple but effective. In the future:1） we will investigate more efficient and elegant implementations;2） while addressing the limited computational resources,collect a larger OWK dataset as input candidates;3) using multimodal OWKs to see if images fromInstagram,Twitter,etc.could help with this task. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "5. Conclusion ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We designed a novel QR-CLIP model. It consists of two modules:1） the Quantity module and 2） the Relevance module. Experiments show that it outperforms all previous SOTA on location and time reasoning by a wide margin. To show how our designed components affect the model, we conduct comprehensive ablation studies and verify that open-world knowledge is beneficial for solving our problem. We hope this paper will serve as a technical foundation for this study area and inspire more fascinating research. ",
        "page_idx": 0
    }
]