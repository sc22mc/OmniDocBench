[
    {
        "type": "table",
        "img_path": "images/91565e4c50da5e31ea9520cce88c77bbe872d6db1862e36a116f3b14724babc6.jpg",
        "table_caption": [
            "Table1: Utilizing BoT with GPT-4,even without human annotations,yields a notable performance enhancement. Once the simple initial prompt of BoT contains CoT examples,the corresponding approach $\\mathrm { B o T + C o T }$ exhibits even higher solving rates. Our framework is also evaluated against leading methods such as Model Selection Zhao et al.(2023),PHPZheng et al.(2023),and CSVZhou et al.(2023a),each achieving state-of-the-art (SOTA) performance on the SVAMP, AQuA,and GSM8K& MATH datasets, respectively. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"2\">Methods</td><td> Humn Annodation</td><td colspan=\"4\">GSM8K DatasetSQUA</td><td rowspan=\"2\">Average</td></tr><tr><td></td><td>SVAMP</td><td></td><td></td><td>MATH</td></tr><tr><td>SOTA</td><td></td><td>93.7</td><td>97</td><td>79.9</td><td>84.3</td><td>88.7</td></tr><tr><td>Standard</td><td></td><td>68.7</td><td>87.1</td><td>40.6</td><td>42.5</td><td>59.7</td></tr><tr><td>CoT</td><td></td><td>77.6</td><td>92</td><td>74.0</td><td>48.93</td><td>73.1</td></tr><tr><td>Zero-shot CoT</td><td></td><td>74.3</td><td>89.6</td><td>73.2</td><td>47.7</td><td>71.2</td></tr><tr><td>Complex-CoT</td><td></td><td>90.5</td><td>94.9</td><td>77.5</td><td>50.4</td><td>78.3</td></tr><tr><td>PHP Complex-CoT</td><td>x&gt;x&lt;xx</td><td>91.9</td><td>95.5</td><td>79.9</td><td>53.9</td><td>80.3</td></tr><tr><td>BoT</td><td></td><td>92.7(↓1)</td><td>97.1(↑0.1)</td><td>81.4(个 2.5)</td><td>62.5(↓21.8)</td><td>83.7(↓7.6)</td></tr><tr><td>BoT+CoT</td><td>&lt;x</td><td>94.9(个1.2)</td><td>98.7(1.7)</td><td>84.9（个5）</td><td>66.3（18）</td><td>86.2(↓2.5)</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/bfb43077f48fda46ee01a693de991b1cb575a9b7a77dc71fae052e3b342f9f86.jpg",
        "image_caption": [
            "Figure 3: Evaluating solve rates by applying BoTand $\\mathrm { B o T + C o T }$ in GPT-4OpenAI2023 and Llama2 Touvron [et al(2023). "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "$1 . 3 \\%$ on average in GSM8K and AQuA datasets. We argue that the CoT examples can be regarded as the success cases in the experience,directly guiding the subsequent thought structures generation of BoT.Thus,cooperating with the iteration refinement, $\\mathrm { B o T + C o T }$ reachesanew SOTA.Italso deserves to show that because BoT can gradualy collect analysis of various reasoning chains (bad or good) as experience,it is consistently close to the $\\mathrm { B o T + C o T }$ However,BoTand $\\mathrm { B o T + C o T } .$ especially BoT, are at least $1 8 \\%$ lower than SOTA in MATH. This observation means weak LLMs may not perform well with BoTdue to their lower ability to analyze reasoning chains for an effective experience,as supported by Fig. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Fig. $3$ presents that with BoT, GPT-4 and Llama2 are respectively improved by $1 1 . 6 \\%$ and $4 . 4 \\%$ on average in three datasets.The two numbers show a clear trend that when the LLM is weaker,BoT's performance drops significantly. With powerful GPT-4, as presented in Fig. $\\textcircled { 3 }$ BoT and BoT-CoT behave similarly to those shown in Table.Additionally,their performance escalates along a similar trend as the numberof trees varies from1 to 20.AsLlama2 is weaker,BoTis unable tobenefit from its analysis to perform the experience-driven iteration process,which is particularly shown by Fig. (a). When provided with valid success cases,i.e.,5-shots,BoT,through progressive refinement,can stillhelp Llama2 to solve more problems than the baseline even though the improvement is limited. ",
        "page_idx": 0
    }
]