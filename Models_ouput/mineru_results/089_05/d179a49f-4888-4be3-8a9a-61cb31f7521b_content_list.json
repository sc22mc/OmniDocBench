[
    {
        "type": "image",
        "img_path": "images/5fb0901a560a844318e58338d37bc5c2adf78db49bbc73110c21121d0f3f4d26.jpg",
        "image_caption": [
            "Figgit filein order to automatize the process of producing the following results. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "table",
        "img_path": "images/6edf8d64d4fdda4fb4514581a017053339435b67d4fe6c13db20f6e46dd19fab.jpg",
        "table_caption": [
            "TABLEII NNARCHITECTURE "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Layer</td><td>Size</td></tr><tr><td>Input Inner1 Inner2 Output</td><td>Depends on the feature size 16 fully connected (ReLU) 8 fully connected (ReLU) Sigmoid function</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "in [29],named dataset2.This dataset presentsa different subfolders organization with respect to dataset 1[4] that was already available.Subsequently，we have incorporated the ability to amalgamateextracted featuresfromdiversedatasetsand store them in distinct folders.This facilitates the efficient reuse of data for conducting multiple experiments on audio chunks with identical settings, preventing the need to re-extract features that are already available.The complete dataflow is shown inFig.1, where itis visible that the featuresare merged only after the extraction from the relative dataset,and following operations are performed as they would be done for a single dataset. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "III.RESULTS ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "A. Experiment Setup ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "All the experiments execute the complete sequences of steps described in Fig.3. Starting from a base configuration,each group of experiments explored the effect of different settings on the classification performances of the model. The base configuration has the following settings. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "1）Audio chunk split:Length of $5 \\mathrm { ~ s ~ }$ with a hop size of 5 s, so they are contiguous without overlapping.   \n2） Feature extraction: For each experiment two types of audio features are extracted:MFCC with 2O coefficients and STFT with a window size of 1024 samples.   \n3）Training/test data split:Trainingdata are $80 \\%$ and the remaining $20 \\%$ are used for the final test.   \n4）K-fold cross-validation: It is performed dividing the training set into 10 folders.   \n5）Classifiers: TheNNhas two fully connected inner layers with anarchitecture resumed in TableII,and the SVMis trained with the C parameter set to one. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "B. NN Size Influence ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In the subsequent set of experiments,we explored the impact of varying the NN size by adjusting the number of layers from 1 to 4 and modifying the number of neurons per layer within the range of2 to 8.The experiments were conducted separately using MFCC features and STFT features.The findings,presented in Figs.4and5,align with the trend observedin the prior study [23], indicating that larger networks tend to achieve higher F1-score. Itis important to note,however, that when utilizing combined datasets,aslightlyloweraccuracy is observed compared to the results obtained with individual datasets.The simplest network, with four layers and two neurons,shows a reduction of the F1- score of about $8 \\%$ and $5 \\%$ withMFCC and STFT,respectively. However,the gapisreduced to $1 \\%$ orlowervalueswhen at least six neurons per layer are present. Colors in Figs.4 and 5 are scaled to the same range to make them comparable. It is possibleto visually observe that STFT featuresbetterperformed over MFCC features in almost all cases,with exceptions for the smallest networks. ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/b8ec993fbc7eeeead04ea43629d051eac34e607034d1ccc333651a9c67ce7eaa.jpg",
        "image_caption": [
            "Fig.4.Cross-validation and final test results changing the number of layers and the number of nodes in the NNusing theMFCC features.Onthe left the F1-score using only dataset1and on the right combining dataset1and dataset2. In these figures is reported the mean value $\\pm$ the standarddeviationand,between parenthesis,thefinal test. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/b955b3a873bba80fd4b8b7e43329af4975cc31a56f140a4607a5e8b79b44226a.jpg",
        "image_caption": [
            "Fig.5.Cross-validation and final test results changing the number of layers and the number of nodesin theNNusing the STFT features.On theleft the F1-scoreusingonlydataset1and ontherightcombiningdataset1and dataset 2.In these figures is reported the mean value $\\pm$ thestandard deviation and, between parenthesis,the final test. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    }
]