[
    {
        "type": "table",
        "img_path": "images/5d2678249680b5c338b28bde21424742137400f5e886082c902a8620c1ef36fc.jpg",
        "table_caption": [],
        "table_footnote": [
            "Table1.Summary of the performance for different baselines on the image location and time prediction. $\\dagger$ means fine-tune the original CLIP (Radford et al., 2021). $\\because G V G ^ { \\prime }$ average relative lift. "
        ],
        "table_body": "<table><tr><td>ID</td><td>Method</td><td>Training Mode</td><td>Accuracy (~ Rank@ 1)</td><td>Rank@5</td><td>Example-F1</td><td>F1-Score</td></tr><tr><td colspan=\"7\">Location Reasoning</td></tr><tr><td></td><td>Swin-T-50 (He at.a1.2016)</td><td>Supervised</td><td>3.70%</td><td>17.0%</td><td>22.10%</td><td>2.27%</td></tr><tr><td>12</td><td></td><td>Zero-Shot</td><td></td><td></td><td></td><td></td></tr><tr><td>3</td><td>CLIP (Radford et al., 2021)</td><td></td><td>11.11%</td><td>27.85%</td><td>44.96%</td><td>9.74%</td></tr><tr><td>4</td><td>CLIP† (Fu et al., 2022)</td><td>Fine-tune</td><td>15.72%</td><td>37.13%</td><td>49.74%</td><td>13.82%</td></tr><tr><td>5</td><td>CLIP+Seg (Fu et al.,2022)</td><td>Fine-tune</td><td>16.46%</td><td>37.48%</td><td>50.52%</td><td>14.63%</td></tr><tr><td>6</td><td>QR-CLIP (Ours)</td><td>Fine-tune</td><td>19.31%</td><td>38.78%</td><td>50.96%</td><td>17.70%</td></tr><tr><td></td><td>Improvements (AVG: 10.66%)</td><td></td><td>+17.31%</td><td>+3.47%</td><td>+0.87%</td><td>+20.98%</td></tr><tr><td colspan=\"7\">Time Reasoning</td></tr><tr><td colspan=\"7\"></td></tr><tr><td>7</td><td>ResNet-50 (He et al., 2016)</td><td>Supervised</td><td>0.84%</td><td>5.14%</td><td>39.99%</td><td>0.46%</td></tr><tr><td>8</td><td>Swin-T (Liu et al., 2021)</td><td>Supervised</td><td>0.97%</td><td>5.53%</td><td>43.95%</td><td>0.72%</td></tr><tr><td>9</td><td>CLIP (Radford et al., 2021)</td><td>Zero-Shot</td><td>0.46%</td><td>2.42%</td><td>39.90%</td><td>0.25%</td></tr><tr><td>10</td><td>CLIP† (Fu et al., 2022)</td><td>Fine-tune</td><td>1.00%</td><td>3.07%</td><td>43.09%</td><td>0.54%</td></tr><tr><td>1</td><td>CLIP+Seg (Fu et al., 2022)</td><td>Fine-tune</td><td>0.92%</td><td>3.15%</td><td>42.89%</td><td>0.71%</td></tr><tr><td>12</td><td>QR-CLIP (Ours)</td><td>Fine-tune</td><td>3.53%</td><td>10.90%</td><td>47.89%</td><td>2.01%</td></tr><tr><td colspan=\"2\"> Improvements (AVG: 134.38%)</td><td></td><td>+253%</td><td>+97.11%</td><td>+8.23%</td><td>+179.17%</td></tr></table>",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "$W _ { i } ^ { o w k }$ and $W _ { i } ^ { \\nu }$ are the weights of the $\\mathsf { C L S } \\rfloor _ { i } ^ { o w k }$ and $[ \\mathrm { C L S } ] _ { i } ^ { \\nu }$ $q$ in this place is the addition of weight vision-language features $\\bar { W } _ { i } ^ { o w k } \\times \\left[ \\mathrm { C L S } \\right] _ { i } ^ { o w k } + W _ { i } ^ { \\nu } \\times \\left[ \\mathrm { C L S } \\right] _ { i } ^ { \\nu }$ q is the groundtruth features generated by $F ^ { G T } = \\mathrm { E n c } _ { t } ( G T )$ ： ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Locationand TimeReasoning.We use the fused features $\\begin{array} { r } { F ^ { f u s e d } = \\sum _ { 1 } ^ { 6 } ( W _ { i } ^ { o w k } \\times \\left[ \\mathbb { C } \\mathbb { Z } S \\right] _ { i } ^ { o w k } + W _ { i } ^ { \\nu } \\times \\left[ \\mathbb { C } \\mathbb { L } S \\right] _ { i } ^ { \\nu } ) } \\end{array}$ as our final features to predict the location and time.The prediction is completed by calculating the similarity between $F ^ { f u s e d }$ and the candidate location/time embeddings. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "We believe that by using the CLIP pre-trained 400M openworld corpus and then fine-tuning it by adding additional [CLS]with location-and-time-specific data, it can basically reason about meta information.QR-CLIPwill then improve its performance by retrieving valuable open-world knowledge and using it as auxiliary cues.Finally, the model balances vision and language embeddings,and by incorporating them into prediction, the model achieves its peak performance. The process is related to Horn's QR rule (Horn, 1984). Also,it mimics a procedure of information spreading (Wang et al.,2O11): diverse individuals have diverse perspectives and attitudes regarding the same thing (sec 3.2), butcombining them effectively fosters amoreprofound comprehension (sec 3.3). ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.Experiments ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "4.1. Training Settings ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Dataset. We used two datasets: TARA dataset (Fu et al., 2022) and ourcollected OWK dataset. TARA dataset includes 15,429 samples. Each sample contains a news picture and the corresponding location, time description. Following the original setup,we train QR-CLIP ona train set containing 12,3O6 instances and evaluate our method using a test set containing1,644 instances.The OWKdatasetisderived from the WIT dataset (Srinivasan et al., 2021). Considering the limited computation resource, we only use 122,408 texts from the 37.5 million entity-rich image-text examples in English Wikipedia that correspond to the countries and years as our open-world knowledge. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "EvaluationMetrics.Fora fair comparison,we first follow thesame evaluationmetricson the TARAbenchmark(Fu et al.,2022): Accuracy and Example-F1. Accuracy is calculated by comparing the predicted results with the entire labels.Example-F1 is calculated by comparing predictions withhierarchicallabels: ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/e884dfb7d08317f9f8bfb41f497e0cd120ff0f25dd1b5cd76cb10854516d39b8.jpg",
        "text": "$$\n{ \\mathrm { E x a m p l e - F } } 1 = { \\frac { 1 } { N } } \\sum _ { i = 1 } ^ { N } { \\frac { 2 \\left| { \\mathrm { G T } } _ { i } \\cap { \\mathrm { P r e d } } _ { i } \\right| } { \\left| { \\mathrm { G T } } _ { i } \\right| + \\left| { \\mathrm { P r e d } } _ { i } \\right| } } ,\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "where $\\mathrm { G T } _ { i }$ represents the hierarchical label,and $\\mathrm { P r e d } _ { i }$ represents the hierarchical prediction. If the entire label is{Zurich,Switzerland, Europe'},the progressive hierarchical labels are the three combinations of true label as $\\{$ Zurich,Switzerland,Europe'}, $\\{$ ‘Switzerland,Eu$r o p e ^ { , } 5 9 4 8 \\}$ and $\\{ \\cdot E u r o p e ^ { \\prime } \\}$ .Inaddition, $\\mathrm { R a n k } @ 5$ and F1-Score are utilized to evaluate the performance of the proposed method. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Implementation Details. QR-CLIP is based on $\\mathrm { C L I P + V I T }$ B/32 model with an input size of $2 2 4 \\times 2 2 4$ ,and itis implementedon thePyTorch1.10.1 platformwith theAdam optimizer to update the neural network's weights and biases. The training batch size is 32,and the initial learning rate is $1 e - 6$ .Our model utilizes a pre-trained model and takes hoursinthe fine-tune process on an NVIDIA RTX 3090 GPU running CUDA 11.7.1. ",
        "page_idx": 0
    }
]