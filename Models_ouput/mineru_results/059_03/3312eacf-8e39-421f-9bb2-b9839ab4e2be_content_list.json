[
    {
        "type": "image",
        "img_path": "images/a99cd5d861ef750ab819b3cfb60c317c29fa3514f5a8245af0c4e7e3d5f9e731.jpg",
        "image_caption": [
            "Figure 2: The overall structure of the proposed HAHT model,which contains 1) hierarchical history conversation encoder,2) history-aware context encoder,and 3) history-aware response generator. The details of each component are shownin Figure 3,4,5,respectively. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "their methods need to retrieve a very large portion ofhistory conversationsto achieve better results than the standard Transformer. In addition, these models still needto concatenate theretrieved raw history conversation text with the current conversation context, yielding concatenations that are still much longer than the 128 token truncation lengths. Therefore, the incorporation of historical contexts in these methods is still limited by the short token truncation lengths of pre-trained models. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3The Proposed Method ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In general,a Multi-Session Conversation (MSC) consists of a current conversation session and several history conversation sessions that happen before the current one, all between the same two interlocutors.A multi-session open-domain dialogue system aims to generate natural, well-informed, and context-relevant responses to the user's utterances based on all history conversation sessions and the current conversation context. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Formally, we denote the MSC dataset $D$ bya list of $N$ conversations in the format of $( H , X , y )$ Here, $X ~ = ~ \\{ x _ { 1 } , x _ { 2 } , \\cdots , x _ { n _ { x } } \\}$ denotes $n _ { x }$ context utterances of the current conversation session. $H = \\{ H ^ { 1 } , H ^ { 2 } , \\cdots , H ^ { M } \\}$ denotes $M$ history conversation sessions, where $H ^ { i } = \\{ h _ { 1 } ^ { i } , h _ { 2 } ^ { i } , \\cdots , h _ { n _ { i } } ^ { i } \\}$ denotes $n _ { i }$ chronologically ordered utterances of the $i$ -th history conversation session. $y$ isthe ground truth response to $X$ under the background of $H$ .The MSC task can be formulated as learning a function $f ( H , X )$ to predict the next utterance $x _ { n _ { x } + 1 }$ based on $H$ and $X$ ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/f4c1f6f0e49fb8c39b836942700eef063e68062c767e39e2ebc797cb9047b2f3.jpg",
        "image_caption": [
            "Figure 3: The structure of the hierarchical history conversation encoderin HAHT. "
        ],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "In this work,we propose a novel model, namely HAHT,for the MSC task.Figure 2 shows the overall structure ofHAHT,which consists of three main components:1） hierarchical history conversation encoder,2) history-aware context encoder,and 3) history-aware response generator. We present the details of each component of HAHT as follows. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "3.1Hierarchical History Conversation Encoder ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "The main challenge in encoding history conversation sessions is the limited maximum input length imposed by pre-trained dialogue systems.If all history conversations are simply concatenated and fed into the pre-trained dialogue system,the length of the concatenation will exceed the maximum input length. Thus,most parts of the input will be truncated. To preserve more information in the history conversation,we encode each history conversation session separatelyina hierarchical fashion. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Specifically, for a history conversation session $H ^ { i } = \\{ h _ { 1 } ^ { i } , h _ { 2 } ^ { i } , \\cdots , h _ { n _ { i } } ^ { i } \\}$ we first prepend aspecial token“User:”or“Assistant:”to each utterance $h _ { j } ^ { i }$ in $H ^ { i }$ depending on the role of the utterance speaker,and then pad all utterances to the same length $l _ { u t t e r }$ .For each utterance $h _ { j } ^ { i }$ ,we apply an embedding layer $E _ { m }$ ， $n _ { e n c }$ Transformer encoder layers,and a Max-poolinglayer to obtain its dense representation as follows, ",
        "page_idx": 0
    },
    {
        "type": "equation",
        "img_path": "images/ce6dc71405248c18512543f99002efe13451b6c6f2d6481795bc0dd2735475c5.jpg",
        "text": "$$\n\\mathbf { u } _ { j } ^ { i } = \\mathbf { M a x - p o o l i n g } \\big ( \\mathrm { T r a n s f o r m e r } _ { n _ { e n c } } ( E _ { m } ( h _ { j } ^ { i } ) ) \\big ) ,\n$$",
        "text_format": "latex",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "where $\\mathbf { u } _ { j } ^ { i } \\in \\mathbb { R } ^ { d }$ .Moreover, we denote all the utterance representations in the history conversation $H ^ { i }$ by $\\mathbf { U } ^ { i } = \\{ \\mathbf { u } _ { 1 } ^ { i } , \\mathbf { u } _ { 2 } ^ { i } , \\cdots , \\mathbf { u } _ { n _ { i } } ^ { i } \\} \\in \\mathbb { R } ^ { n _ { i } \\times d }$ ,where $n _ { i }$ is the turn number of $H ^ { i }$ Next,we apply a ",
        "page_idx": 0
    }
]