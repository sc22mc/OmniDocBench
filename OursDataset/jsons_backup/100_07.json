{
  "layout_dets": [
    {
      "category_type": "header",
      "poly": [
        450.00000000000006,
        110,
        1225,
        110,
        1225,
        152.5,
        450.00000000000006,
        152.5
      ],
      "ignore": false,
      "order": 1,
      "anno_id": 1,
      "text": "Published as a conference paper at ICLR 2024",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            450.00000000000006,
            110,
            1225,
            110,
            1225,
            152.5,
            450.00000000000006,
            152.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "table_caption",
      "poly": [
        440,
        337.5,
        2102.5,
        337.5,
        2102.5,
        552.5,
        440,
        552.5
      ],
      "ignore": false,
      "order": 2,
      "anno_id": 1,
      "text": "**Table 1:** Utilizing BoT with GPT-4, even without human annotations, yields a notable performance enhancement. Once the simple initial prompt of BoT contains CoT examples, the corresponding approach BoT+CoT exhibits even higher solving rates. Our framework is also evaluated against leading methods such as Model Selection Zhao et al. (2023), PHP Zheng et al. (2023), and CSV Zhou et al. (2023a), each achieving state-of-the-art (SOTA) performance on the SVAMP, AQuA, and GSM8K & MATH datasets, respectively.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            440,
            337.5,
            2102.5,
            337.5,
            2102.5,
            552.5,
            440,
            552.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "figure_caption",
      "poly": [
        437.5,
        2085,
        2100,
        2085,
        2100,
        2170,
        437.5,
        2170
      ],
      "ignore": false,
      "order": "5",
      "anno_id": 1,
      "text": "Figure 3: Evaluating solve rates by applying BoT and BoT+CoT in GPT-4 OpenAI (2023) and Llama2 Touvron et al. (2023).",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            437.5,
            2085,
            2100,
            2085,
            2100,
            2170,
            437.5,
            2170
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "figure",
      "poly": [
        445,
        1102.5,
        2100,
        1102.5,
        2100,
        2050,
        445,
        2050
      ],
      "ignore": false,
      "order": 4,
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            445,
            1102.5,
            2100,
            1102.5,
            2100,
            2050,
            445,
            2050
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "table",
      "poly": [
        440,
        590,
        2105,
        590,
        2105,
        1027.5,
        440,
        1027.5
      ],
      "ignore": false,
      "order": "3",
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            440,
            590,
            2105,
            590,
            2105,
            1027.5,
            440,
            1027.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": "",
        "table_layout": "",
        "language": ""
      },
      "html": "| Methods         | No need<br>Human Annotation | Datasets            |                     |                     |               | Average      |\n|-----------------|-----------------------------|---------------------|---------------------|---------------------|---------------|--------------|\n|                 |                             | SVAMP               | GSM8K               | AQuA                | MATH          |              |\n| SOTA            | X                           | 93.7                | 97                  | 79.9                | 84.3          | 88.7         |\n| Standard        | ✓                           | 68.7                | 87.1                | 40.6                | 42.5          | 59.7         |\n| CoT             | X                           | 77.6                | 92                  | 74.0                | 48.93         | 73.1         |\n| Zero-shot CoT   | ✓                           | 74.3                | 89.6                | 73.2                | 47.7          | 71.2         |\n| Complex-CoT     | X                           | 90.5                | 94.9                | 77.5                | 50.4          | 78.3         |\n| PHP Complex-CoT | X                           | 91.9                | 95.5                | 79.9                | 53.9          | 80.3         |\n| BoT             | ✓                           | 92.7 (↓ 1)          | <b>97.1</b> (↑ 0.1) | <b>81.4</b> (↑ 2.5) | 62.5 (↓ 21.8) | 83.7 (↓ 7.6) |\n| BoT + CoT       | X                           | <b>94.9</b> (↑ 1.2) | <b>98.7</b> (↑ 1.7) | <b>84.9</b> (↑ 5)   | 66.3 (↓ 18)   | 86.2 (↓ 2.5) |\n",
      "latex": ""
    },
    {
      "category_type": "plain_text",
      "poly": [
        442.5,
        2292.5,
        2107.5,
        2292.5,
        2107.5,
        2660,
        442.5,
        2660
      ],
      "ignore": false,
      "order": 6,
      "anno_id": 1,
      "text": "$1.3\\%$  on average in GSM8K and AQuA datasets. We argue that the CoT examples can be regarded as the success cases in the *experience*, directly guiding the subsequent thought structures generation of BoT. Thus, cooperating with the iteration refinement, BoT+CoT reaches a new SOTA. It also deserves to show that because BoT can gradually collect analysis of various reasoning chains (bad or good) as *experience*, it is consistently close to the BoT+CoT. However, BoT and BoT+CoT, especially BoT, are at least  $18\\%$  lower than SOTA in MATH. This observation means weak LLMs may not perform well with BoT due to their lower ability to analyze reasoning chains for an effective *experience*, as supported by Fig. 3.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            442.5,
            2292.5,
            2107.5,
            2292.5,
            2107.5,
            2660,
            442.5,
            2660
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        450,
        2680,
        2115,
        2680,
        2115,
        3047.5,
        450,
        3047.5
      ],
      "ignore": false,
      "order": 7,
      "anno_id": 1,
      "text": "Fig. 3 presents that with BoT, GPT-4 and Llama2 are respectively improved by  $11.6\\%$  and  $4.4\\%$  on average in three datasets. The two numbers show a clear trend that when the LLM is weaker, BoT's performance drops significantly. With powerful GPT-4, as presented in Fig. 3, BoT and BoT-CoT behave similarly to those shown in Table. 1. Additionally, their performance escalates along a similar trend as the number of trees varies from 1 to 20. As Llama2 is weaker, BoT is unable to benefit from its analysis to perform the *experience*-driven iteration process, which is particularly shown by Fig. 3 (a). When provided with valid success cases, i.e., 5-shots, BoT, through progressive refinement, can still help Llama2 to solve more problems than the baseline even though the improvement is limited.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            450,
            2680,
            2115,
            2680,
            2115,
            3047.5,
            450,
            3047.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "footer",
      "poly": [
        1242.5,
        3117.5,
        1285,
        3117.5,
        1285,
        3175,
        1242.5,
        3175
      ],
      "ignore": false,
      "order": 8,
      "anno_id": 1,
      "text": "7",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1242.5,
            3117.5,
            1285,
            3117.5,
            1285,
            3175,
            1242.5,
            3175
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    }
  ],
  "extra": {
    "relation": []
  },
  "page_info": {
    "page_attribute": {},
    "page_no": 26,
    "height": 3300,
    "width": 2550,
    "image_path": "100_7_png.jpg"
  }
}