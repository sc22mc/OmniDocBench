{
  "layout_dets": [
    {
      "category_type": "header",
      "poly": [
        177.5,
        135,
        217.5,
        135,
        217.5,
        172.5,
        177.5,
        172.5
      ],
      "ignore": false,
      "order": 1,
      "anno_id": 1,
      "text": "238 ",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            177.5,
            135,
            217.5,
            135,
            217.5,
            172.5,
            177.5,
            172.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "header",
      "poly": [
        1037.5,
        132.5,
        2315,
        132.5,
        2315,
        167.5,
        1037.5,
        167.5
      ],
      "ignore": false,
      "order": 2,
      "anno_id": 1,
      "text": "IEEE TRANSACTIONS ON AGRIFOOD ELECTRONICS, VOL. 2, NO. 2, SEPTEMBER/OCTOBER 2024",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1037.5,
            132.5,
            2315,
            132.5,
            2315,
            167.5,
            1037.5,
            167.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "figure",
      "poly": [
        280,
        272.5,
        2210,
        272.5,
        2210,
        970,
        280,
        970
      ],
      "ignore": false,
      "order": 3,
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            280,
            272.5,
            2210,
            272.5,
            2210,
            970,
            280,
            970
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "figure_caption",
      "poly": [
        172.5,
        995,
        2315,
        995,
        2315,
        1080,
        172.5,
        1080
      ],
      "ignore": false,
      "order": 4,
      "anno_id": 1,
      "text": "Fig. 2. Step for extracting the MFCC from the chunk audio file, and then compressed into a single dimension vector computing the average value. Finally the machine learning model will use them to predict the presence of the queen bee.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            172.5,
            995,
            2315,
            995,
            2315,
            1080,
            172.5,
            1080
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "table_caption",
      "poly": [
        187.5,
        1162.5,
        1212.5,
        1162.5,
        1212.5,
        1272.5,
        187.5,
        1272.5
      ],
      "ignore": false,
      "order": 5,
      "anno_id": 1,
      "text": "TABLE I  DATASET AUGMENTED SIZES WITH HOP SIZE OF 5 S AND DIFFERENT CHUNK I.engths",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            187.5,
            1162.5,
            1212.5,
            1162.5,
            1212.5,
            1272.5,
            187.5,
            1272.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "table",
      "poly": [
        292.50000000000006,
        1320,
        1115,
        1320,
        1115,
        1515,
        292.50000000000006,
        1515
      ],
      "ignore": false,
      "order": 6,
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            292.50000000000006,
            1320,
            1115,
            1320,
            1115,
            1515,
            292.50000000000006,
            1515
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": "",
        "table_layout": "",
        "language": ""
      },
      "html": "| Chunk size | Dataset 1 [4] | Dataset 2 [24] | total   |\n|------------|---------------|----------------|---------|\n| 0.5 s      | 85 200        | 67 524         | 152 724 |\n| 1 s        | 85 200        | 67 497         | 152 697 |\n| 3 s        | 85 200        | 67 342         | 152 542 |\n| 5 s        | 78 100        | 67 049         | 145 149 |",
      "latex": ""
    },
    {
      "category_type": "plain_text",
      "poly": [
        172.5,
        1600,
        1222.5,
        1600,
        1222.5,
        1790,
        172.5,
        1790
      ],
      "ignore": false,
      "order": 7,
      "anno_id": 1,
      "text": " lasting approximately  $10 \\text{ min.}$  In total, there are 576 audio files, half labeled as \"Missing Queen\" and the other half as \"Normal Beehive.\" These recordings represent the activities of the two hives over two distinct days.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            172.5,
            1600,
            1222.5,
            1600,
            1222.5,
            1790,
            172.5,
            1790
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        167.5,
        1930,
        1230,
        1930,
        1230,
        2822.5,
        167.5,
        2822.5
      ],
      "ignore": false,
      "order": "9",
      "anno_id": 1,
      "text": " Despite utilizing two datasets with a substantial amount of data, this study incorporates a data augmentation technique to expand the number of input samples. This involves breaking down audio files into smaller segments. Adjusting the hop size, which represents the space between the start of consecutive chunks within the original audio, it is possible to increase the number of chunks. This technique is described in  $[31]$  with the name \"time slicing window\" and can be found in other works [32] with similar names like \"TimeShiftRange\" and \"RandXTranslation.\" We subsequently evaluate the impact on the results by exploring various chunk sizes: 0.5, 1, 3, and 5 s, while maintaining the same number of chunks and avoiding overlap. To attain this objective, a hop size of  $5 \\text{ s}$  was utilized, leading to varying numbers of samples as detailed in Table I. The inconsistency in the count of resulting segments arises from the consideration of only complete chunks, with no employment of padding techniques to ensure uniform length across all audio files.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            167.5,
            1930,
            1230,
            1930,
            1230,
            2822.5,
            167.5,
            2822.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        167.5,
        2955,
        1230,
        2955,
        1230,
        3105,
        167.5,
        3105
      ],
      "ignore": false,
      "order": "11",
      "anno_id": 1,
      "text": " Using raw audio data for sound classification is not convenient due to its high dimensionality, leading to computational challenges and increased processing costs. Raw waveforms may",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            167.5,
            2955,
            1230,
            2955,
            1230,
            3105,
            167.5,
            3105
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1267.5,
        3010,
        2325,
        3010,
        2325,
        3102.5,
        1267.5,
        3102.5
      ],
      "ignore": false,
      "order": "15",
      "anno_id": 1,
      "text": " *Short Time Fourier Transform*: The second method uses only the Fourier transform as explained in  $[34]$ , to obtain a reduced ",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1267.5,
            3010,
            2325,
            3010,
            2325,
            3102.5,
            1267.5,
            3102.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1260,
        1707.5,
        2320,
        1707.5,
        2320,
        2347.5,
        1260,
        2347.5
      ],
      "ignore": false,
      "order": "13",
      "anno_id": 1,
      "text": " Mel-Frequency cepstral coefficients: The first method is a widely-used technique for acoustic-applications [33], in particular, to capture relevant characteristics of the human auditory system by transforming the audio signal into a compact representation with a user-defined number, denoted as  $n$ , of coefficients. Results were compared across varying  $n$  values from 10 to 50. By extracting a relatively small number of coefficients, MFCCs reduce the dimensionality of the feature space while retaining essential information about the audio signal. This is crucial for efficient processing and classification. The MFCC extraction is done on each audio chunk of the dataset, this process is explained by the following sequence of steps necessary to obtain the MFCC coefficients vector.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1260,
            1707.5,
            2320,
            1707.5,
            2320,
            2347.5,
            1260,
            2347.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1267.5,
        1165,
        2325,
        1165,
        2325,
        1707.5,
        1267.5,
        1707.5
      ],
      "ignore": false,
      "order": 12,
      "anno_id": 1,
      "text": " not directly capture relevant features for classification, requiring sophisticated feature extraction techniques. In addition, raw audio is susceptible to environmental noise, making models sensitive and less robust. To address these issues, different preprocessing techniques are commonly employed to enhance model efficiency and performance by providing more compact and informative representations of the audio signals. In this study are compared two types of features: mel-frequency cepstral coefficients (MFCC) and the STFT spectrograms. These preprocessing techniques are used to produce the input vector used by the machine learning model as shown in Fig. 2.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1267.5,
            1165,
            2325,
            1165,
            2325,
            1707.5,
            1267.5,
            1707.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1304.9999999999998,
        2355,
        2325,
        2355,
        2325,
        3000,
        1304.9999999999998,
        3000
      ],
      "ignore": false,
      "order": "14",
      "anno_id": 1,
      "text": " - 1) Division of the audio into windows of 2048 audio samples with partial overlap, using a hop length of 512 samples. - 2) Application of the Hann windowing function to smooth the signal at the window edges. - 3) Use of discrete Fourier transform (DFT) to convert the signal to the frequency domain. - 4) Application of the mel filterbank set of triangular filters, evenly spaced on the Mel scale. - 5) Application of discrete cosine transform to obtain coefficients for each window. - 6) Computation of the mean value for each coefficient across all windows, resulting in the  $n$  features that will be used as input for the classifiers.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1304.9999999999998,
            2355,
            2325,
            2355,
            2325,
            3000,
            1304.9999999999998,
            3000
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        170,
        2885,
        552.5,
        2885,
        552.5,
        2930,
        170,
        2930
      ],
      "ignore": false,
      "order": "10",
      "anno_id": 1,
      "text": " ## C. Feature Extraction",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            170,
            2885,
            552.5,
            2885,
            552.5,
            2930,
            170,
            2930
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        172.49999999999994,
        1862.5,
        542.5,
        1862.5,
        542.5,
        1902.5,
        172.49999999999994,
        1902.5
      ],
      "ignore": false,
      "order": "8",
      "anno_id": 1,
      "text": " ## B. Audio Chunk Split",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            172.49999999999994,
            1862.5,
            542.5,
            1862.5,
            542.5,
            1902.5,
            172.49999999999994,
            1902.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    }
  ],
  "extra": {
    "relation": []
  },
  "page_info": {
    "page_attribute": {},
    "page_no": 922,
    "height": 3300,
    "width": 2475,
    "image_path": "89_3_png.jpg"
  }
}