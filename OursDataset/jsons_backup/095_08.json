{
  "layout_dets": [
    {
      "category_type": "table",
      "poly": [
        350,
        264,
        2202,
        264,
        2202,
        736,
        350,
        736
      ],
      "ignore": false,
      "order": 1,
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            350,
            264,
            2202,
            264,
            2202,
            736,
            350,
            736
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": "",
        "table_layout": "",
        "language": ""
      },
      "html": "\n| Model Type                | Model                | Language     |              |              | Text background |              |              | Text Rotate  |              |              |              |\n|---------------------------|----------------------|--------------|--------------|--------------|-----------------|--------------|--------------|--------------|--------------|--------------|--------------|\n|                           |                      | EN           | ZH           | Mixed        | White           | Single       | Multi        | Normal       | Rotate90     | Rotate270    | Horizontal   |\n| Expert Vision<br>Models   | PaddleOCR [23]       | 0.071        | <b>0.055</b> | <b>0.118</b> | <b>0.060</b>    | <b>0.038</b> | <b>0.085</b> | <b>0.060</b> | <b>0.015</b> | 0.285        | <b>0.021</b> |\n|                           | Tesseract OCR $^{5}$ | 0.179        | 0.553        | 0.553        | 0.453           | 0.463        | 0.394        | 0.448        | 0.369        | 0.979        | 0.982        |\n|                           | Surya $^{6}$         | 0.057        | 0.123        | 0.164        | 0.093           | 0.186        | 0.235        | 0.104        | 0.634        | 0.767        | 0.255        |\n|                           | GOT-OCR [45]         | 0.041        | 0.112        | 0.135        | 0.092           | 0.052        | 0.155        | 0.091        | 0.562        | 0.966        | 0.097        |\n|                           | Mathpix $^{4}$       | 0.033        | 0.240        | 0.261        | 0.185           | 0.121        | 0.166        | 0.180        | 0.038        | <b>0.185</b> | 0.638        |\n| Vision Language<br>Models | Qwen2-VL-72B [44]    | 0.072        | 0.274        | 0.286        | 0.234           | 0.155        | 0.148        | 0.223        | 0.273        | 0.721        | 0.067        |\n|                           | Intern VL2-76B [8]   | 0.074        | 0.155        | 0.242        | 0.113           | 0.352        | 0.269        | 0.132        | 0.610        | 0.907        | 0.595        |\n|                           | GPT4o [2]            | <b>0.020</b> | 0.224        | 0.125        | 0.167           | 0.140        | 0.220        | 0.168        | 0.115        | 0.718        | 0.132        |\n",
      "latex": ""
    },
    {
      "category_type": "table",
      "poly": [
        262,
        890,
        1214,
        890,
        1214,
        1286,
        262,
        1286
      ],
      "ignore": false,
      "order": "3",
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            262,
            890,
            1214,
            890,
            1214,
            1286,
            262,
            1286
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": "",
        "table_layout": "",
        "language": ""
      },
      "html": "\n| Models             | CDM         | ExpRate@CDM | BLEU         | Norm Edit    |\n|--------------------|-------------|-------------|--------------|--------------|\n| GOT-OCR [45]       | 74.1        | 28.0        | 55.07        | 0.290        |\n| Mathpix $4$        | 86.6        | 2.8         | <b>66.56</b> | 0.322        |\n| Pix2Tex $7$        | 73.9        | 39.5        | 46.00        | 0.337        |\n| UniMERNet-B $[40]$ | 85.0        | 60.2        | 60.84        | <b>0.238</b> |\n| GPT4o [2]          | <b>86.8</b> | <b>65.5</b> | 45.17        | 0.282        |\n| InternVL2-76B [8]  | 67.4        | 54.5        | 47.63        | 0.308        |\n| Qwen2-VL-72B [44]  | 83.8        | 55.4        | 53.71        | 0.285        |\n",
      "latex": ""
    },
    {
      "category_type": "table_caption",
      "poly": [
        270,
        770,
        2276,
        770,
        2276,
        812,
        270,
        812
      ],
      "ignore": false,
      "order": "2",
      "anno_id": 1,
      "text": " Table 8. Component-level evaluation on OmniDocBench OCR subset: results grouped by text attributes using the edit distance metric.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            270,
            770,
            2276,
            770,
            2276,
            812,
            270,
            812
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "table_caption",
      "poly": [
        242,
        1330,
        1234,
        1330,
        1234,
        1402,
        242,
        1402
      ],
      "ignore": false,
      "order": 4,
      "anno_id": 1,
      "text": " Table 9. Component-level formula recognition evaluation on OmniDocBench formula subset.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            242,
            1330,
            1234,
            1330,
            1234,
            1402,
            242,
            1402
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        246,
        1464.0000000000002,
        1230,
        1464.0000000000002,
        1230,
        1942.0000000000002,
        246,
        1942.0000000000002
      ],
      "ignore": false,
      "order": 5,
      "anno_id": 1,
      "text": " ever, struggle with high-density documents like newspapers due to limitations in input resolution and token length. In contrast, pipeline tools leverage layout-based segmentation to process components individually, maintaining accuracy in complex layouts. Enhancing VLMs with layout-aware designs and domain-specific fine-tuning offers a promising path forward. OmniDocBench facilitates this by providing detailed annotations for layout, text, formulas, and tables, enabling comprehensive benchmarking and modular tool development for diverse document parsing tasks.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            246,
            1464.0000000000002,
            1230,
            1464.0000000000002,
            1230,
            1942.0000000000002,
            246,
            1942.0000000000002
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        237.99999999999983,
        2076,
        1232,
        2076,
        1232,
        2658,
        237.99999999999983,
        2658
      ],
      "ignore": false,
      "order": "7",
      "anno_id": 1,
      "text": " **Layout Detection Results.** Layout detection is the first step in document parsing using pipeline tools. A robust layout detection algorithm should perform well across a variety of document types. Table 6 presents an evaluation of leading layout detection models. The DocLayout-YOLO method, which is pre-trained on diverse synthetic document data, significantly outperforms other approaches. This superiority is a key factor in MinerU's integration of DocLayout-YOLO, contributing to its outstanding overall performance. Other methods perform well on books and academic literature but struggle with more diverse formats due to limited training data.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            237.99999999999983,
            2076,
            1232,
            2076,
            1232,
            2658,
            237.99999999999983,
            2658
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        238,
        2674,
        1234,
        2674,
        1234,
        2966,
        238,
        2966
      ],
      "ignore": false,
      "order": "8",
      "anno_id": 1,
      "text": " Table Recognition Results. In Table 7, We evaluate table recognition models across three dimensions on our OmniDocBench table subset: language",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            238,
            2674,
            1234,
            2674,
            1234,
            2966,
            238,
            2966
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1324,
        2828,
        2310,
        2828,
        2310,
        2958,
        1324,
        2958
      ],
      "ignore": false,
      "order": "15",
      "anno_id": 1,
      "text": "This project was supported by National Key R&D Program of China (NO.2022ZD0160102) and Shanghai Artificial Intelligence Laboratory.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1324,
            2828,
            2310,
            2828,
            2310,
            2958,
            1324,
            2958
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1320,
        2192,
        2310,
        2192,
        2310,
        2672,
        1320,
        2672
      ],
      "ignore": false,
      "order": "13",
      "anno_id": 1,
      "text": " This paper addresses the lack of diverse and realistic benchmarks in document parsing research by introducing OmniDocBench, a dataset featuring a variety of page types with comprehensive annotations, along with a flexible and reliable evaluation framework. OmniDocBench enables systematic and fair assessments of document parsing methods, providing crucial insights for advancing the field. Its task-specific and attribute-level evaluations facilitate targeted model optimization, promoting more robust and effective parsing solutions.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1320,
            2192,
            2310,
            2192,
            2310,
            2672,
            1320,
            2672
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1316,
        1549.9999999999998,
        2310,
        1549.9999999999998,
        2310,
        2041.9999999999998,
        1316,
        2041.9999999999998
      ],
      "ignore": false,
      "order": "11",
      "anno_id": 1,
      "text": " **Formula Recognition Results.** Table 9 presents results on formula parsing, using CDM, BLEU, and normalized Edit Distance. GPT-40, Mathpix, and UniMERNet achieve results of 86.8%, 86.6%, and 85.0%, respectively. Notably, GPT-40 excels with a recall rate of 65.5% under strict conditions requiring perfect character accuracy. Although Mathpix shows high character-level precision, it occasionally omits punctuation, such as commas, leading to a lower overall correctness rate. Nonetheless, all three models are strong candidates for formula recognition tasks.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1316,
            1549.9999999999998,
            2310,
            1549.9999999999998,
            2310,
            2041.9999999999998,
            1316,
            2041.9999999999998
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1312,
        896,
        2310,
        896,
        2310,
        1278,
        1312,
        1278
      ],
      "ignore": false,
      "order": "9",
      "anno_id": 1,
      "text": "  diversity, table frame types, and special situations. Among all models, OCRbased models demonstrate superior overall performance, with RapidTable achieving the highest scores in language diversity and maintaining stable performance across different frame types. Expert VLMs show competitive results in specific scenarios, with StructEqTable [55] excelling in noframe tables and showing better rotation robustness. General VLMs (Qwen2-VL-7B and InternVL2-8B) exhibit relatively lower but consistent performance, suggesting that while general-purpose VLMs have made progress in table understanding, they still lag behind specialized solutions.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1312,
            896,
            2310,
            896,
            2310,
            1278,
            1312,
            1278
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1320,
        1292,
        2312,
        1292,
        2312,
        1534,
        1320,
        1534
      ],
      "ignore": false,
      "order": "10",
      "anno_id": 1,
      "text": " Text Recognition Results. Table 8 compares OCR tools across languages, backgrounds, and rotations using Edit Distance. PaddleOCR outperforms all competitors, followed by GOT-OCR and Mathpix. General VLMs struggle to handle text rotation or mixed-language scenarios.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1320,
            1292,
            2312,
            1292,
            2312,
            1534,
            1320,
            1534
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        1324,
        2098,
        1604,
        2098,
        1604,
        2144,
        1324,
        2144
      ],
      "ignore": false,
      "order": "12",
      "anno_id": 1,
      "text": " # 6. Conclusion",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1324,
            2098,
            1604,
            2098,
            1604,
            2144,
            1324,
            2144
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        244,
        1996.0000000000002,
        930,
        1996.0000000000002,
        930,
        2040.0000000000002,
        244,
        2040.0000000000002
      ],
      "ignore": false,
      "order": "6",
      "anno_id": 1,
      "text": "## **5.2. Single Task Evaluation Results**",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            244,
            1996.0000000000002,
            930,
            1996.0000000000002,
            930,
            2040.0000000000002,
            244,
            2040.0000000000002
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        1320,
        2740,
        1758,
        2740,
        1758,
        2780,
        1320,
        2780
      ],
      "ignore": false,
      "order": "14",
      "anno_id": 1,
      "text": "# 7.Acknowledgments",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1320,
            2740,
            1758,
            2740,
            1758,
            2780,
            1320,
            2780
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "footer",
      "poly": [
        1224,
        3112,
        1324,
        3112,
        1324,
        3160,
        1224,
        3160
      ],
      "ignore": false,
      "order": 16,
      "anno_id": 1,
      "text": "24845",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1224,
            3112,
            1324,
            3112,
            1324,
            3160,
            1224,
            3160
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    }
  ],
  "extra": {
    "relation": []
  },
  "page_info": {
    "page_attribute": {},
    "page_no": 989,
    "height": 3300,
    "width": 2550,
    "image_path": "95_8_png.jpg"
  }
}