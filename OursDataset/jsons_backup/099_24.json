{
  "layout_dets": [
    {
      "category_type": "header",
      "poly": [
        448,
        118,
        1224,
        118,
        1224,
        154,
        448,
        154
      ],
      "ignore": false,
      "order": 1,
      "anno_id": 1,
      "text": " Published as a conference paper at ICLR 2024",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            448,
            118,
            1224,
            118,
            1224,
            154,
            448,
            154
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "table",
      "poly": [
        454,
        410.0049999999999,
        2114,
        410.0049999999999,
        2114,
        2744.0550000000003,
        454,
        2744.0550000000003
      ],
      "ignore": false,
      "order": 2,
      "anno_id": 1,
      "text": "",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            454,
            410.0049999999999,
            2114,
            410.0049999999999,
            2114,
            2744.0550000000003,
            454,
            2744.0550000000003
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": "",
        "table_layout": "",
        "language": ""
      },
      "html": "\n| Model                          | Auto-J Rating | GPT-4 Win-rate | Rank GPT-4 | Rank Auto-J | Î”  |\n|--------------------------------|---------------|----------------|------------|-------------|----|\n| XwinLM 70b V0.1                | 5.694         | 95.57          | 1          | 1           | 0  |\n| LLaMA2 Chat 70B                | 5.678         | 92.66          | 2          | 2           | 0  |\n| XwinLM 13b V0.1                | 5.647         | 91.76          | 3          | 3           | 0  |\n| OpenChat V3.1 13B              | 5.532         | 89.49          | 4          | 8           | 4  |\n| WizardLM 13B V1.2              | 5.547         | 89.17          | 5          | 6           | 1  |\n| Vicuna 33B v1.3                | 5.570         | 88.99          | 6          | 5           | -1 |\n| Humpback LLaMa2 70B            | 5.498         | 87.94          | 7          | 11          | 4  |\n| XwinLM 7b V0.1                 | 5.584         | 87.83          | 8          | 4           | -4 |\n| OpenBudddy-LLaMA2-70B-v10.1    | 5.448         | 87.67          | 9          | 14          | 5  |\n| OpenChat V2-W 13B              | 5.533         | 87.13          | 10         | 7           | -3 |\n| OpenBuddy-LLaMA-65B-v8         | 5.458         | 86.53          | 11         | 13          | 2  |\n| WizardLM 13B V1.1              | 5.497         | 86.32          | 12         | 12          | 0  |\n| OpenChat V2 13B                | 5.519         | 84.97          | 13         | 9           | -4 |\n| Humpback LLaMa 65B             | 5.379         | 83.71          | 14         | 19          | 5  |\n| Vicuna 13B v1.3                | 5.388         | 82.11          | 15         | 18          | 3  |\n| OpenBuddy-LLaMA-30B-v7.1       | 5.391         | 81.55          | 16         | 17          | 1  |\n| LLaMA2 Chat 13B                | 5.518         | 81.09          | 17         | 10          | -7 |\n| OpenChat-13B                   | 5.437         | 80.87          | 18         | 15          | -3 |\n| OpenBuddy-Falcon-40B-v9        | 5.373         | 80.70          | 19         | 20          | 1  |\n| UltraLM 13B                    | 5.342         | 80.64          | 20         | 22          | 2  |\n| OpenChat8192-13B               | 5.429         | 79.54          | 21         | 16          | -5 |\n| OpenCoderPlus-15B              | 5.357         | 78.70          | 22         | 21          | -1 |\n| OpenBudddy-LLaMA2-13B-v11.1    | 5.340         | 77.49          | 23         | 23          | 0  |\n| Vicuna 7B v1.3                 | 5.332         | 76.84          | 24         | 25          | 1  |\n| WizardLM 13B                   | 5.247         | 75.31          | 25         | 32          | 7  |\n| JinaChat                       | 5.319         | 74.13          | 26         | 26          | 0  |\n| airoboros 65B                  | 5.318         | 73.91          | 27         | 27          | 0  |\n| airoboros 33B                  | 5.289         | 73.29          | 28         | 30          | 2  |\n| Guanaco 65B                    | 5.313         | 71.80          | 29         | 29          | 0  |\n| LLaMA2 Chat 7B                 | 5.334         | 71.37          | 30         | 24          | -6 |\n| Vicuna 13B                     | 5.314         | 70.43          | 31         | 28          | -3 |\n| OpenBuddy-Falcon-7b-v6         | 5.214         | 70.36          | 32         | 34          | 2  |\n| Baize-v2 13B                   | 5.165         | 66.96          | 33         | 38          | 5  |\n| LLAMA 33B OASST RLHF           | 5.173         | 66.52          | 34         | 37          | 3  |\n| Minotaur 13B                   | 5.210         | 66.02          | 35         | 36          | 1  |\n| Guanaco 33B                    | 5.212         | 65.96          | 36         | 35          | -1 |\n| Nous Hermes 13B                | 5.271         | 65.47          | 37         | 31          | -6 |\n| Vicuna 7B                      | 5.237         | 64.41          | 38         | 33          | -5 |\n| Baize-v2 7B                    | 5.083         | 63.85          | 39         | 39          | 0  |\n| LLAMA 33B OASST SFT            | 4.985         | 54.97          | 40         | 41          | 1  |\n| Guanaco 13B                    | 5.027         | 52.61          | 41         | 40          | -1 |\n| ChatGLM2-6B                    | 4.846         | 47.13          | 42         | 46          | 4  |\n| Guanaco 7B                     | 4.943         | 46.58          | 43         | 43          | 0  |\n| Falcon 40B Instruct            | 4.934         | 45.71          | 44         | 44          | 0  |\n| Alpaca Farm PPO Sim (GPT-4) 7B | 4.978         | 44.10          | 45         | 42          | -3 |\n| Pythia 12B SFT                 | 4.809         | 41.86          | 46         | 47          | 1  |\n| Alpaca Farm PPO Human 7B       | 4.907         | 41.24          | 47         | 45          | -2 |\n| Cohere Chat                    | 4.524         | 29.57          | 48         | 51          | 3  |\n| Cohere                         | 4.522         | 28.39          | 49         | 52          | 3  |\n| Alpaca 7B                      | 4.658         | 26.46          | 50         | 48          | -2 |\n| Pythia 12B OASST SFT           | 4.620         | 25.96          | 51         | 49          | -2 |\n| Falcon 7B Instruct             | 4.537         | 23.60          | 52         | 50          | -2 |\n| Baichuan-13B-Chat              | 4.291         | 21.80          | 53         | 53          | 0  |\n",
      "latex": ""
    },
    {
      "category_type": "table_caption",
      "poly": [
        447.9999999999999,
        2788,
        2105.9999999999995,
        2788,
        2105.9999999999995,
        2964,
        447.9999999999999,
        2964
      ],
      "ignore": false,
      "order": 3,
      "anno_id": 1,
      "text": "Table 24: Values and ranking by Auto-J and GPT-4 for open-source LLMs on AlpacaEval. Value of AUTO-J is the model's average rating on AlpacaEval dataset assigned by AUTO-J in single-response evaluation protocol, and value of GPT-4 is the model's win-rate against Davinci003 determined by GPT-4 on AlpacaEval dataset.  $\\Delta = \\text{Rank}_{\\text{Auto-J}} - \\text{Rank}_{\\text{GPT-4}}$ ",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            447.9999999999999,
            2788,
            2105.9999999999995,
            2788,
            2105.9999999999995,
            2964,
            447.9999999999999,
            2964
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "footer",
      "poly": [
        1246,
        3132,
        1296,
        3132,
        1296,
        3166,
        1246,
        3166
      ],
      "ignore": false,
      "order": 4,
      "anno_id": 1,
      "text": "24",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1246,
            3132,
            1296,
            3132,
            1296,
            3166,
            1246,
            3166
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    }
  ],
  "extra": {
    "relation": []
  },
  "page_info": {
    "page_attribute": {},
    "page_no": 1045,
    "height": 3300,
    "width": 2550,
    "image_path": "99_24_png.jpg"
  }
}