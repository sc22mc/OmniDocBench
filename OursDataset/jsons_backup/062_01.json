{
  "layout_dets": [
    {
      "category_type": "header",
      "poly": [
        195,
        97.5,
        1060,
        97.5,
        1060,
        145,
        195,
        145
      ],
      "ignore": false,
      "order": "2",
      "anno_id": 1,
      "text": "JOURNAL OF LATEX CLASS FILES,vol. 14, NO.8, AUGUST 2022",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            195,
            97.5,
            1060,
            97.5,
            1060,
            145,
            195,
            145
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "header",
      "poly": [
        2312.5,
        97.49999999999999,
        2357.5,
        97.49999999999999,
        2357.5,
        127.49999999999999,
        2312.5,
        127.49999999999999
      ],
      "ignore": false,
      "order": "3",
      "anno_id": 1,
      "text": "1",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            2312.5,
            97.49999999999999,
            2357.5,
            97.49999999999999,
            2357.5,
            127.49999999999999,
            2312.5,
            127.49999999999999
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "title",
      "poly": [
        220,
        240,
        2330,
        240,
        2330,
        450,
        220,
        450
      ],
      "ignore": false,
      "order": "4",
      "anno_id": 1,
      "text": " # Cooperative Saliency-based Obstacle Detection and AR Rendering for Increased Situational Awareness",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            220,
            240,
            2330,
            240,
            2330,
            450,
            220,
            450
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        670,
        487.5,
        1882.5,
        487.5,
        1882.5,
        597.5,
        670,
        597.5
      ],
      "ignore": false,
      "order": "5",
      "anno_id": 1,
      "text": " Gerasimos Arvanitis, Nikolaos Stagakis, Evangelia I. Zacharaki,   and Konstantinos Moustakas, Senior Member, IEEE",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            670,
            487.5,
            1882.5,
            487.5,
            1882.5,
            597.5,
            670,
            597.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "arxiv",
      "poly": [
        67.5,
        895,
        152.5,
        895,
        152.5,
        2312.5,
        67.5,
        2312.5
      ],
      "ignore": false,
      "order": "1",
      "anno_id": 1,
      "text": "arXiv:2302.00916v1 [cs.CV] 2 Feb 2023",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            67.5,
            895,
            152.5,
            895,
            152.5,
            2312.5,
            67.5,
            2312.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        200,
        782.5,
        1260,
        782.5,
        1260,
        1652.5,
        200,
        1652.5
      ],
      "ignore": false,
      "order": 6,
      "anno_id": 1,
      "text": "Abstract—Autonomous vehicles are expected to operate safely in real-life road conditions in the next years. Nevertheless, unanticipated events such as the existence of unexpected objects in the range of the road, can put safety at risk. The advancement of sensing and communication technologies and Internet of Things may facilitate the recognition of hazardous situations and information exchange in a cooperative driving scheme, providing new opportunities for the increase of collaborative situational awareness. Safe and unobtrusive visualization of the obtained information may nowadays be enabled through the adoption of novel Augmented Reality (AR) interfaces in the form of windshields. Motivated by these technological opportunities, we propose in this work a saliency-based distributed, cooperative obstacle detection and rendering scheme for increasing the driver's situational awareness through (i) automated obstacle detection, (ii) AR visualization and (iii) information sharing (upcoming potential dangers) with other connected vehicles or road infrastructure. An extensive evaluation study using a variety of real datasets for pothole detection showed that the proposed method provides favorable results and features compared to other recent and relevant approaches.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            200,
            782.5,
            1260,
            782.5,
            1260,
            1652.5,
            200,
            1652.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        197.5,
        1674.9999999999998,
        1260,
        1674.9999999999998,
        1260,
        1802.4999999999998,
        197.5,
        1802.4999999999998
      ],
      "ignore": false,
      "order": 7,
      "anno_id": 1,
      "text": " Index Terms—pothole detection, collaborative awareness, point cloud processing, augmented reality, CARLA, visualization, driver's safety",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            197.5,
            1674.9999999999998,
            1260,
            1674.9999999999998,
            1260,
            1802.4999999999998,
            197.5,
            1802.4999999999998
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        560,
        1875,
        897.5,
        1875,
        897.5,
        1917.5,
        560,
        1917.5
      ],
      "ignore": false,
      "order": 8,
      "anno_id": 1,
      "text": " # I. INTRODUCTION",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            560,
            1875,
            897.5,
            1875,
            897.5,
            1917.5,
            560,
            1917.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        202.5,
        1932.5,
        1257.5,
        1932.5,
        1257.5,
        2422.5,
        202.5,
        2422.5
      ],
      "ignore": false,
      "order": 9,
      "anno_id": 1,
      "text": " INformation-centric technologies have started to play a central role in the recent automotive industry boosting new research trends in semi or fully Automated Driving Systems (ADS). Autonomous vehicles, ranging from level 3 to level 5 of autonomy [1], are expected to operate safely in real-life road conditions, but the reality is that obstacles like potholes, bumps, and other unexpected objects are not uncommon in an everyday driving context. For this reason, the detection and identification of obstacles are imperative for reliable operation of autonomous vehicles [2].",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            202.5,
            1932.5,
            1257.5,
            1932.5,
            1257.5,
            2422.5,
            202.5,
            2422.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        202.5,
        2440,
        1250,
        2440,
        1250,
        2927.5,
        202.5,
        2927.5
      ],
      "ignore": false,
      "order": 10,
      "anno_id": 1,
      "text": " Moreover, driver inattentiveness plays a major role in driving safety and is the culprit of road accidents around the world [3], [4], thus a lot of work has been devoted in the quantification of the abstract mechanics of human situational awareness [5]. Enhancing situational awareness is especially critical in the case of semi-autonomous cars, where the operator may be distracted by secondary activities, e.g. looking at the phone or reading a book. If the driver has to take over control, it is important to minimize the required reaction time. This can be achieved by monitoring",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            202.5,
            2440,
            1250,
            2440,
            1250,
            2927.5,
            202.5,
            2927.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        197.5,
        2962.5,
        1252.5,
        2962.5,
        1252.5,
        3117.5,
        197.5,
        3117.5
      ],
      "ignore": false,
      "order": 11,
      "anno_id": 1,
      "text": " G. Arvanitis, N. Stagakis, E. I. Zacharaki and K. Moustakas are with the Department of Electrical and Computer Engineering, University of Patras, Greece (e-mail: arvanitis@ece.upatras.gr, nick.stag@ece.upatras.gr, ezachar@upatras.gr, moustakas@ece.upatras.gr)",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            197.5,
            2962.5,
            1252.5,
            2962.5,
            1252.5,
            3117.5,
            197.5,
            3117.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1295,
        2925,
        2357.5,
        2925,
        2357.5,
        3117.5,
        1295,
        3117.5
      ],
      "ignore": false,
      "order": "15",
      "anno_id": 1,
      "text": " To avoid any information visualization clutter, we propose the use of AR for visualizing critical information in the driver's field of view. AR rendering is based on classical perspective projection, where for each point (of the point cloud) the",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1295,
            2925,
            2357.5,
            2925,
            2357.5,
            3117.5,
            1295,
            3117.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1285,
        1925,
        2357.5,
        1925,
        2357.5,
        2915,
        1285,
        2915
      ],
      "ignore": false,
      "order": "14",
      "anno_id": 1,
      "text": " The purpose of this work is to increase the driver's situational awareness through automated cooperative obstacle detection, visualization and information sharing with other connected vehicles in a  $V2X$  (vehicle-to-everything) setting. To address the above issues, we developed a point cloud processing system that takes as input road environment data and classifies them into safe and potentially hazardous regions by identifying obstacles lying in the range of the road. We selected LiDAR as sensing modality for the surrounding environment due to its ability to retrieve depth information and its large range, making it suitable for driving environments. For more robust estimation, LiDAR data are fused with information on driving patterns, such as the steering angle of the wheels. For implementation and evaluation, we utilized the open-source CARLA simulator [16] including also a multi-agent system of vehicles, and we augmented it with our obstacle detection and tracking component. In this simulated environment, information sharing between agents is enabled, so that vehicles are notified about incoming obstacles even when there is no direct line-of-sight.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1285,
            1925,
            2357.5,
            1925,
            2357.5,
            2915,
            1285,
            2915
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1287.5,
        1077.5,
        2352.5,
        1077.5,
        2352.5,
        1927.5,
        1287.5,
        1927.5
      ],
      "ignore": false,
      "order": "13",
      "anno_id": 1,
      "text": " The problem of road pothole detection is commonly targeted using imaging (camera) data and computer vision techniques [7], [8], [9]. Although image-based techniques have achieved great success, one common drawback is that they are sensitive to motion blur and changes in lighting and/or even shadows [10]. Also, most techniques do not account for other passing vehicles [11]. This can make them unreliable in real use cases, which is a major weakness in problems involving human safety. In light of all this, the use of a 3D LiDAR (Light Detection and Ranging) sensor could provide more robust sensing capabilities for the analysis of potholes, in the same way that it is used to increase the accuracy of road's boundary detection  $[12]$ ,  $[13]$ ,  $[14]$ . On the other hand, a limitation of the LiDAR sensor is that, due to refraction and reflection, water appears as a black hole in the imagery calculated from LiDAR data [15], imposing additional challenges in the detection of potholes filled with water.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1287.5,
            1077.5,
            2352.5,
            1077.5,
            2352.5,
            1927.5,
            1287.5,
            1927.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        1292.5,
        780,
        2352.5,
        780,
        2352.5,
        1067.5,
        1292.5,
        1067.5
      ],
      "ignore": false,
      "order": "12",
      "anno_id": 1,
      "text": " and presenting to the driver the crucial information about the environment, thus keeping him/her aware of potentially hazardous situations. Inherent challenges include the need for unobtrusive information display, avoiding the effects of tunnel vision which could lead to actually overlooking critical information  $[6]$ .",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1292.5,
            780,
            2352.5,
            780,
            2352.5,
            1067.5,
            1292.5,
            1067.5
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    }
  ],
  "extra": {
    "relation": []
  },
  "page_info": {
    "page_attribute": {},
    "page_no": 627,
    "height": 3300,
    "width": 2550,
    "image_path": "62_1_png.jpg"
  }
}