{
  "layout_dets": [
    {
      "category_type": "header",
      "poly": [
        452,
        110,
        1228,
        110,
        1228,
        152,
        452,
        152
      ],
      "ignore": false,
      "order": 1,
      "anno_id": 1,
      "text": " Published as a conference paper at ICLR 2024",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            452,
            110,
            1228,
            110,
            1228,
            152,
            452,
            152
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        440,
        360,
        2096,
        360,
        2096,
        442,
        440,
        442
      ],
      "ignore": false,
      "order": 2,
      "anno_id": 1,
      "text": "where  $\\boldsymbol{o} = (w_{L+1}, \\dots, w_{L+N})$  denotes the continuation of context c. This objective helps improve generalization and circumvent excessive reliance on, and overfitting to, the autoencoding task.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            440,
            360,
            2096,
            360,
            2096,
            442,
            440,
            442
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        446.0000000000001,
        566,
        2104,
        566,
        2104,
        778,
        446.0000000000001,
        778
      ],
      "ignore": false,
      "order": "4",
      "anno_id": 1,
      "text": "After pretraining, the memory slots produced by the pretrained ICAE are expected to represent the original context. However, for LLMs, the purpose of providing a context extends beyond rote memorization or continuation; instead, the more common use scenario is using the provided context as a basis for accurately and appropriately responding to various prompts, ultimately accomplishing the tasks we want it to perform (Wei et al., 2021; Ouyang et al., 2022).",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            446.0000000000001,
            566,
            2104,
            566,
            2104,
            778,
            446.0000000000001,
            778
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        444,
        816,
        2100,
        816,
        2100,
        946,
        444,
        946
      ],
      "ignore": false,
      "order": "5",
      "anno_id": 1,
      "text": "To enhance the interaction of memory slots produced by the ICAE with diverse prompts, we further fine-tune the ICAE with the PWC dataset (**Prompt-with-Context**), a dataset<sup> $1$ </sup> introduced in this paper consisting of thousands of (context, prompt, response) samples (as shown in Figure 1).",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            444,
            816,
            2100,
            816,
            2100,
            946,
            444,
            946
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        444,
        978,
        2106,
        978,
        2106,
        1110,
        444,
        1110
      ],
      "ignore": false,
      "order": "6",
      "anno_id": 1,
      "text": "Formally, the ICAE is fine-tuned for learning to encode the context into the memory slots based on which the decoder (i.e., the target LLM) can produce a desirable response  $r_1 \\dots r_n$  according to a given prompt  $p_1 \\dots p_m$ , as shown in Figure 8 in Appendix A:",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            444,
            978,
            2106,
            978,
            2106,
            1110,
            444,
            1110
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        451.9999999999999,
        1468,
        2106,
        1468,
        2106,
        1682,
        451.9999999999999,
        1682
      ],
      "ignore": false,
      "order": "10",
      "anno_id": 1,
      "text": "**Data** We pretrain the ICAE with the Pile (Gao et al., 2020). For instruction fine-tuning, we use the PWC dataset, as introduced in Section 2.3, which contains 240k (context, prompt, response) samples for training and 18k samples for testing. The context length distribution of test samples is shown in Figure 10. By default, the maximal token length (excluding memory slots) we set during training is 512 in both the ICAE's encoder and decoder in our experiments.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            451.9999999999999,
            1468,
            2106,
            1468,
            2106,
            1682,
            451.9999999999999,
            1682
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        454.0000000000001,
        1752,
        2098,
        1752,
        2098,
        1966,
        454.0000000000001,
        1966
      ],
      "ignore": false,
      "order": "11",
      "anno_id": 1,
      "text": "**Model Configuration** We use the LlaMa (Touvron et al., 2023a;b) as the target LLM to test the ICAE's performance in context compression. For the encoder of the ICAE, LoRA is applied to the query and value projections of the LLM's multi-head attention. In our default setting, the memory slot length k is set to 128, and the LoRA rank r is set to 128 unless otherwise specified. The resulting ICAE only adds about 1% learnable parameters on top of the target LLM.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            454.0000000000001,
            1752,
            2098,
            1752,
            2098,
            1966,
            454.0000000000001,
            1966
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        448,
        2324,
        2098,
        2324,
        2098,
        2678,
        448,
        2678
      ],
      "ignore": false,
      "order": "15",
      "anno_id": 1,
      "text": "Figure 4 presents the autoencoding results of the ICAE based on the Llama-7b. The ICAE demonstrates a very low overall loss, below 0.05, indicating that the produced memory slots retain almost all the information of the original context. When the context length is within 300, the ICAE can almost perfectly reconstruct the original context, achieving nearly 100% BLEU and EM scores. As the context length increases beyond 400, both BLEU and EM scores start to decline, indicating insufficient capacity of the 128-length memory slots. However, even at a context length of 500, the median BLEU remains over 0.98, and the median EM approaches 0.6 (e.g., perfectly reconstructing about the first 300 words of a 512-token context), showing remarkable performance of ICAE.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            448,
            2324,
            2098,
            2324,
            2098,
            2678,
            448,
            2678
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        438,
        2716,
        2094,
        2716,
        2094,
        2796,
        438,
        2796
      ],
      "ignore": false,
      "order": "16",
      "anno_id": 1,
      "text": "We then analyze the effect of the memory size  $k$  on the result. According to Figure 5, as the memory slot length  $k$  decreases, the ICAE's ability to memorize longer samples significantly deteriorates.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            438,
            2716,
            2094,
            2716,
            2094,
            2796,
            438,
            2796
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "plain_text",
      "poly": [
        444,
        2164,
        2106,
        2164,
        2106,
        2290,
        444,
        2290
      ],
      "ignore": false,
      "order": "14",
      "anno_id": 1,
      "text": "We first evaluate the autoencoding performance of the pretrained ICAE (without instruction finetuning) using the following three metrics to understand how well it restores the original context from its produced memory slots: BLEU (Papineni et al., 2002), Exact-Match  $(EM)^2$  and cross entropy loss.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            444,
            2164,
            2106,
            2164,
            2106,
            2290,
            444,
            2290
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        444,
        2028,
        706,
        2028,
        706,
        2072,
        444,
        2072
      ],
      "ignore": false,
      "order": "12",
      "anno_id": 1,
      "text": "## 3.2 RESULTS",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            444,
            2028,
            706,
            2028,
            706,
            2072,
            444,
            2072
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        450,
        2092,
        928,
        2092,
        928,
        2130,
        450,
        2130
      ],
      "ignore": false,
      "order": "13",
      "anno_id": 1,
      "text": "### 3.2.1 PRETRAINED ICAE",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            450,
            2092,
            928,
            2092,
            928,
            2130,
            450,
            2130
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        447.9999999999999,
        1308,
        833.9999999999999,
        1308,
        833.9999999999999,
        1354,
        447.9999999999999,
        1354
      ],
      "ignore": false,
      "order": "8",
      "anno_id": 1,
      "text": "# 3 EXPERIMENTS",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            447.9999999999999,
            1308,
            833.9999999999999,
            1308,
            833.9999999999999,
            1354,
            447.9999999999999,
            1354
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        448,
        1386,
        996,
        1386,
        996,
        1434,
        448,
        1434
      ],
      "ignore": false,
      "order": "9",
      "anno_id": 1,
      "text": "## 3.1 EXPERIMENTAL SETTING",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            448,
            1386,
            996,
            1386,
            996,
            1434,
            448,
            1434
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "isolate_formula",
      "poly": [
        720,
        1132,
        1830,
        1132,
        1830,
        1292,
        720,
        1292
      ],
      "ignore": false,
      "order": "7",
      "anno_id": 1,
      "text": "$$ \\mathcal{L}_{FT} = \\max_{\\widetilde{m}_{1} ... \\widetilde{m}_{k}} P(r_{1} ... r_{n} | \\widetilde{m}_{1} ... \\widetilde{m}_{k}, p_{1} ... p_{m}; \\Theta_{LLM}) $$  $$ = \\max_{\\Theta_{LoRA}, e_{m}} P(r_{1} ... r_{n} | m_{1} ... m_{k}, p_{1} ... p_{m}; \\Theta_{LLM}, \\Theta_{LoRA}, e_{m}) $$",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            720,
            1132,
            1830,
            1132,
            1830,
            1292,
            720,
            1292
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "section",
      "poly": [
        446.0000000000001,
        486,
        1048,
        486,
        1048,
        532,
        446.0000000000001,
        532
      ],
      "ignore": false,
      "order": "3",
      "anno_id": 1,
      "text": "## 2.3 Instruction Fine-Tuning",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            446.0000000000001,
            486,
            1048,
            486,
            1048,
            532,
            446.0000000000001,
            532
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "footer",
      "poly": [
        1258,
        3132,
        1290,
        3132,
        1290,
        3170,
        1258,
        3170
      ],
      "ignore": false,
      "order": "19",
      "anno_id": 1,
      "text": "4",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            1258,
            3132,
            1290,
            3132,
            1290,
            3170,
            1258,
            3170
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "footer",
      "poly": [
        448,
        2830,
        2104,
        2830,
        2104,
        2960,
        448,
        2960
      ],
      "ignore": false,
      "order": "17",
      "anno_id": 1,
      "text": "<sup>1</sup>Despite some (prompt, response) datasets such as Self-Instruct (Wang et al., 2022), most of their samples either have no context or very short contexts, which are not suitable for evaluation in our setting. Therefore, we establish the PwC dataset with the help of the GPT-4 (OpenAI, 2023). We include the details in Appendix C.",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            448,
            2830,
            2104,
            2830,
            2104,
            2960,
            448,
            2960
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    },
    {
      "category_type": "footer",
      "poly": [
        444,
        2962,
        2108,
        2962,
        2108,
        3048,
        444,
        3048
      ],
      "ignore": false,
      "order": "18",
      "anno_id": 1,
      "text": "$^{2}$ EM denotes the proportion of the exact matching prefix length to the total length. For a context of 512 tokens, if its first 256 tokens are perfectly restored but its 257th token is not, the EM score is  $256/512 = 0.5$ .",
      "line_with_spans": [
        {
          "category_type": "text_span",
          "poly": [
            444,
            2962,
            2108,
            2962,
            2108,
            3048,
            444,
            3048
          ],
          "text": ""
        }
      ],
      "attribute": {
        "text_language": "",
        "text_background": "",
        "text_rotate": ""
      }
    }
  ],
  "extra": {
    "relation": []
  },
  "page_info": {
    "page_attribute": {},
    "page_no": 1024,
    "height": 3300,
    "width": 2550,
    "image_path": "98_4_png.jpg"
  }
}