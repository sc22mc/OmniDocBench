arXiv:2302.01073v1 [cs.GT] 2 Feb 2023

# Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium

Yuma Fujimoto

Research Center for Integrative Evolutionary Science, SOKENDAI (The Graduate University for Advanced Studies). Universal Biology Institute (UBI), the University of Tokyo. CyberAgent, Inc.

fujimoto\_yuma@soken.ac.jp

Kaito Ariu CyberAgent, Inc. /  $KTH$ kaito\_ariu@cyberagent.co.jp

Kenshi Abe CyberAgent, Inc. abe\_kenshi@cyberagent.co.jp

Abstract

Repeated games consider a situation where multiple agents are motivated by their independent rewards throughout learning. In general, the dynamics of their learning become complex. Especially when their rewards compete with each other like zero-sum games, the dynamics often do not converge to their optimum, i.e., Nash equilibrium. To tackle such complexity, many studies have understood various learning algorithms as dynamical systems and discovered qualitative insights among the algorithms. However, such studies have yet to handle multi-memory games (where agents can memorize actions they played in the past and choose their actions based on their memories), even though memorization plays a pivotal role in artificial intelligence and interpersonal relationship. This study extends two major learning algorithms in games, i.e., replicator dynamics and gradient ascent, into multi-memory games. Then, we prove their dynamics are identical. Furthermore, theoretically and experimentally, we clarify that the learning dynamics diverge from the Nash equilibrium in multi-memory zero-sum games and reach heteroclinic cycles (sojourn longer around the boundary of the strategy space), providing a fundamental advance in learning in games.

# 1 Introduction

Repeated game models that multiple agents aim to optimize their objective functions based on a normal-form game [1]. It is known that in this game, the set of optimal strategies for all the agents always exists as Nash equilibria [2]. Various algorithms with which each agent achieves its optimal strategy have been proposed, such as Cross learning [3], replicator dynamics [4, 5], gradient ascent [6, 7, 8, 9], Q-learning [10, 11, 12], and so on. In zero-sum games where two agents have conflicts in their benefits, however, the above learning algorithms cannot converge to their equilibrium [13, 14]. Indeed, the dynamics of learning draw a loop around the equilibrium point, even though the shape of the trajectory differs more or less depending on the algorithm. Thus, solving the dynamics around the Nash equilibrium is a touchstone for discussing whether the learning works well.

Currently, several studies attempt to understand trajectories of multi-agent learning by integrating various cross-disciplinary algorithms  $[15, 16, 17, 18]$ . For example, if we take an infinitesimal step size of learning, Cross learning draws the same trajectory as a replicator dynamics. The replicator dynamics can be interpreted as the weighted version of infinitesimal gradient ascent. Furthermore, Q-learning differs only in the extra term of exploration with the replicator dynamics. Another study has shown a relationship between the replicator dynamics and Q-learning by introducing a generalized regularizer which pulls the strategy back to the probabilistic simplex at the shortest distance [13]. Like these studies, it is important to understand the trajectory of multi-agent learning theoretically.

1

Repeated games potentially include memories of agents, i.e., a possibility that agents determine their actions depending on past actions they chose (see Fig. 1 for the illustration). Such memories can expand the choice of strategies and thus lead to the agents handling their gameplay better; for example, by reading how the other player chooses its action [19]. Indeed, agents with memories can use tit-for-tat [20] and winstay-lose-shift [21] strategies in prisoner's dilemma games, and these strategies achieve cooperation as Nash equilibrium, explaining human behaviors. Furthermore, how a region of the Nash equilibrium is extended by multi-memory strategies is enthusiastically studied as folk theorem [22]. In practice, Q-learning is frequently implemented in multi-memory games [23, 24, 25]. Several studies [26, 27] partly discuss the relation between the replicator dynamics and the gradient ascent but consider only prisoner's dilemma games. In conclusion, this relation is still unclear in games with general numbers of memories and actions. Furthermore, the convergence of dynamics in such multi-memory games has been unexplored.

![](_page_1_Figure_1.jpeg)

Figure 1: A. Illustration of a multi-memory repeated game. Focusing on the area surrounded by the purple dots, a normal-form game is illustrated. Player X (resp. Y) chooses its action  $a_1$  or  $a_2$  in the row (resp.  $b_1$  or  $b_2$ ) in the column. Then, each of them receives its payoff depending on their actions. The panel shows a penny-matching game, where blue (resp. red) panels show that  $X$  (resp.  $Y$ ) gains a payoff of 1 and the other loses it. Looking at the whole, each player memorizes their actions of the past  $n$  rounds. This memorized state is described as  $s_i$  given by  $2n$ -length bits of actions. **B.** Illustration for the detailed single round of repeated games, where present state  $s_i$  transitions to next state  $s_{i'}$ . In this transition, the oldest 2 bits are lost, and the other bits  $s_{i}^{-}$ , colored in green, are maintained. X's and Y's choices ( $a_2$  (blue) and  $b_1$  (red) in this figure) are appended as the newest 2 bits in  $s_{i'}$ . This transition occurs with the probability of  $M_{i'i}$ . Finally, X gains a payoff of  $u_{i'}$  in the state transition.

This study provides a basic analysis of the multi-memory repeated game. First, we extend the two learning algorithms, i.e., replicator dynamics and gradient ascent, for multi-memory games. Then, we name them multi-memory replicator dynamics (MMRD) and gradient ascent (MMGA). As well as shown in the zero-memory games, the equivalence between MMRD and MMGA is proved in Theorems 1-3. Next, we tackle the convergence problem of such algorithms from both viewpoints of theory and experiment. Theorem 4 shows that Nash equilibrium uniquely exists in multi-memory zero-sum games as well as zero-memory ones. This theorem is nontrivial if taking into account the fact that diversification of strategies can expand the region of Nash equilibrium in general games. Then, while utilizing these theorems, we see how multi-memory learning complicates the dynamics, leading to divergence from the Nash equilibrium with sensitivity to its initial condition like chaos.

2

# 2 Preliminary

## 2.1 Two-Player Normal-Form Game 

Let us define two-player (of X and Y)  $m \in \mathbb{N}$ -action games (see illustration of Fig. 1-A). Player X and Y choose their actions from  $\mathcal{A} = \{a_1, \dots, a_m\}$  and  $\mathcal{B} = \{b_1, \dots, b_m\}$  in a single round. After they finish choosing their actions  $a \in \mathcal{A}$  and  $b \in \mathcal{B}$ , each of them gains a payoff  $U(a, b) \in \mathbb{R}$  and  $V(a, b) \in \mathbb{R}$ , respectively.

## 2.2 Two-Player Multi-Memory Repeated Game

We further consider two-player  $n (\in \mathbb{N})$ -memory repeated games as an iteration of the two-player normal-  form game (see illustration Fig. 1-A). The players are assumed to memorize their actions in the last  $n$   rounds. Since each player can take  $m$  actions, there are  $m^{2n}$  cases for possible memorized states, described  as  $S = \prod_{k=1}^{n} (A \times B)$ . Under any memorized state, player X can choose any action stochastically. Such a  stochastic choice of an action is described by a parameter  $x^{a|s}$ , which means the probability of choosing an  action  $a \in A$  under memorized state  $s \in S$ . Thus, X's strategy is represented by  $|S| (= m^{2n})$ -numbers of  $(m - 1)$ -dimension simplexes,  $\mathbf{x} \in \prod_{s \in S} \Delta^{m-1}$ , while Y's is  $\mathbf{y} \in \prod_{s \in S} \Delta^{m-1}$ .

## 2.3 Formulation as Markov Games

In order to handle this multi-memory repeated game as a Markov game [\[28, 29\]](#page-11-0), we define a vector notation of memorized states;

$$
\boldsymbol{s} = (\underbrace{a_1 b_1 \cdots a_1 b_1}_{\times n}, \underbrace{a_1 b_1 \cdots a_1 b_1}_{\times (n-1)} a_1 b_2, \cdots, \underbrace{a_m b_m \cdots a_m b_m}_{\times n}), 
$$

(1)

which orders all the elements of  $\mathcal{S}$  as a vector. We also define a vector notation of utility function as

$$
\boldsymbol{u} = (\underbrace{U(a_1, b_1), \cdots, U(a_1, b_1)}_{{\times m^{2n-2}}}, \underbrace{U(a_1, b_2), \cdots, U(a_1, b_2)}_{{\times m^{2n-2}}}, \cdots, \underbrace{U(a_m, b_m), \cdots, U(a_m, b_m)}_{{\times m^{2n-2}}}),
$$

(2)

which orders all the last-round payoffs for  $S$  as a vector. The utility function for Y, i.e.,  $v$ , is defined similarly. In addition, we denote an index for these vectors as  $i \in \{1, ..., m^{2n}\}\}$ . For example, when player X is in state  $s_i$ , the last-round payoff for player X was  $u_i$ .

Let  $p \in \Delta^{|S|-1}$  be a probability distribution on  $s$  in a round. As the name Markov matrix implies, a distribution in the next round  $p'$  is given by  $p' = Mp$ , where  $M$  is a Markov transition matrix;

$$
M_{i'i} = \begin{cases} x^{a|s_i} y^{b|s_i} & (s_{i'} = abs_i^-) \\ 0 & (\text{otherwise}) \end{cases} ,
$$

(3)

which shows the transition probability from *i*-th state to *i'*-th one for  $i, i' \in \{1, \ldots, m^{2n}\}$ . Here, note that  $s_i^-$  shows the state  $s_i$  except for the oldest two actions. See Fig. 1-B illustrating an example of Markov transition.

## 2.4 Nash Equilibrium 

We now analyze the Nash equilibrium in multi-memory repeated games based on the formulation of Markov games. Let us assume that every agent uses a fixed strategy  $\mathbf{x}$  and  $\mathbf{y}$  or learns slowly enough for the timescale of the Markov transitions. If we further assume that the strategies are within the interiors of simplexes (i.e., the Markov matrix is ergodic), this stationary distribution is unique and described as  $p^{\text{st}}(\mathbf{x}, \mathbf{y})$ . This stationary distribution satisfies  $p^{\text{st}} = Mp^{\text{st}}$ . We also denote each player's expected payoff in the stationary distribution as  $u^{\text{st}}(\mathbf{x}, \mathbf{y}) = p^{\text{st}} \cdot u$  and  $v^{\text{st}}(\mathbf{x}, \mathbf{y}) = p^{\text{st}} \cdot v$ . The goal of learning in the multi-memory game is to search for the Nash equilibrium, denoted by  $(\mathbf{x}^*, \mathbf{y}^*)$ , where their payoffs are maximized as

$$
\begin{cases} \mathbf{x}^* \in \operatorname{argmax}_{\mathbf{x}} u^{\text{st}}(\mathbf{x}, \mathbf{y}^*) \\ \mathbf{y}^* \in \operatorname{argmax}_{\mathbf{y}} v^{\text{st}}(\mathbf{x}^*, \mathbf{y}) \end{cases}
$$

(4)

Here,  $u^{st}$  and  $v^{st}$  are complex non-linear functions for high-dimensional variables of  $(x, y)$ . This Nash equilibrium is difficult to find in general.

3

# 3 Algorithm 

In the following, we define multi-memory versions of two major learning algorithms, i.e., replicator dynamics and gradient ascent. Although we consider the learning of player X, that of player Y can be formulated in the same manner.

**Definition 1** (expected future payoff). We define the expected future payoff from the distribution  $\mathbf{p}$  as

$$
\pi(\boldsymbol{p}, \mathbf{x}, \mathbf{y}) := \sum_{t=0}^{\infty} \boldsymbol{M}^{t}(\boldsymbol{p} - \boldsymbol{p}^{\text{st}}) \cdot \boldsymbol{u}, 
$$

(5)

which is the total payoff player  $X$  obtains from the present round to the future.

In this definition, the stationary payoff  $\boldsymbol{p}^{\text{st}} \cdot \boldsymbol{u} = u^{\text{st}}$  is the offset term every round, and thus  $\pi(\boldsymbol{p}^{\text{st}}, \mathbf{x}, \mathbf{y}) =$ 0.

**Definition 2** (normalization). We define the normalization function Norm:  $\prod_{s\in\mathcal{S}}\mathbb{R}^m_+ \mapsto \prod_{s\in\mathcal{S}}\text{int}(\Delta^{m-1})$  as

$$
\text{Norm}(\mathbf{x}) = \left\{ \frac{x^{a|s}}{\sum_{a'} x^{a'|s}} \right\}_{a,s},
$$

(6)

In this definition,  $\text{Norm}(\mathbf{x})$  satisfies the condition of probability variables for all s. 

Based on these definitions, we formulate discretized MMRD and MMGA as Algorithm 1 and 2.

 Algorithm 1 Discretized MMRD Input: $\eta$ 1: for $t = 0, 1, 2, \cdots$ do X chooses $a$ with probability $x^{a|s_i}$ 2: (Y chooses *b* with probability $y^{b|s_i}$ ) 3: $s_{i'} \leftarrow abs_{i}^{-}$ 4: $x^{a|s_i} \leftarrow x^{a|s_i} + \eta \pi(\boldsymbol{e}_{i'}, \mathbf{x}, \mathbf{y})$ 5: $\mathbf{x} \leftarrow \text{Norm}(\mathbf{x})$ 6: 7: $s_i \leftarrow s_{i'}$ 8: end for

Algorithm 1 (Discretized MMRD) takes its learning rate  $\eta$  as an input. In each time step, the players choose their actions following their strategies (lines 2 and 3), while the state is updated by their chosen actions (lines 4 and 7). Then, each player reinforces its strategy by how much payoff the chosen action brings up to the future. Here, note that for simplicity, this payoff is given by an expected payoff (line 5).

 $Algorithm 2 Discretized MMGA$

```
Input: \eta, \gamma1: for t = 0, 1, 2, \cdots do
             for a \in \mathcal{A}, s \in \mathcal{S} do
  2:
  3:
                 \mathbf{x}' \leftarrow \mathbf{x}x'^{a|s} \leftarrow x'^{a|s} + \gamma4:
                  \Delta^{a|s} \leftarrow \frac{u^{\text{st}}(\text{Norm}(\mathbf{x}'), \mathbf{y}) - u^{\text{st}}(\mathbf{x}, \mathbf{y})}{\gamma}5:
             end for
  6.
             for a \in \mathcal{A}, s \in \mathcal{S} do
  7:
                 x^{a|s} \leftarrow x^{a|s}(1 + n\Delta^{a|s})8:
  9:
             end for
             \mathbf{x} \leftarrow \text{Norm}(\mathbf{x})10:
11: end for
```
4

Algorithm 2 (Discretized MMGA) takes not only its learning rate  $\eta$  but a small value  $\gamma$  in measuring an approximate gradient as inputs. In each time step, each player measures the gradients of its payoff for each variable of its strategy (lines 2-6). Then, the player updates its strategy by the gradients (lines 7-10). Here, note that the strategy update is weighted by the probability  $x^{a|s}$  (line 8) in order to correspond to Algorithm 1. Here, each of lines 3-5 and line 8 can be updated in parallel with respect to  $a$  and  $s$ .

# 4 Theoretical Analysis

## 4.1 Continuous-Time Equivalence of Algorithms

The following theorems provide a unified understanding of different algorithms. Theorem 1 and 2 are concerned with continualization of the two discrete algorithms. Surprisingly, Theorem 3 proves the correspondence between these different continualized algorithms by Theorem 1 and 2.

**Theorem 1** (Continuous MMRD). Let  $p^{a|s}$  be the expected distribution when X chooses a under state s;

$$
p_{i'}^{a|s} := \begin{cases} y^{b|s} & (s_{i'} = ab\bar{s})\\ 0 & (\text{otherwise}) \end{cases}
$$

(7)

In the limit of  $\eta \to 0$ , Algorithm 1 is continuousized as dynamics

$$
\dot{x}^{a|s_i}(\mathbf{x}, \mathbf{y}) = p_i^{\text{st}} x^{a|s_i} \left( \pi(\boldsymbol{p}^{a|s_i}, \mathbf{x}, \mathbf{y}) - \bar{\pi}^{s_i}(\mathbf{x}, \mathbf{y}) \right),
$$

(8)

$$
\bar{\pi}^{s_i}(\mathbf{x}, \mathbf{y}) = \sum_{a} x^{a|s_i} \pi(\boldsymbol{p}^{a|s_i}, \mathbf{x}, \mathbf{y}),
$$

(9)

for all  $a \in A$  and  $s \in S$ . Here,  $\bar{\pi}^{s_i}$  is the expected payoff under state  $s_i$ .

**Theorem 2** (Continualized MMGA). In the limit of  $\gamma \to 0$  and  $\eta \to 0$ , Algorithm 2 is continualized as dynamics.

$$
\dot{x}^{a|s}(\mathbf{x}, \mathbf{y}) = x^{a|s} \frac{\partial}{\partial x^{a|s}} u^{\text{st}}(\text{Norm}(\mathbf{x}), \mathbf{y}),
$$

(10)

for all  $a \in \mathcal{A}$  and  $s \in \mathcal{S}$ .

See Appendix A.1 and A.2 for the proof of Theorems 1 and 2.  $\frac{1}{2}$ 

**Theorem 3** (Equivalence between the algorithms). The dynamics Eqs.  $(8)$  and  $(10)$  are equivalent.

*Proof Sketch.* Let  $\mathbf{x}'$  be the strategy given by  $x^{a|s} \leftarrow x^{a|s} + \gamma$  in  $\mathbf{x}$  for  $a \in \mathcal{A}$  and  $s \in \mathcal{S}$ . Then, we consider the changes of the Markov transition matrix  $\mathrm{d}\boldsymbol{M} := \boldsymbol{M}(\mathrm{Norm}(\mathbf{x}'), \mathbf{y}) - \boldsymbol{M}(\mathbf{x}, \mathbf{y})$  and the stationary distribution  $\mathrm{d}\boldsymbol{p}^{\mathrm{st}} := \boldsymbol{p}^{\mathrm{st}}(\mathrm{Norm}(\mathbf{x}'), \mathbf{y}) - \boldsymbol{p}^{\mathrm{st}}(\mathbf{x}, \mathbf{y})$ . By considering this changes in the stationary condition  $\boldsymbol{p}^{\mathrm{st}} = \boldsymbol{M}\boldsymbol{p}^{\mathrm{st}}$ , we get  $\mathrm{d}\boldsymbol{p}^{\mathrm{st}} = (\boldsymbol{E} - \boldsymbol{M})^{-1} \mathrm{d}\boldsymbol{M} \boldsymbol{p}^{\mathrm{st}}$  in  $O(\gamma)$ . The right-hand (resp. left-hand) side of this equation corresponds to the continualized MMRD (resp. MMGA). See Appendix A.3 for the full proof.  $\square$

For games with a general number of actions, the study [\[7\]](#page-7-1) has proposed a gradient ascent algorithm in relation to replicator dynamics. In light of this study, Theorem 3 extends the relation to the multi-memory games. This extension is neither simple nor trivial. The relation between replicator dynamics and gradient ascent has been proved by directly calculating  $u^{st} = p^{st} \cdot u$  [\[17\]](#page-7-1). In multi-memory games, however,  $u^{st} = p^{st} \cdot u$  is too hard to calculate. Thus, as seen in the proof sketch, we proved the relation by considering a slight change in the stationary condition  $p^{st} = Mp^{st}$ , technically avoiding such a hard direct calculation.

## 4.2 Learning Dynamics Near Nash Equilibrium

Below, let us discuss the learning dynamics in multi-memory games, especially divergence from Nash equilibrium in zero-sum payoff matrices. In order to obtain a phenomenological insight into the learning dynamics simply, we assume one-memory two-action zero-sum games in Assumption 1.

5

**Assumption 1** (One-memory two-action zero-sum game). We assume a two-action (i.e.,  $\mathcal{A} = \{a_1, a_2\}$ and  $\mathcal{B} = \{b_1, b_2\}$ ), one-memory (i.e.,  $\mathbf{s} = (a_1b_1, a_1b_2, a_2b_1, a_2b_2)$ ), and zero-sum game (i.e.,  $\mathbf{v} = -\mathbf{u}$ ). Inparticular, we discuss zero-sum games where both  $u_1$  and  $u_4$  are smaller or larger than both  $u_2$  and  $u_3$ .

Under Assumption 1, we exclude uninteresting zero-sum payoff matrices that the Nash equilibrium exists as a set of pure strategies because the learning dynamics trivially converge to such pure strategies. The condition that both  $u_1$  and  $u_4$  are smaller or larger than both  $u_2$  and  $u_3$  is necessary and sufficient for the existence of no dominant pure strategy.

In the rest of this paper, we use a vector notation for strategies of X and Y;  $x := \{x_i\}_{i=1,...,4}$  and  $y := \{y_i\}_{i=1,...,4}$  as  $x_i := x^{a_1|s_i}$  and  $y_i := y^{b_1|s_i}$ . Indeed,  $x^{a_2|s_i} = 1 - x_i$  and  $y^{b_2|s_i} = 1 - y_i$  hold.**Theorem 4** (Uniqueness of Nash equilibrium). Under Assumption 1, the unique Nash equilibrium of this game is  $(x_i, y_i) = (x^*, y^*)$  for all *i* as

$$
x^* = \frac{-u_3 + u_4}{u_1 - u_2 - u_3 + u_4}, \ y^* = \frac{-u_2 + u_4}{u_1 - u_2 - u_3 + u_4}.
$$

(11)

*Proof Sketch.* Let us prove that X's strategy in Nash equilibrium is uniquely  $x = x^*1$ . First, we define  $u^*$  and  $v^*$  as X's and Y's payoffs in the Nash equilibrium in the zero-memory game. If  $x = x^*1$ , X's expected payoff is  $u^{st} = u^*$ , regardless of Y's strategy  $y$ . Second, we consider that X uses another strategy  $x \neq x^*1$ . Then, there is Y's strategy such that  $v^{st} > v^* \Leftrightarrow u^{st} < u^*$ . Thus, X's minimax strategy is uniquely  $x = x^*1$ , completing the proof. See Appendix A.4 for the full proof.  $\Box$ 

![](_page_5_Figure_6.jpeg)

Figure 2: Multi-memory learning dynamics near the Nash equilibrium in the penny-matching game. In the upper six panels, colored lines indicate the time series of  $\delta_i$  (X's strategy). The solid (resp. broken) lines are approximated (resp. experimental) trajectories of learning dynamics. From the top, the trajectories are predicted by approximations up to the first, second, and third orders. The bottom panel shows the errors between the approximated and experimental trajectories.

Regarding Theorem 4,  $X (Y)$  chooses each action in the same probability independent of the last state. Here, they do not utilize their memory. Thus, note that in this sense, the Nash equilibrium is the same as

6

![](_page_6_Figure_0.jpeg)

Figure 3: Initial state sensitivity in learning dynamics in multi-memory games. In the top panels, colored lines are time series of  $x_i$  (X's strategy). The black line is the distance between the solid (sample of  $x$ ) and broken ( $x'$ ) lines. In the bottom panels, the black lines indicate the maximum eigenvalue in the learning dynamics of the solid line.

![](_page_6_Figure_2.jpeg)

Figure 4:  $\mathbf{A}$ . Payoff matrices of three-action (rock-paper-scissors) and four-action (extended rock-paper-scissors) games. **B.** In each panel, colored lines indicate time series of  $x^{a|s}$  for random  $a \in \mathcal{A}$  and  $s \in \mathcal{S}$ . The black broken line indicates the Kullback-Leibler divergence averaged over all the states  $s \in \mathcal{S}$ , intuitively meaning a distance from the Nash equilibrium.

that in the zero-memory version of the game. This theorem means that in zero-sum games, the region of Nash equilibrium does not expand even if players have memories. Taking into account that having multiple memories expands the region of Nash equilibrium, such as a cooperative equilibrium in prisoner's dilemma games  $[20]$ , this theorem is nontrivial.

In order to discuss whether our algorithms converge to this unique Nash equilibrium under Assumption 1, we consider the neighbor of Nash equilibrium and define sufficient small deviation from the Nash equilibrium, i.e.,  $\delta := x - x^*1$  and  $\epsilon := y - y^*1$ . Here, we assume that these deviations have the same scale  $O(\delta) := O(\delta_i) = O(\epsilon_i)$  for all *i*. Then, defining that the superscript  $(k)$  shows  $O(\delta^k)$  terms, the dynamics are

7

approximated by  $\dot{\boldsymbol{x}} \simeq \dot{\boldsymbol{x}}^{(1)} + \dot{\boldsymbol{x}}^{(2)}$  and  $\dot{\boldsymbol{y}} \simeq \dot{\boldsymbol{y}}^{(1)} + \dot{\boldsymbol{y}}^{(2)}$ ;

$$
\dot{\boldsymbol{x}}^{(1)} = +\boldsymbol{x}^*(1-\boldsymbol{x}^*)(\boldsymbol{u}\cdot\boldsymbol{1}_{\mathsf{z}})\boldsymbol{p}^*\circ\boldsymbol{\epsilon},
$$

(12)

$$
\dot{\boldsymbol{y}}^{(1)} = -\boldsymbol{y}^*(1-\boldsymbol{y}^*)(\boldsymbol{u}\cdot\boldsymbol{1}_{\mathsf{z}})\boldsymbol{p}^*\circ\boldsymbol{\delta},
$$

(13)

$$
\dot{x}^{(2)} = -(x^* - \tilde{x}^*)(u \cdot 1_{z}) \delta \bigcirc \epsilon \bigcirc p^* + x^* \tilde{x}^*(u \cdot 1_{z}) \{ (\delta \cdot p^*) \epsilon \bigcirc y^* \bigcirc 1_{x} + (\epsilon \cdot p^*) \epsilon \bigcirc x^* \bigcirc 1_{y} + (\delta \bigcirc \epsilon \bigcirc y^* \cdot 1_{x}) p^* \},
$$

(14)

$$
\dot{\boldsymbol{y}}^{(2)} = + (\boldsymbol{y}^* - \tilde{\boldsymbol{y}}^*)(\boldsymbol{u} \cdot \mathbf{1}_{\mathsf{z}}) \boldsymbol{\delta} \circ \boldsymbol{\epsilon} \circ \boldsymbol{p}^* - \boldsymbol{y}^* \tilde{\boldsymbol{y}}^* (\boldsymbol{u} \cdot \mathbf{1}_{\mathsf{z}}) \{ (\boldsymbol{\delta} \cdot \boldsymbol{p}^*) \boldsymbol{\delta} \circ \boldsymbol{y}^* \circ \mathbf{1}_{\mathsf{x}} + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^*) \boldsymbol{\delta} \circ \boldsymbol{x}^* \circ \mathbf{1}_{\mathsf{y}} + (\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \circ \boldsymbol{x}^* \circ \mathbf{1}_{\mathsf{y}}) \boldsymbol{p}^* \},
$$

(15)

with  $x^* := (x^*, x^*, \tilde{x}^*, \tilde{x}^*), y^* := (y^*, \tilde{y}^*, y^*, \tilde{y}^*), p^* := x^* \circ y^*, \mathbf{1}_x := (+1, +1, -1, -1), \mathbf{1}_y := (+1, -1, +1, -1),$  and  $\mathbf{1}_z := \mathbf{1}_x \circ \mathbf{1}_y$ . Eqs. (12)-(15) are derived by considering small changes in the stationary condition  $p^{*\text{st}} = Mp^{*\text{st}}$  for deviations of  $\delta$  and  $\epsilon$  (see Appendix B.1 and B.2 for the detailed calculation). By this, we can avoid a direct calculation of  $p^{*\text{st}}$ , which is hard to be obtained.

## 5 Experimental Findings

## 5.1 Simulation and Low-Order Approximation

From the obtained dynamics, i.e., Eqs.  $(12)$ - $(15)$ , we interpret the learning dynamics in detail. In the first-order dynamics, multi-memory learning is no more than a simple extension of the zero-memory one. Indeed,
the zero-memory learning draws an elliptical orbit given by Hamiltonian as the conserved quantity [\[30, 14\]](#page-9-1).
Eqs.  $(12)$  and  $(13)$  mean that the multi-memory dynamics also draw similar elliptical orbits for each pair
of  $x_i$  and  $y_i$ . In other words, the dynamics are given by a linear flow on a four-dimensional torus. Because
no interaction occurs between the pair of  $i$  and  $i'$  such that  $i \neq i'$ , the dynamics of the multi-memory
learning for each state are qualitatively the same as learning without memories. Fig. 2 shows the time
series of the multi-memory learning dynamics near the Nash equilibrium in an example of a two-action
zero-sum game, the penny-matching game ( $u_1 = u_4 = 1, u_2 = u_3 = -1$ ). The experimental trajectories are
generated by the Runge-Kutta fourth-order method of Eq. (10) (see Appendix B.3 for details), while the
approximated trajectories are by the Runge-Kutta fourth-order method for the first- (Eqs.  $(12)$  and  $(13)$ ),
the second- (Eqs.  $(14)$  and  $(15)$ ), and the third-order approximations (in Appendix B.2). The step-size is
 $10^{-2}$  in common. The top-left panel in the figure shows that the dynamics roughly draw a circular orbit
for each state and are well approximated by the first-order dynamics of Eqs. (12) and (13). However, the
top-right panel, where a sufficiently long time has passed, shows that the dynamics deviate from the circular
orbits.

Such deviation from the circular orbits is given by higher-order dynamics than Eqs. (12) and (13). In the second-order dynamics given by Eqs. (14) and (15), the multi-memory learning is qualitatively different from the zero-memory one. Indeed, Eqs. (14) and (15) obviously mean that interactions occur between the pair of  $i$  and  $i'$  such that  $i \neq i'$ . Thereby, the dynamics of multi-memory learning become much more complex than that of zero-memory learning. In practice, no Hamiltonian function, denoted by  $H^{(2)}$ , exists in the second-order dynamics, as different from the first-order one. One can check this by calculating  $\frac{\partial \dot{x}_{i}^{(2)}}{\partial \epsilon_{i'}} + \frac{\partial \dot{y}_{i'}^{(2)}}{\partial \delta_{i}} \neq 0$  for  $i$  and  $i' \neq i$ , if assuming that Hamiltonian should satisfy  $\dot{x}^{(2)} = +\frac{\partial H^{(2)}}{\partial \epsilon}$  and  $\dot{y}^{(2)} = -\frac{\partial H^{(2)}}{\partial \delta}$ . Thus, the multi-memory dynamics might not have any conserved quantities and not draw any closed trajectory. Indeed, the right panels in Fig. 2 show that the dynamics tend to diverge from the Nash equilibrium. This divergence from the Nash equilibrium is surprising because zero-memory learning in zero-sum games always has a closed trajectory and keeps the Kullback-Leibler divergence from the Nash equilibrium constant [31, 14]. Here, note that we need the third-order dynamics to fit the experimental dynamics well, as seen by comparing the middle-right and lower-right panels in Fig. 2. The error between

8

the experiment ( $\boldsymbol{\delta}$  and  $\boldsymbol{\epsilon}$ ) and approximation ( $\boldsymbol{\delta}'$  and  $\boldsymbol{\epsilon}'$ ) is evaluated by

$$
error := \frac{1}{4} \sum_{i=1}^{4} \sqrt{|\delta_i - \delta'_i|^2 + |\epsilon_i - \epsilon'_i|^2}.
$$

(16)

## 5.2 Chaos-Like and Heteroclinic Dynamics

Interestingly, learning dynamics in multi-memory games are complex. Fig. 3 shows two learning dynamics between which there is a slight difference in their initial strategies ( $x = y = 0.8 \times 1$  in the solid line, but in the broken line ( $x'$  and  $y'$ ),  $x'_1 = 0.801$  and others are the same as the solid line). We use Algorithm 2 with  $\eta = 10^{-3}$  and  $\gamma = 10^{-6}$ . These dynamics are similar in the beginning ( $0 \le t \le 320$ ). However, the difference between these dynamics is gradually amplified ( $320 \le t \le 360$ ), leading to the crucial difference eventually ( $360 \le t \le 420$ ). We here introduce the distance between  $x'$  and  $x$  as

$$
D(\boldsymbol{x}', \boldsymbol{x}) := \frac{1}{4} \sum_{i=1}^{4} |L(x_i') - L(x_i)|, 
$$

(17)

with  $L(x) := log x - log(1-x)$ ;  $L(x)$  is the measure taking into account the weight in replicator dynamics. Furthermore, in order to analyze how the difference is amplified, Fig. 3 also shows the maximum eigenvalue in learning dynamics. We can see that the larger the maximum eigenvalue is, the more the difference between the two trajectories is amplified. We observe that such an amplification typically occurs when strategies are close to the boundary of the simplex. In conclusion, the learning dynamics provide chaos-like sensitivity to the initial condition.

## 5.3 Divergence in General Memories and Actions

Although we have focused on the one-memory two-action zero-sum games so far, numerical simulations demonstrate that similar phenomena are seen in games of other numbers of memories and actions. Fig. 4 shows the trajectories of learning dynamics in various multi-memory and multi-action games, where we use Algorithm 2 with  $\eta = 10^{-2}$  and  $\gamma = 10^{-6}$ . Note that we consider zero-sum games in all the panels (see Fig. 4-A for the payoff matrices). In Fig. 4-B, each panel shows that strategy variables  $x^{a|s}$  roughly diverge from the Nash equilibrium and sojourn longer at the edges of the simplex, i.e.,  $x^{a|s} = 0$  or 1. Furthermore, Kullback-Leibler divergence from the Nash equilibrium averaged over the whole states, i.e.,

$$
D_{\mathrm{KL}}(\mathbf{x}^* \| \mathbf{x}) := \frac{1}{|\mathcal{S}|} \sum_{s \in \mathcal{S}} \sum_{a \in \mathcal{A}} x^{*a|s} \log \frac{x^{*a|s}}{x^{a|s}}.
$$

(18)

also increases with time in each panel of the figure. Thus, we confirm that learning reaches heteroclinic cycles under various (action, memory) pairs.

# 6 Conclusion

This study contributes to an understanding of a cutting-edge model of learning in games in Sections 3 and 4. In practice, several famous algorithms, i.e., replicator dynamics and gradient ascent, were newly extended to multi-memory games (Algorithms 1 and 2). Then, we proved the correspondence between these algorithms (Theorems 1-3) in general and the uniqueness of Nash equilibrium in two-action zerosum games (Theorem 4). As a background, multi-agent learning dynamics are generally complicated; thus, many theoretical approaches usually have been taken to grasp such complicated dynamics. In light of this background, our theorems succeeded in capturing the learning dynamics in multi-memory games, which are even more complicated than usual memory-less ones.

This study also experimentally discovered a novel and non-trivial phenomenon that simple learning algorithms such as replicator dynamics and gradient ascent asymptotically reaches a heteroclinic cycle in multi-memory zero-sum games. In other words, the players choose actions in highly skewed proportions throughout learning. Such a phenomenon is specific to multi-memory games: Perhaps this is because the

9

gameplay becomes extreme in learning between those who can use equally sophisticated (i.e., multi-memory) strategies. We also found a novel problem that Nash equilibrium is difficult to reach in multi-memory zerosum games. Here, note that convergence to Nash equilibrium, either as a last-iterate  $[32, 33, 34, 35, 36, 37, 38]$ or as an average of trajectories  $[39, 40, 41]$ , is a frequently discussed topic. In general, heteroclinic cycles fail to converge even on average. What algorithm can converge to Nash equilibrium in multi-memory zero-sum games would be interesting future work.

# References

- [1] Drew Fudenberg and Jean Tirole. *Game theory*. MIT press, 1991.
- [2] John F Nash Jr. Equilibrium points in n-person games. *Proceedings of the National Academy of Sciences*.  $36(1):48-49, 1950.$
- [3] John G Cross. A stochastic learning model of economic behavior. The Quarterly Journal of Economics,  $87(2):239-266, 1973.$
- [4] Tilman Börgers and Rajiv Sarin. Learning through reinforcement and replicator dynamics. Journal of *Economic Theory*,  $77(1):1-14$ , 1997.
- [5] Josef Hofbauer, Karl Sigmund, et al. *Evolutionary games and population dynamics*. Cambridge university press,  $1998$ .
- [6] Satinder Singh, Michael J Kearns, and Yishay Mansour. Nash convergence of gradient dynamics in general-sum games. In  $UAI$ , pages 541–548, 2000.
- [7] Martin Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In *ICML*, pages  $928-936$ ,  $2003$ .
- [8] Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artificial Intelligence,  $136(2):215-250, 2002.$
- [9] Michael Bowling. Convergence and no-regret in multiagent learning. In *NeurIPS*, pages 209–216, 2004.
- [10] Christopher JCH Watkins and Peter Dayan. Q-learning. *Machine learning*, 8(3):279–292, 1992.
- [11] Michael Kaisers and Karl Tuyls. Frequency adjusted multi-agent q-learning. In AAMAS, pages 309–316, 2010.
- [12] Sherief Abdallah and Michael Kaisers. Addressing the policy-bias of q-learning by repeating updates. In  $AAMAS$ , pages 1045–1052, 2013.
- [13] Panayotis Mertikopoulos and William H Sandholm. Learning in games via reinforcement and regularization. Mathematics of Operations Research, 41(4):1297–1324, 2016.
- [14] Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras. Cycles in adversarial regularized learning. In  $SODA$ , pages 2703–2717, 2018.
- [15] Karl Tuyls and Ann Nowé. Evolutionary game theory and multi-agent reinforcement learning. The Knowledge Engineering Review, 20(1):63–90, 2005.
- [16] Karl Tuyls, Pieter Jan'T Hoen, and Bram Vanschoenwinkel. An evolutionary dynamical analysis of multi-agent learning in iterated games. Autonomous Agents and Multi-Agent Systems, 12(1):115–153, 2006.
- [17] Daan Bloembergen, Karl Tuyls, Daniel Hennes, and Michael Kaisers. Evolutionary dynamics of multiagent learning: A survey. *Journal of Artificial Intelligence Research*, 53:659–697, 2015.
- [18] Wolfram Barfuss. Towards a unified treatment of the dynamics of collective learning. In *Challenges and* Opportunities for Multi-Agent Reinforcement Learning, AAAI Spring Symposium, 2020.

10

- [19] Yuma Fujimoto and Kunihiko Kaneko. Functional dynamic by intention recognition in iterated games. New Journal of Physics, 21(2):023025, 2019.
- [20] Robert Axelrod and William D Hamilton. The evolution of cooperation. *Science*,  $211(4489):1390-1396$ , 1981.
- [21] Martin Nowak and Karl Sigmund. A strategy of win-stay, lose-shift that outperforms tit-for-tat in the prisoner's dilemma game. *Nature*, 364(6432):56–58, 1993.
- [22] Drew Fudenberg and Eric Maskin. The folk theorem in repeated games with discounting or with incomplete information. In A long-run collaboration on long-run games, pages 209–230. World Scientific. 2009.
- [23] Wolfram Barfuss, Jonathan F Donges, and Jürgen Kurths. Deterministic limit of temporal difference reinforcement learning for stochastic games. *Physical Review E*,  $99(4):043305$ , 2019.
- [24] Wolfram Barfuss. Reinforcement learning dynamics in the infinite memory limit. In  $\text{AAMAS}$ , pages  $1768 - 1770, 2020.$
- [25] Janusz M Meylahn, Lars Janssen, et al. Limiting dynamics for q-learning with memory one in symmetric two-player, two-action games. *Complexity*, 2022, 2022.
- [26] Yuma Fujimoto and Kunihiko Kaneko. Emergence of exploitation as symmetry breaking in iterated prisoner's dilemma. *Physical Review Research*, 1(3):033077, 2019.
- [27] Yuma Fujimoto and Kunihiko Kaneko. Exploitation by asymmetry of information reference in coevolutionary learning in prisoner's dilemma game. *Journal of Physics: Complexity*,  $2(4):045007, 2021$ .
- [28] Lloyd S Shapley. Stochastic games. Proceedings of the National Academy of Sciences,  $39(10):1095-1100$ , 1953.
- [29] Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In *ICML*, pages  $157-163$ ,  $1994$ .
- [30] Josef Hofbauer. Evolutionary dynamics for bimatrix games: A hamiltonian system? Journal of Mathematical Biology,  $34(5):675-688$ , 1996.
- [31] Georgios Piliouras, Carlos Nieto-Granda, Henrik I Christensen, and Jeff S Shamma. Persistent patterns: Multi-agent learning beyond equilibrium and utility. In  $AAMAS$ , pages 181–188, 2014.
- [32] Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs with optimism. In  $ICLR$ , 2018.
- [33] Constantinos Daskalakis and Ioannis Panageas. Last-iterate convergence: Zero-sum games and constrained min-max optimization. In ITCS, pages 27:1-27:18, 2019.
- [34] Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay Chandrasekhar, and Georgios Piliouras. Optimistic mirror descent in saddle-point problems: Going the extra(-gradient) mile. In  $ICLR$ , 2019.
- [35] Noah Golowich, Sarath Pattathil, and Constantinos Daskalakis. Tight last-iterate convergence rates for no-regret learning in multi-player games. In *NeurIPS*, pages  $20766-20778$ ,  $2020$ .
- [36] Chen-Yu Wei, Chung-Wei Lee, Mengxiao Zhang, and Haipeng Luo. Linear last-iterate convergence in constrained saddle-point optimization. In  $ICLR$ , 2021.
- [37] Qi Lei, Sai Ganesh Nagarajan, Ioannis Panageas, et al. Last iterate convergence in no-regret learning: constrained min-max optimization for convex-concave landscapes. In  $AISTATS$ , pages 1441–1449, 2021.
- [38] Kenshi Abe, Mitsuki Sakamoto, and Atsushi Iwasaki. Mutation-driven follow the regularized leader for last-iterate convergence in zero-sum games. In  $UAI$ , pages 1–10, 2022.

11

- [39] Bikramjit Banerjee and Jing Peng. Efficient no-regret multiagent learning. In  $AAAI$ , pages 41–46, 2005.
- [40] Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization in games with incomplete information. In *NeurIPS*, pages 1729–1736, 2007.
- [41] Constantinos Daskalakis, Alan Deckelbaum, and Anthony Kim. Near-optimal no-regret algorithms for zero-sum games. In  $SODA$ , pages 235–254, 2011.

12

# A Proofs

## A.1 Proof of Theorem 1 

First, line  $6$  in Algorithm 1 is equal to

$$
x^{a'|s_i} \leftarrow \begin{cases} x^{a'|s_i} + (1 - x^{a'|s_i})\eta\pi(\mathbf{e}_{i'}, \mathbf{x}, \mathbf{y}) + O(\gamma^2) & (a' = a) \\ x^{a'|s_i} - x^{a'|s_i}\eta\pi(\mathbf{e}_{i'}, \mathbf{x}, \mathbf{y}) + O(\gamma^2) & (a' \neq a) \end{cases},
$$

(A1)

from Definition  $1$ .

In the stationary state of the repeated games, state  $s_i$  occurs with the probability of  $p_i^{\text{st}}$ . Then, player X (resp. Y) chooses action a (resp. b) with the probability of  $x^{a|s_i}$  and  $y^{b|s_i}$ . If we take the limit  $\eta \to 0$  for updating  $1/\eta$  times, Algorithm 1 is continualized as dynamics

$$
\dot{x}^{a|s_i} = p_i^{\text{st}} \sum_b y^{b|s_i} \left( x^{a|s_i} (1 - x^{a|s_i}) \pi(\mathbf{e}_{i'(a,b)}, \mathbf{x}, \mathbf{y}) + \sum_{a' \neq a} x^{a'|s_i} (-x^{a|s_i}) \pi(\mathbf{e}_{i'(a',b)}, \mathbf{x}, \mathbf{y}) \right)
$$

(A2)

$$
=p_i^{\text{st}}x^{a|s}\left\{\pi(\underbrace{\sum_{b}y^{b|s_i}e_{i'(a,b)},\mathbf{x},\mathbf{y})}_{a} - \sum_{a'}x^{a'|s_i}\pi(\underbrace{\sum_{b}y^{b|s_i}e_{i'(a',b)},\mathbf{x},\mathbf{y})}_{a'}\right\} 
$$

(A3)

$$
=p^{a|s}
$$

$$
=p^{a'|s_i}
$$

$$
= p_i^{st} x^{a|s_i} \left( \pi(p^{a|s_i}, \mathbf{x}, \mathbf{y}) - \sum_{a'} x^{a'|s_i} \pi(p^{a'|s_i}, \mathbf{x}, \mathbf{y})\right)
$$

$$
= p_i^{st} x^{a|s_i} \left( \pi(p^{a|s_i}, \mathbf{x}, \mathbf{y}) - \underbrace{\sum_{a'} x^{a'|s_i} \pi(p^{a'|s_i}, \mathbf{x}, \mathbf{y})}_{=\bar{\pi}^{s_i}(\mathbf{x}, \mathbf{y})}\right)
$$

(A4)

Here,  $i'(a, b)$  indicates the next state index  $i'$  such that  $s_{i'} = a b \bar{s_i}$ . Eq. (A4) corresponds to Eqs (8) and (9) in the main manuscript.  $\Box$ 

## A.2 Proof of Theorem 2 

Taking the limit  $\gamma \to 0$ , we obtain

$$
\Delta^{a|s} = \frac{\partial u^{\text{st}}(\text{Norm}(\mathbf{x}), \mathbf{y})}{\partial x^{a|s}}.
$$

(A5)

Then, if we take the limit  $\eta \to 0$  for updating  $1/\eta$  times, Algorithm 2 is continualized as dynamics

$$
\dot{x}^{a|s}(\mathbf{x}, \mathbf{y}) = x^{a|s} \frac{\partial}{\partial x^{a|s}} u^{\text{st}}(\text{Norm}(\mathbf{x}), \mathbf{y}).
$$

(A6)

Eq.  $(A6)$  corresponds to Eq.  $(10)$ .

## A.3 Proof of Theorem 3 

We assume infinitesimal  $dx^{a|s_i}$ , and the infinitesimal change in **x**;

$$
x^{a|s_i} \leftarrow x^{a|s_i} + \mathrm{d}x^{a|s_i},
$$

(A7)

$$
\mathbf{x} \leftarrow \text{Norm}(\mathbf{x}),
$$

(A8)

By this change, the Markov transition matrix  $M(\mathbf{x}, \mathbf{y})$  changes into  $M(\mathbf{x}, \mathbf{y}) + dM(\mathbf{x}, \mathbf{y}, dx^{a|s})$ , described  $as$ 

$$
\mathrm{d}M_{i'i''} = \mathrm{d}x^{a|s_i}y^{b|s_i} \times \begin{cases} 1 - x^{a|s_i} & (s_{i''} = s_i, \ s_{i'} = abs_i^-) \\ -x^{a'|s_i} & (s_{i''} = s_i, \ s_{i'} = a'bs_i^-, a' \neq a) \\ 0 & (\text{otherwise}) \end{cases}
$$

(A9)

13

for all  $i', i'' \in \{1, ..., |S|\}$ . Then, the equilibrium state  $\boldsymbol{p}^{\text{st}}(\mathbf{x}, \mathbf{y})$  changes into  $\boldsymbol{p}^{\text{st}}(\mathbf{x}, \mathbf{y}) + \mathrm{d}\boldsymbol{p}^{\text{st}}(\mathbf{x}, \mathbf{y}, \mathrm{d}x^{a|s})$ .  Here, note that  $\mathrm{d}\boldsymbol{M}$  and  $\mathrm{d}\boldsymbol{p}^{\text{st}}$  are a matrix and a vector of order  $O(\mathrm{d}x^{a|s})$ , respectively. From the stationary state condition, both  $(E-M)\boldsymbol{p}^{\text{st}}=0$  and  $(E-(M+\mathrm{d}M))(\boldsymbol{p}^{\text{st}}+\mathrm{d}\boldsymbol{p}^{\text{st}})=0$  hold.

$$
(E – (M + dM)) (p^{\text{st}} + dp^{\text{st}}) – (E – M)p^{\text{st}} = 0 
$$

(A10)

$$
\Leftrightarrow (\boldsymbol{E} - \boldsymbol{M}) \mathrm{d} \boldsymbol{p}^{\mathrm{st}} = \mathrm{d} \boldsymbol{M} \boldsymbol{p}^{\mathrm{st}} + \boldsymbol{1} \times O((\mathrm{d} x^{a|s})^2).
$$

(A11)

Here, the term of  $O((dx^{a|s})^2)$  is small enough to be ignored. Then, the rest term  $\delta \boldsymbol{M} \boldsymbol{p}^{st}$  is calculated as

$$
(\mathrm{d}\boldsymbol{M}\boldsymbol{p}^{\mathrm{st}})_{i'} = \sum_{i''}\mathrm{d}M_{i'i''}p_{i''}^{\mathrm{st}} = \mathrm{d}M_{i'i}p_i^{\mathrm{st}}
$$

(A12)

$$
= \mathrm{d}x^{a|s_i} p_i^{\mathrm{st}} y^{b|s_i} \times \begin{cases} 1 - x^{a|s_i} & (s_{i'} = abs_i^-) \\ -x^{a'|s_i} & (s_{i'} = a'bs_i^-, a' \neq a) \\ 0 & (\text{otherwise}) \end{cases} .
$$

(A13)

$$
= \mathrm{d}x^{a|s_i} p_i^{\mathrm{st}} \left( p_{i'}^{a|s_i} - \sum_{a'} x^{a'|s_i} p_{i'}^{a'|s_i} \right). 
$$

(A14)

Thus,

$$
(\boldsymbol{E} - \boldsymbol{M}) \mathrm{d} \boldsymbol{p}^{\mathrm{st}} = \mathrm{d} x^{a|s_i} p_i^{\mathrm{st}} \left( \boldsymbol{p}^{a|s_i} - \sum_{a'} x^{a'|s_i} \boldsymbol{p}^{a'|s_i} \right)
$$

(A15)

$$
\mathrm{d}\boldsymbol{p}^{\mathrm{st}} = \mathrm{d}x^{a|s_i} p_i^{\mathrm{st}} (\boldsymbol{E} - \boldsymbol{M})^{-1} \left( \boldsymbol{p}^{a|s_i} - \sum_{a'} x^{a'|s_i} \boldsymbol{p}^{a'|s_i} \right)
$$

(A16)

$$
\Leftrightarrow \frac{\mathrm{d}\boldsymbol{p}^{\mathrm{st}}}{\mathrm{d}x^{a|s_i}} = \frac{\partial}{\partial x^{a|s_i}} \boldsymbol{p}^{\mathrm{st}}(\mathrm{Norm}(\mathbf{x}), \mathbf{y}) = p_{s_i}^{\mathrm{st}} (\boldsymbol{E} - \boldsymbol{M})^{-1} \left(\boldsymbol{p}^{a|s_i} - \sum_{a'} x^{a'|s_i} \boldsymbol{p}^{a'|s_i}\right)
$$

(A17)

$$
\Leftrightarrow \frac{\partial}{\partial x^{a|s_i}} \boldsymbol{p}^{\text{st}}(\text{Norm}(\mathbf{x}), \mathbf{y}) \cdot \boldsymbol{u} = p_i^{\text{st}} (\boldsymbol{E} - \boldsymbol{M})^{-1} \left( \boldsymbol{p}^{a|s_i} - \sum_{a'} x^{a'|s_i} \boldsymbol{p}^{a'|s_i} \right) \cdot \boldsymbol{u}
$$

(A18)

$$
\Leftrightarrow \frac{\partial}{\partial x^{a|s_i}} u^{\text{st}}(\text{Norm}(\mathbf{x}), \mathbf{y}) = p_i^{\text{st}} \left( \pi(\boldsymbol{p}^{a|s_i}, \mathbf{x}, \mathbf{y}) - \sum_{a'} x^{a'|s_i} \pi(\boldsymbol{p}^{a'|s_i}, \mathbf{x}, \mathbf{y}) \right).
$$

(A19)

The left-hand (resp. right-hand) side of Eq.  $(A19)$  corresponds to continualized MMGA (resp. MMRD).  $\Box$ 

## A.4 Proof of Theorem 4 

Let us prove that X's strategy in Nash equilibrium is uniquely  $x = x^*$ . First, we define  $u^*$  and  $v^*$ ;

$$
u^* = x^*y^*u_1 + x^*(1 - y^*)u_2 + (1 - x^*)y^*u_3 + (1 - x^*)(1 - y^*)u_4 
$$

(A20)

$$
= \frac{u_1 u_4 + u_2 u_3}{u_1 - u_2 - u_3 + u_4} 
$$

(A21)

$$
(=-v^*)
$$

(A22)

as X's and Y's payoffs in the Nash equilibrium in the zero-memory game. If X uses the Nash equilibrium strategy  $x = x^*\mathbf{1}$ , the stationary state condition  $p^{\text{st}} = Mp^{\text{st}}$  satisfies

$$
\boldsymbol{p}^{\text{st}} = \begin{pmatrix} x^* y_1 & x^* y_2 & x^* y_3 & x^* y_4 \\ x^* \tilde{y}_1 & x^* \tilde{y}_2 & x^* \tilde{y}_3 & x^* \tilde{y}_4 \\ \tilde{x}^* y_1 & \tilde{x}^* y_2 & \tilde{x}^* y_3 & \tilde{x}^* y_4 \\ \tilde{x}^* \tilde{y}_1 & \tilde{x}^* \tilde{y}_2 & \tilde{x}^* \tilde{y}_3 & \tilde{x}^* \tilde{y}_4 \end{pmatrix} \boldsymbol{p}^{\text{st}}
$$ 

(A23)

$$
\Rightarrow \boldsymbol{p}^{\text{st}} = (x^* \boldsymbol{p}^{\text{st}} \cdot \boldsymbol{y}, x^* (1 - \boldsymbol{p}^{\text{st}} \cdot \boldsymbol{y}), \tilde{x}^* \boldsymbol{p}^{\text{st}} \cdot \boldsymbol{y}, \tilde{x}^* (1 - \boldsymbol{p}^{\text{st}} \cdot \boldsymbol{y}))^{\text{T}}
$$

(A24)

$$
\Rightarrow \boldsymbol{p}^{\mathrm{st}} \cdot \boldsymbol{u} = u^* 
$$

(A25)

$$
\Leftrightarrow u^{\text{st}} = u^*.
$$

(A26)

14

which shows that X's payoff in the stationary state is  $u^*$ , regardless of Y's strategy  $u$ .

Below, we show that if X uses another strategy  $\mathbf{x} \neq x^*\mathbf{1}$ , there always is Y's strategy such that  $v^{\text{st}} > v^* \Leftrightarrow u^{\text{st}} < u^*$ . As X's non-equilibrium strategy, we assume the case  $x_1 \neq x^*$  representatively. Then, Y's strategy  $\mathbf{y} = y^* \mathbf{1} + dy_1 \mathbf{e}_1$  with sufficiently small  $dy_1$  satisfies

$$
\boldsymbol{p}^{\text{st}} = \begin{pmatrix} x_1(y^* + \mathrm{d}y_1) & x_2y^* & x_3y^* & x_4y^* \\ x_1(\tilde{y}^* - \mathrm{d}y_1) & x_2\tilde{y}^* & x_3\tilde{y}^* & x_4\tilde{y}^* \\ \tilde{x}_1(y^* + \mathrm{d}y_1) & \tilde{x}_2y^* & \tilde{x}_3y^* & \tilde{x}_4y^* \\ \tilde{x}_1(\tilde{y}^* - \mathrm{d}y_1) & \tilde{x}_2\tilde{y}^* & \tilde{x}_3\tilde{y}^* & \tilde{x}_4\tilde{y}^* \end{pmatrix} \boldsymbol{p}^{\text{st}}.
$$

(A27)

In this equation, we approximate  $\mathbf{p}^{\text{st}} \simeq \mathbf{p}^{\text{st}(0)} + \mathbf{p}^{\text{st}(1)}$ , where  $\mathbf{p}^{\text{st}(k)}$  describes the  $O((\mathrm{d}y_1)^k)$  term in  $\mathbf{p}^{\text{st}}$ . We can derive these 0-th and 1-st order terms by comparing the left-hand and right-side of this equation. Here, the 0-th order term satisfies  $\mathbf{p}^{\text{st}(0)} \cdot \mathbf{u} = u^*$ , which means that the term does not contribute to the deviation from the Nash equilibrium payoff. On the other hand, the 1-st order term gives

$$
\boldsymbol{p}^{\text{st}(1)} = p_1^{\text{st}(0)} \text{d}y_1 (+x_1, -x_1, +\tilde{x}_1, -\tilde{x}_1)^{\text{T}} 
$$

(A28)

$$
\Rightarrow v^{\text{st}(1)} = p_1^{\text{st}(0)} dy_1 \underbrace{(v_1 - v_2 - v_3 + v_4)}_{=v \cdot \mathbf{1} \neq 0} (x_1 - x^*).
$$

(A29)

Here, we use  $\mathbf{1}_z := (+1, -1, -1, +1)$ . Thus, in the leading order,  $v^{\text{st}(1)} > v^* \Leftrightarrow u^{\text{st}(1)} < u^*$  holds by taking  $dy_1 > 0$  if  $\boldsymbol{v} \cdot \mathbf{1}_z(x_1 - x^*) > 0$ , while by taking  $dy_1 < 0$  if  $\boldsymbol{v} \cdot \mathbf{1}_z(x_1 - x^*) < 0$ . In other words, X's minimax strategy is  $\boldsymbol{x} = x^*\mathbf{1}$ . Similarly, we can prove that Y's minimax strategy is  $\boldsymbol{y} = y^*\mathbf{1}$ . Thus, the Nash equilibrium is given by  $(\boldsymbol{x}, \boldsymbol{y}) = (x^* \mathbf{1}, y^* \mathbf{1}).$   $\Box$ 

# B Analysis of Learning Dynamics 

## B.1 Simpler MMGA for Two-action Games 

This section is concerned with the contents in **Section 4.2** in the main manuscript.

Especially in two-action games, we can use the formulation of Assumption 1 in the main manuscript. By replacing the strategies  $(\mathbf{x}, \mathbf{y})$  by  $(\mathbf{x}, \mathbf{y})$ , we can formulation another simpler algorithm of MMGA as

```
Algorithm A1 Discretized MMGA for two-action
Input: \eta, \gamma1: for t = 0, 1, 2, \cdots do
  2:
           for i = 1, 2, \ldots, |\mathcal{S}| do
                x' \leftarrow x3:
               x'_{i} \leftarrow x'_{i} + \gamma<br>\Delta_{i} \leftarrow (1 - x_{i}) \frac{u^{\text{st}}(\boldsymbol{x}', \boldsymbol{y}) - u^{\text{st}}(\boldsymbol{x}, \boldsymbol{y})}{\gamma}4:
  5:
           end for
  6:
           for i = 1, 2, ..., |S| do
  7:
                x_i \leftarrow x_i (1 + \eta \Delta_i)8:
           end for
  9:
           \mathbf{x} \leftarrow \text{Norm}(\mathbf{x})10:
11: end for
```

There is a major difference between the original and simpler MMGAs in lines 4 and 5. The equivalence

15

of lines 4 and 5 between these algorithms are proved as

$$
\begin{cases}
 x'^{a_1|s_i} \leftarrow x'^{a_1|s_i} + \gamma \\
 \mathbf{x}' \leftarrow \text{Norm}(\mathbf{x}') \\
 \Delta^{a_1|s_i} \leftarrow u^{st}(\mathbf{x}', \mathbf{y}) - u^{st}(\mathbf{x}, \mathbf{y})
\end{cases}
$$

(A30)

$$
\left\{
 \begin{aligned}
 &\gamma \\
 \Leftrightarrow \left\{
 \begin{aligned}
 x'^{a_1|s_i} &\leftarrow x'^{a_1|s_i} + (1 - x'^{a_1|s_i})\gamma + O(\gamma^2) \\
 x'^{a_2|s_i} &\leftarrow x'^{a_2|s_i} - x'^{a_2|s_i}\gamma + O(\gamma^2) \\
 \Delta^{a_1|s_i} &\leftarrow \frac{u^{\text{st}}(\mathbf{x}', \mathbf{y}) - u^{\text{st}}(\mathbf{x}, \mathbf{y})}{\gamma}
 \end{aligned}
 \right.
 \end{aligned}
\right. 
$$

(A31)

$$
\Leftrightarrow \begin{cases} x_i' \leftarrow x_i' + (1 - x_i')\gamma \\ \Delta_i \leftarrow \frac{u^{\text{st}}(\boldsymbol{x}', \boldsymbol{y}) - u^{\text{st}}(\boldsymbol{x}, \boldsymbol{y})}{\gamma} \end{cases} 
$$

(A32)

$$
\Leftrightarrow \left\{ \begin{array}{l} x_i' \leftarrow x_i' + \gamma \\ \Delta_i \leftarrow (1 - x_i') \frac{u^{\text{st}}(\boldsymbol{x}', \boldsymbol{y}) - u^{\text{st}}(\boldsymbol{x}, \boldsymbol{y})}{\gamma} \end{array} \right. .
$$

(A33)

Here, we ignore terms of  $O(\gamma^2)$  and use the definition of  $\boldsymbol{x}$  (i.e.,  $x'_i = x'^{a_1|s_i} = 1 - x'^{a_2|s_i}$ ) between Eqs. (A31) and (A32). Thus, we use the continualized version of this algorithm;

$$
\dot{\boldsymbol{x}} = \boldsymbol{x} \circ (\boldsymbol{1} - \boldsymbol{x}) \circ \frac{\partial}{\partial \boldsymbol{x}} u^{\text{st}}(\boldsymbol{x}, \boldsymbol{y}).
$$

(A34)

## B.2 Approximation of learning dynamics

In **Section 4.2** and **5.1**, we introduce a method to approximate the learning dynamics up to k-th order terms for deviations from the Nash equilibrium. The stationary state condition of the one-memory two-action game is given by

$$
\boldsymbol{p}^{\text{st}} = \boldsymbol{M} \boldsymbol{p}^{\text{st}},
$$

(A35)

$$
M = \begin{pmatrix} x_1 y_1 & x_2 y_2 & x_3 y_3 & x_4 y_4 \ x_1 \tilde{y}_1 & x_2 \tilde{y}_2 & x_3 \tilde{y}_3 & x_4 \tilde{y}_4 \ \tilde{x}_1 y_1 & \tilde{x}_2 y_2 & \tilde{x}_3 y_3 & \tilde{x}_4 y_4 \ \tilde{x}_1 \tilde{y}_1 & \tilde{x}_2 \tilde{y}_2 & \tilde{x}_3 \tilde{y}_3 & \tilde{x}_4 \tilde{y}_4 \end{pmatrix}.
$$

(A36)

Here, for any variable  $\mathcal{X}$ , we define  $\tilde{\mathcal{X}} := 1 - \mathcal{X}$ . In addition, let us denote  $O(\delta^k)$  term in any variable  $\mathcal{X}$  as  $\mathcal{X}^{(k)}$ . The neighbor of the Nash equilibrium, by substituting  $\boldsymbol{x} = x^* \boldsymbol{1} + \boldsymbol{\delta}$  and  $\boldsymbol{y} = y^* \boldsymbol{1} + \boldsymbol{\epsilon}$ , we can decompose  $\boldsymbol{M} = \sum_{k=0}^2 \boldsymbol{M}^{(k)}$  as

$$
M = \underbrace{(x^* \circ y^*) \otimes 1}_{=M^{(0)}} + \underbrace{(y^* \circ 1_x) \otimes \delta + (x^* \circ 1_y) \otimes \epsilon}_{=M^{(1)}} + \underbrace{1_z \otimes (\delta \circ \epsilon)}_{=M^{(2)}},
$$

(A37)

$$
=M^{(0)}
$$

$$
=M^{(1)}
$$

$$
=M^{(2)}
$$

$$
\mathbf{x}^* := (x^*, x^*, \tilde{x}^*, \tilde{x}^*), \qquad \mathbf{y}^* := (y^*, \tilde{y}^*, y^*, \tilde{y}^*),
$$

(A38)

$$
\mathbf{1}_x := (+1, +1, -1, -1)^{\mathrm{T}}, \quad \mathbf{1}_y := (+1, -1, +1, -1)^{\mathrm{T}}, \quad \mathbf{1}_z := \mathbf{1}_x \circ \mathbf{1}_y = (+1, -1, -1, +1)^{\mathrm{T}}. 
$$

(A39)

In the same way, we can decompose  $\mathbf{p}^{\text{st}} \simeq \sum_{k=0}^{\infty} \mathbf{p}^{\text{st}(k)}$  as

$$
\boldsymbol{p}^{\text{st}(0)} = \boldsymbol{M}^{(0)} \boldsymbol{p}^{\text{st}(0)} = \underbrace{(\boldsymbol{p}^{\text{st}(0)} \cdot \boldsymbol{1})}_{=1} \boldsymbol{x}^* \circ \boldsymbol{y}^* =: \boldsymbol{p}^*,
$$

(A40)

$$
\boldsymbol{p}^{\text{st}(1)} = \boldsymbol{M}^{(1)} \boldsymbol{p}^{\text{st}(0)} = (\boldsymbol{\delta} \cdot \boldsymbol{p}^*) \boldsymbol{y}^* \circ \mathbf{1}_x + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^*) \boldsymbol{x}^* \circ \mathbf{1}_y, 
$$

(A41)

$$
\boldsymbol{p}^{\text{st}(2)} = \boldsymbol{M}^{(2)} \boldsymbol{p}^{\text{st}(0)} + \boldsymbol{M}^{(1)} \boldsymbol{p}^{\text{st}(1)} 
$$

(A42)

$$
= (\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{p}^*) \mathbf{1}_z + \{ (\boldsymbol{\delta} \cdot \boldsymbol{p}^*) (\boldsymbol{\delta} \circ \boldsymbol{y}^* \cdot \mathbf{1}_x) + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^*) (\boldsymbol{\delta} \circ \boldsymbol{x}^* \cdot \mathbf{1}_y) \} \, \boldsymbol{y}^* \circ \mathbf{1}_x + \{ (\boldsymbol{\delta} \cdot \boldsymbol{p}^*) (\boldsymbol{\epsilon} \circ \boldsymbol{y}^* \cdot \mathbf{1}_x) + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^*) (\boldsymbol{\epsilon} \circ \boldsymbol{x}^* \cdot \mathbf{1}_y) \} \, \boldsymbol{x}^* \circ \mathbf{1}_y.
$$

(A43)

16

We also get  $\boldsymbol{p}^{\text{st}(k)} = \boldsymbol{M}^{(2)} \boldsymbol{p}^{\text{st}(k-2)} + \boldsymbol{M}^{(1)} \boldsymbol{p}^{\text{st}(k-1)}$  for further orders of  $k \geq 2$ . Then, the equilibrium payoff is given by

$$
u^{\text{st}(0)} = \boldsymbol{p}^{\text{st}(0)} \cdot \boldsymbol{u} = u^{\text{st}}
$$

(A44)

$$
u^{\text{st}(1)} = \boldsymbol{p}^{\text{st}(1)}, \boldsymbol{u} = 0, 
$$

(A45)

$$
u^{\text{st}(2)} = \boldsymbol{p}^{\text{st}(2)} (\boldsymbol{r} \cdot \boldsymbol{s}^*) (A46)$$

$$u^{\text{st}(2)} = \boldsymbol{p}^{\text{st}(2)} \cdot \boldsymbol{u} = (\boldsymbol{u} \cdot \boldsymbol{1}_z)(\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{p}^*), \tag{A46}$$

$$u^{\text{st}(3)} = \boldsymbol{p}^{\text{st}(3)} \cdot \boldsymbol{u} = (\boldsymbol{M}^{(2)} \boldsymbol{p}^{\text{st}(1)} + \boldsymbol{M}^{(1)} \boldsymbol{p}^{\text{st}(2)}) \cdot \boldsymbol{u} = (\boldsymbol{u} \cdot \boldsymbol{1}_z)(\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{p}^{\text{st}(1)}) \tag{A47}$$

$$u^{\text{st}(3)} = p^{\text{st}(3)} \cdot u = (M^{(2)} p^{\text{st}(1)} + M^{(1)} p^{\text{st}(2)}) \cdot u = (u \cdot 1_z) (\delta \circ \epsilon \circ p^{\text{st}(1)}) \tag{A47}$$
$$= (u \cdot 1_z) \{(\delta \cdot p^*)(\delta \circ \epsilon \circ y^* \cdot 1_x) + (\epsilon \cdot p^*)(\delta \circ \epsilon \circ x^* \cdot 1_u)\}, \tag{A48}$$

$$-(\boldsymbol{u} \cdot \boldsymbol{1}_{z})(\boldsymbol{\delta} \circ \boldsymbol{p})(\boldsymbol{\delta} \circ \boldsymbol{y} \cdot \boldsymbol{1}_{x}) + (\boldsymbol{\epsilon} \cdot \boldsymbol{p})(\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{x} \cdot \boldsymbol{1}_{y})f,$$
(A48)
$$u^{\text{st}(4)} = \boldsymbol{p}^{\text{st}(4)} \cdot \boldsymbol{u} = (\boldsymbol{M}^{(2)} \boldsymbol{p}^{\text{st}(2)} + \boldsymbol{M}^{(1)} \boldsymbol{p}^{\text{st}(3)}) \cdot \boldsymbol{u} = (\boldsymbol{u} \cdot \boldsymbol{1}_z)(\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{p}^{\text{st}(2)})$$
(A49)

$$= (u \cdot 1_{z})[(\delta \circ \epsilon \cdot p^{*})(\delta \circ \epsilon \cdot 1_{z})
+ \{(\delta \cdot p^{*})(\delta \circ y^{*} \cdot 1_{x}) + (\epsilon \cdot p^{*})(\delta \circ x^{*} \cdot 1_{y})\}(\delta \circ \epsilon \circ y^{*} \cdot 1_{x})
+ \{(\delta \cdot p^{*})(\epsilon \circ y^{*} \cdot 1_{x}) + (\epsilon \cdot p^{*})(\epsilon \circ x^{*} \cdot 1_{y})\}(\delta \circ \epsilon \circ x^{*} \cdot 1_{y})].$$
(A50)

Here, we used  $\mathbf{H}$ 

$$
\boldsymbol{M}^{(1)\mathrm{T}}\boldsymbol{u} = \underbrace{(\boldsymbol{y}^* \circ \boldsymbol{1}_x \cdot \boldsymbol{u})}_{=0} \boldsymbol{\delta} + \underbrace{(\boldsymbol{x}^* \circ \boldsymbol{1}_y \cdot \boldsymbol{u})}_{=0} \boldsymbol{\epsilon} = \boldsymbol{0},
$$

(A51)

$$
\boldsymbol{M}^{(2)\mathrm{T}}\boldsymbol{u} = (\boldsymbol{u}\cdot\boldsymbol{1}_z)\boldsymbol{\delta}\circ\boldsymbol{\epsilon}.
$$

(A52)

Then, the gradient of this payoff is given by

$$
\frac{\partial u^{\text{st}(1)}}{\partial \boldsymbol{\delta}} = \mathbf{0},
$$

(A53)

$$
\frac{\partial u^{\text{st}(2)}}{\partial \boldsymbol{\delta}} = (\boldsymbol{u} \cdot \mathbf{1}_z) \boldsymbol{\epsilon} \circ \boldsymbol{p}^*, 
$$

(A54)

$$
\frac{\partial u^{\text{st}(3)}}{\partial {\delta}} = ({u} \cdot {1}_z) \{ ({\delta} \cdot {p}^*) {\epsilon} \circ {y}^* \circ {1}_x + ({\epsilon} \cdot {p}^*) {\epsilon} \circ {x}^* \circ {1}_y + ({\delta} \circ {\epsilon} \circ {y}^* \cdot {1}_x) {p}^* \}, 
$$

(A55)

$$
\frac{\partial u^{\text{st}(4)}}{\partial \boldsymbol{\delta}} = (\boldsymbol{u} \cdot \boldsymbol{1}_{z}) [(\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{p}^{*})(\boldsymbol{\epsilon} \circ \boldsymbol{1}_{z}) + (\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \cdot \boldsymbol{1}_{z})(\boldsymbol{\epsilon} \circ \boldsymbol{p}^{*}) \\
+ \{(\boldsymbol{\delta} \cdot \boldsymbol{p}^{*})(\boldsymbol{\delta} \circ \boldsymbol{y}^{*} \cdot \boldsymbol{1}_{x}) + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^{*})(\boldsymbol{\delta} \circ \boldsymbol{x}^{*} \cdot \boldsymbol{1}_{y})\}(\boldsymbol{\epsilon} \circ \boldsymbol{y}^{*} \circ \boldsymbol{1}_{x}) \\
+ \{(\boldsymbol{\delta} \cdot \boldsymbol{p}^{*})(\boldsymbol{\epsilon} \circ \boldsymbol{y}^{*} \cdot \boldsymbol{1}_{x}) + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^{*})(\boldsymbol{\epsilon} \circ \boldsymbol{x}^{*} \cdot \boldsymbol{1}_{y})\}(\boldsymbol{\epsilon} \circ \boldsymbol{x}^{*} \circ \boldsymbol{1}_{y}) \\
+ (\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \circ \boldsymbol{y}^{*} \cdot \boldsymbol{1}_{x})\{(\boldsymbol{\delta} \cdot \boldsymbol{p}^{*})\boldsymbol{y}^{*} \circ \boldsymbol{1}_{x} + (\boldsymbol{\epsilon} \cdot \boldsymbol{p}^{*})\boldsymbol{x}^{*} \circ \boldsymbol{1}_{y} + (\boldsymbol{\delta} \circ \boldsymbol{y}^{*} \cdot \boldsymbol{1}_{x})\boldsymbol{p}^{*}\} \\
+ (\boldsymbol{\delta} \circ \boldsymbol{\epsilon} \circ \boldsymbol{x}^{*} \cdot \boldsymbol{1}_{y})(\boldsymbol{\epsilon} \circ \boldsymbol{y}^{*} \cdot \boldsymbol{1}_{x})\boldsymbol{p}^{*}].
$$

(A56)

The learning dynamics (of continualized MMGA) in two-action one-memory games are given by

$$
\dot{\boldsymbol{\delta}} = +\left(x^* \mathbf{1} + \boldsymbol{\delta}\right) \circ \left(\tilde{x}^* \mathbf{1} - \boldsymbol{\delta}\right) \circ \frac{\partial u^{\text{st}}}{\partial \boldsymbol{\delta}}, 
$$

(A57)

$$
\dot{\boldsymbol{\epsilon}} = -\left(y^* \mathbf{1} + \boldsymbol{\epsilon}\right) \circ \left(\tilde{y}^* \mathbf{1} - \boldsymbol{\epsilon}\right) \circ \frac{\partial u^{\text{st}}}{\partial \boldsymbol{\epsilon}}.
$$

(A58)

17

We can decompose  $\dot{\delta} \simeq \sum_{k=0} \dot{\delta}^{(k)}$  and  $\dot{\epsilon} \simeq \sum_{k=0} \dot{\epsilon}^{(k)}$  as

$$
\dot{\boldsymbol{\delta}}^{(0)} = +x^* \tilde{x}^* \frac{\partial u^{\text{st}(1)}}{\partial \boldsymbol{\delta}},
$$

(A59)

$$
\dot{\boldsymbol{\delta}}^{(1)} = +x^* \tilde{x}^* \frac{\partial u^{\text{st}(2)}}{\partial \boldsymbol{\delta}} - (x^* - \tilde{x}^*) \boldsymbol{\delta} \circ \frac{\partial u^{\text{st}(1)}}{\partial \boldsymbol{\delta}}, 
$$

(A60)

$$
\dot{\boldsymbol{\delta}}^{(2)} = +x^* \tilde{x}^* \frac{\partial u^{\text{st}(3)}}{\partial \boldsymbol{\delta}} - (x^* - \tilde{x}^*) \boldsymbol{\delta} \circ \frac{\partial u^{\text{st}(2)}}{\partial \boldsymbol{\delta}} + \boldsymbol{\delta} \circ \frac{\partial u^{\text{st}(1)}}{\partial \boldsymbol{\delta}}, 
$$

(A61)

$$
\dot{\boldsymbol{\delta}}^{(3)} = +x^* \tilde{x}^* \frac{\partial u^{\text{st}(4)}}{\partial \boldsymbol{\delta}} - (x^* - \tilde{x}^*) \boldsymbol{\delta} \circ \frac{\partial u^{\text{st}(3)}}{\partial \boldsymbol{\delta}} + \boldsymbol{\delta} \circ \boldsymbol{\delta} \circ \frac{\partial u^{\text{st}(2)}}{\partial \boldsymbol{\delta}}, 
$$

(A62)

$$
\dot{\epsilon}^{(0)} = -y^* \tilde{y}^* \frac{\partial u^{\text{st}(1)}}{\partial \epsilon}, 
$$

(A63)

$$
\dot{\boldsymbol{\epsilon}}^{(1)} = -y^* \tilde{y}^* \frac{\partial u^{\text{st}(2)}}{\partial \boldsymbol{\epsilon}} + (y^* - \tilde{y}^*) \boldsymbol{\epsilon} \circ \frac{\partial u^{\text{st}(1)}}{\partial \boldsymbol{\epsilon}}, 
$$

(A64)

$$
\dot{\boldsymbol{\epsilon}}^{(2)} = -y^* \tilde{y}^* \frac{\partial u^{\text{st}(3)}}{\partial \boldsymbol{\epsilon}} + (y^* - \tilde{y}^*) \boldsymbol{\epsilon} \circ \frac{\partial u^{\text{st}(2)}}{\partial \boldsymbol{\epsilon}} - \boldsymbol{\epsilon} \circ \boldsymbol{\epsilon} \circ \frac{\partial u^{\text{st}(1)}}{\partial \boldsymbol{\epsilon}}, 
$$

(A65)

$$
\dot{\epsilon}^{(3)} = -y^* \tilde{y}^* \frac{\partial u^{\text{st}(4)}}{\partial \epsilon} + (y^* - \tilde{y}^*) \epsilon \circ \frac{\partial u^{\text{st}(3)}}{\partial \epsilon} - \epsilon \circ \epsilon \circ \frac{\partial u^{\text{st}(2)}}{\partial \epsilon}. 
$$

(A66)

In cases of one-memory penny-matching games, the solution is obtained if we substitute

$$
x^* = \frac{1}{2} \mathbf{1}, \quad y^* = \frac{1}{2} \mathbf{1}, \quad p^* = \frac{1}{4} \mathbf{1}.
$$

(A67)

## B.3 Method to Calculate the Stationary State

Regarding **Section 5.1**, we use an analytical solution of the stationary state, which is known only in the case of two-action one-memory games as

$$
p_1^{\text{st}} = k\{(x_4 + (x_3 - x_4)y_3)(y_4 + (y_2 - y_4)x_2) - x_3y_2(x_2 - x_4)(y_3 - y_4)\},
$$

(A68)

$$
p_2^{\text{st}} = k\{(x_4 + (x_3 - x_4)y_4)(\tilde{y}_3 - (y_1 - y_3)x_1) - x_4\tilde{y}_1(x_1 - x_3)(y_3 - y_4)\},
$$

(A69)

$$
p_3^{\text{st}} = k\{(\tilde{x}_2 - (x_1 - x_2)y_1)(y_4 + (y_2 - y_4)x_4) - \tilde{x}_1 y_4 (x_2 - x_4)(y_1 - y_2)\},
$$

(A70)

$$
p_4^{\text{st}} = k\{(\tilde{x}_2 - (x_1 - x_2)y_2)(\tilde{y}_3 - (y_1 - y_3)x_3) - \tilde{x}_2\tilde{y}_3(x_1 - x_3)(y_1 - y_2)\},
$$

(A71)

$$
k = \frac{1}{p_1^{\text{st}} + p_2^{\text{st}} + p_3^{\text{st}} + p_4^{\text{st}}},
$$

(A72)

under the notation in Assumption 1.

In other parts (Sections  $5.2$  and  $5.3$ ), we calculate the stationary state of a Markov transition matrix  $\boldsymbol{M}$  by the power iteration method. Compared to the analytical solution  $\boldsymbol{p}^{\text{st}}$ , the computational solution  $\hat{\boldsymbol{p}}^{\text{st}}$  is accurate except for  $10^{-9}$  error in  $L^2$  norm (i.e., $\|\hat{p}^{st} - p^{st}\|_2 \leq 10^{-9}$).

18