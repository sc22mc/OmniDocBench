arXiv:2302.01301v1 [cs.LG] 2 Feb 2023

# MARLIN: Soft Actor-Critic based Reinforcement Learning for Congestion Control in Real Networks

Raffaele Galliera<sup>†, \*</sup>, Alessandro Morelli<sup>†</sup>, Roberto Fronteddu<sup>†</sup>, Niranjan Suri<sup>†,\*,‡</sup>

<sup>†</sup>*Florida Institute for Human & Machine Cognition (IHMC)* 

<sup>\*</sup>Department of Intelligent Systems & Robotics - The University of West Florida (UWF)

Pensacola, FL, USA

<sup>‡</sup>US Army Research Laboratory (ARL)

Adelphi, MD, USA

{rgalliera, amorelli, rfronteddu, nsuri}@ihmc.org

Abstract-Fast and efficient transport protocols are the foundation of an increasingly distributed world. The burden of continuously delivering improved communication performance to support next-generation applications and services, combined with the increasing heterogeneity of systems and network technologies, has promoted the design of Congestion Control (CC) algorithms that perform well under specific environments. The challenge of designing a generic CC algorithm that can adapt to a broad range of scenarios is still an open research question. To tackle this challenge, we propose to apply a novel Reinforcement Learning (RL) approach. Our solution, MARLIN, uses the Soft Actor-Critic algorithm to maximize both entropy and return and models the learning process as an infinite-horizon task. We trained MARLIN on a real network with varying background traffic patterns to overcome the sim-to-real mismatch that researchers have encountered when applying RL to CC. We evaluated our solution on the task of file transfer and compared it to TCP Cubic. While further research is required, results have shown that MARLIN can achieve comparable results to TCP with little hyperparameter tuning, in a task significantly different from its training setting. Therefore, we believe that our work represents a promising first step towards building CC algorithms based on the maximum entropy RL framework.

Index Terms—Computer Networks, Communications Protocol, Machine Learning, Congestion Control, Reinforcement Learning, Soft Actor-Critic.

# I. INTRODUCTION

Network communications are the backbone of an everincreasingly distributed digital world. Each day, technologies such as software, platform, and infrastructure as a service over Cloud/Edge/Fog computing systems, the Internet of Things, 5G networks, space communications and networks, peer-topeer networks, blockchain, ad-hoc and mesh networks enable countless organizations and people to run their business and perform tasks that have become part of our daily routine with just a tap on a screen. The different characteristics of those technologies, combined with the requirements of very diverse services built on top of them, present unique challenges for network protocol designers and researchers.

Transport protocols that can achieve high network resource utilization while maintaining congestion, i.e., endto-end queueing delays, low in presence of ever-changing network conditions and high churn of connections are the key enabling technology that underlies modern distributed

systems. Within transport protocol design, this dual goal (high channel utilization and low congestion) is the objective of Congestion Control (CC) algorithms. The critical impact of CC on the performance of distributed applications and services has led researchers and engineers to design many variants, each one with different target scenarios and trade-offs in terms of aggressiveness, responsiveness to loss and congestion, fairness, and friendliness [1, 2, 3]. However, the challenge of designing a generic CC algorithm that can provide nearoptimal performance in a broad range of network and traffic scenarios is still open.

Recent advances in computational capacity, made possible by novel CPU, GPU, and Application-Specific Integrated Circuit (ASIC) architectures, along with the availability of low-cost, yet powerful, versions of such hardware accelerators, have led to an explosion of successful applications of Machine Learning (ML) technology in several domains, coming to meet the requirements of resource constrained environments such as the edge [4]. This has sprung renewed research interest towards the development of ML models, Reinfocement Learning (RL) agents, algorithms, and applications that can face the unique challenges of the real-world  $[5, 6, 7, 8]$ .

The difficulty of designing an efficient generic CC algorithm, in contrast with the relative ease of collecting data on the communication performance and the small size of the action space, which typically focuses solely on adjusting the size of the Congestion Window (CWND), has prompt researchers to investigate the use of ML for CC optimization [9, 10]. While many studies have already shown promising results, they still underperform when compared to existing TCP CC algorithms in many scenarios. Reasons may include: difficulties to train RL agents and/or ML models using real networks and hardware; low fidelity of simulation and emulation environments; sub-optimal environment representation (state), action space, and/or reward function design; and delayed action effects on the state due to communications delay. Additional research is needed to address these problems.

This paper introduces our initial work on MARLIN, a RL agent for CC. Our approach is based on Soft Actor-Critic (SAC) [11], an off-policy, entropy-regularized RL algorithm, that has seen successful application in numerous real-world

©2023 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works.

problems, especially robotics [7]. We describe the design and architecture of MARLIN and discuss how we trained the agent, the hyperparameters used, and the main assumptions and choices we made during its design phase, comparing our work with other RL-based approaches to CC. Furthermore, we discuss the future directions of this research, of which this paper is just the first step. Finally, we present preliminary experimental results obtained in a real networking scenario, where we investigate the current performance of MARLIN and compare it to the Transmission Control Protocol (TCP) in handling a file transfer task.

# II. DESIGNING A REINFORCEMENT LEARNING AGENT FOR CONGESTION CONTROL

MARLIN is a RL agent for CC, trained in a real network by using continuous actions to update the CWND. For the purpose of this research, we did not consider fairness towards competing MARLIN flows and/or other protocols as one of the optimization objectives of MARLIN, which we leave as a future research question.

Within MARLIN, learning is based on SAC [11], which trains a stochastic, off-policy, entropy-regularized agent. The agent interfaces with a custom transport protocol, presented in Section III, via Remote Procedure Calls (RPCs) in a nonblocking, bi-directional fashion, as shown in Figure 1. The frequency of action taking depends on the latest Smoothed Round-Trip Time (SRTT) estimated for the path. The reward function is shaped as a strictly negative reward to intrinsically encode the principle that every step taken to transfer the information is a step too much.

Although it is generally desirable that the agent completes its tasks as quickly as possible, the RL setting proposed here does not aim for terminal states. It is inconvenient for the agent to overfit its trajectories on a fixed amount of bytes to transfer, or a fixed episode length, such that it will try to find a terminal state at any cost in a certain number of timesteps. Instead, we aim at designing RL agents able to work on "crowded" links and achieve high channel utilization independently from the volume of data transferred during training. For such reason, training is based on infinite-horizon tasks, never encountering a terminal state and following the Partial Episode Bootstrapping (PEB) principles for infinitehorizon training presented in [12].

## A. Congestion Control as Markov Decision Process

Traditional CC algorithms implemented within transport protocols such as TCP take actions, e.g., updating their CWND, in response to changes in the environment they sense with the goal of achieving maximum throughput and minimum congestion. Changes may include, for example, packet loss, duplicate packets, or variations in the measured Round-Trip Time (RTT). Such sequential decision making setting can be naturally posed as a Markov Decision Process (MDP) [13].

The MDP framework formalizes with  $\langle S, A, p, R \rangle$  the sequential agent-environment interaction as the exchange of three different signals at every time step  $t$  of the process. The

![](_page_1_Figure_8.jpeg)

Figure 1: Agent-Protocol interface. Communication protocol and agent communicate through a gRPC server.

RL "learning from interaction" fundamentals [\[14\]](#page-14-1) stem from this abstraction: the *action*  $a_t \in A(s_t)$  taken by the agent at a certain step  $t$  after sensing the state  $s_t \in S$  leads to a consequent response of the environment as a reward signal  $r_{t+1} \in R \subset \mathbb{R}$  and transitions to a new state  $s_{t+1}$ . This process follows the environment dynamics described by the probability function  $p(s_{t+1}, r_{t+1} | s_t, a_t) : S \times R \times S \times A \longrightarrow [0, 1].$  Such formulation of the transition dynamics also defines the *Markov* property, which requires the state to include all the relevant information about past interactions that would impact the future, as each possible values of  $r_t \in R$  and  $s_t \in S$  depend solely on the previous state and action  $s_{t-1}, a_{t-1}$ .

Communication networks are a subtle environment for a RL agent. Packets in modern networks are often transmitted a few milliseconds apart, sometimes microseconds. At the same time, running Stochastic Gradient Descent (SGD) to update the model during training, or even simply computing a new action, can take up a significantly greater amount of time. As a consequence, the environment might have already changed significantly by the time the action is taken, potentially making it obsolete or, even, wrong. This has been identified as one of the main reasons why it is hard to maintain the performance achieved in a simulated environment after moving to a realworld deployment [15].

Furthermore, actions do not instantly affect the agent's perception, as the effects of changing the CWND require some time before they have an impact on the network and that impact is propagated back to the source. As new packets are transmitted at a different rate after an action has been taken, they first need to reach their destination, then the receiver needs to generate and transmit a new ACKnowledge (ACK) message for those packets, and finally that ACK needs to arrive back at the source before any information about the impact of

![](_page_2_Figure_0.jpeg)

Figure 2: A dumbbell network is used for training and testing. Background and agent traffic are transmitted from one sub-network to the other over the bottleneck link. Senders and receivers are all connected to a WiFi Access Point.

the action taken can be inferred. These events will take, at the very least, one RTT to complete. This raises a fundamental question for anybody who wants to apply RL to the problem of CC: when should the next action be taken? While other similar approaches, like [16, 17], gather statistics during a fixed amount of time to compute the state and take the next action, MARLIN implements a different heuristic algorithm. Every time the transport protocol makes new network statistics available to MARLIN, if at least the last reported SRTT has elapsed since the time the last action was taken, data collected up to that time is refined, the reward  $r_t$  with regards to action  $a_{t-1}$  is assigned, a new state  $s_t$  is fed to the agent, and a new action  $a_t$  is taken. Waiting for at least one SRTT between actions prevents that a new action is taken before the impact of the previous one can be sensed by the agent.

## B. Training Environment Setup

A key design point of MARLIN is to be trained and evaluated on real networks. Utilizing real components allowed us to understand the challenges that the CC domain brings to RL, avoiding the risk of failing a "sim-to-real" transposition, which could be caused, by inaccurate emulated/simulated patterns and weak assumptions, at the cost of a slower training time. Relying on real hardware and protocol implementation intrinsically diversifies training experience thanks to the stochasticity of the environment and it prevents the agent from overfitting on artificial patterns that would not be found in the real world.

To this end, we built the network shown in Fig. 2, which comprises two sources of traffic and two receivers, two WiFi Access Point (AP), two network switches, and one router connected in a dumbbell topology. Traffic sources are connected to the WiFi AP located at one end of the dumbbell, while traffic receivers are connected to the AP at the other end. The "Background Traffic Source", on the left, is responsible for generating background and send it to the "Background Traffic Receiver" node, on the right. The "Sender Application" transmits traffic to the "Receiver Application" via a custom transport protocol implementation that uses MARLIN as its CC algorithm.

The need for implementation flexibility led us to integrate MARLIN with Mockets, a protocol originally designed for

communication environments characterized by limited bandwidth, typically found in tactical and wireless sensor networks. Further details regarding Mockets are presented in Section III. To avoid running into situations where MARLIN could underperform due to the protocol implementation, we shaped the network scenario around the typical Mockets use case for this first iteration. We set a Smart Queue policy on the router to limit the maximum amount of traffic flowing between the two subnetworks to 250 KB/s; we also used TC NetEm - Traffic Control Network Emulator (TC-NETEM) on the "Sender Application" node to introduce 100 ms of latency and 3% random packet loss that only affect the traffic generated by the application. Note that the router's manual reports that it effectively limits the actual rate to 95% of the specified value when Smart Queue is enabled [18].

The background traffic patterns generated can be divided into elephant flows, i.e., long-lived data transfers that represent a large percentage of the total traffic (imagine large file transfers or video streaming), and mice flows, i.e., shortlived data transfers at low throughput (e-mail or RPC calls). Roughly 87% of the background traffic in our testbed is made by elephant flows, consisting of four different flows that alternate each other over the link every two seconds, and two mice flows, which continuously generate very short-lived (in the order of milliseconds) traffic bursts with intervals that follow a Poisson distribution.

The Background Traffic Source generates traffic using Multi-Generator Network Test Tool (MGEN) [19]. Elephant flows consist of two UDP communications transferring data at 100 KB/s, one UDP communication transferring data at 50 KB/s, and one TCP connection producing 200 KB/s of traffic. One TCP and one UDP mice flow introduce an extra 17 KB of traffic in average over the link each second. Each elephant flow is assigned to a different temporal slot of a 2 second duration and they repeat every 8 seconds. Mice flows start with the first elephant flow and continue until traffic generation is stopped.

## C. Agent Design

The interface between MARLIN and the transport protocol, shown in Figure 1, is implemented using  $gRPC$ . The  $gRPC$ middleware sitting between the transport protocol and MAR-

|    | Feature                  | Description                       |   | Statistic                |
|----|--------------------------|-----------------------------------|---|--------------------------|
| 1  | Current CWND             | Current CWND                      | 1 | Last                     |
| 2  | KBs Sent                 | Amount of KB sent *               | 2 | Mean                     |
| 3  | New KBs sent             | Amount of KB acked *              | 3 | STD                      |
| 4  | Acked KBs                | Amount of KB acked *              | 4 | Min                      |
| 5  | Packets sent             | Packets sent *                    | 5 | Max                      |
| 6  | Retransmissions          | Number of packets retransmitted * | 6 | EMA                      |
| 7  | Instantaneous Throughput | Throughput *                      | 7 | Difference from Previous |
| 8  | Instantaneous Goodput    | Goodput *                         |   |                          |
| 9  | Unacked KBs              | Amount of KBs in flight           |   |                          |
| 10 | Last RTT                 | Last RTT detected *               |   |                          |
| 12 | Min RTT                  | Min RTT *                         |   |                          |
| 12 | Max RTT                  | Max RTT *                         |   |                          |
| 13 | SRTT                     | Smoothed RTT *                    |   |                          |
| 14 | VAR RTT                  | RTT variance *                    |   |                          |
|    |                          | * During the last RTT timeframe   |   |                          |

Table I: Features composing the state of the environment. The horizon of the observation is also augmented with the previous 10 observations.

LIN takes care of delivering observations and reward pairs to the agent and actions back to the protocol.

At a given step  $t$ , the agent receives network statistics from the transport protocol via the gRPC middleware. Statistics are then processed and stacked with the previous 10 observations to form the state  $s_t$ . The agent can take actions that will increase, maintain constant, or decrease the transport protocol CWND by a chosen factor. Ideally, the agent should learn to maximize the volume of data transmitted in the minimum amount of time, while being watchful of growing queueing delays, which would manifest with an increase in the measured RTT.

MARLIN is implemented on top of the RL Baselines3 Zoo (RL-Zoo3) [20] framework, which follows best practices for using Stable-Baselines3 (SB3) [21], a PyTorch [22]-based library that implements state-of-the-art RL algorithms following the OpenAI gym interface [23].

1) State: Table I describes the state encoded in MARLIN. 14 features are gathered during the time frame that follows the action taken at step  $t - 1$ . MARLIN then augments the state space with 7 statistics, i.e., last, mean, standard deviation, minimum, maximum, Exponential Moving Average (EMA), and difference from the previous state  $s_{t-1}$ , that are computed for each of the 14 features. The previous  $N$  states are also stacked together to form a history of the previous observations, attempting to adhere to the Markov property. The final state served to the agent will then have  $N \times |Features| \times |Stats|$  features. This totals up to 980 

N  is considered a hyperparameter of the problem; table II has a complete list of all hyperparameters used for MARLIN. For the choice of  $N$ , we followed the empirical considerations made in  $[24]$ , and chose 10 as the length of the history after some preliminary trials and evaluations. During the training process, observations are processed and normalized through a moving average by using the VecNormalize environment wrapper present in SB3.

2) Actions: MARLIN takes continuous actions contained in the range  $[-1, 1]$ , which represent the percentage gain of the CWND size. For example, if the action chosen by the agent is

going to be 1, the CWND is doubled; a value of  $0$  means no change in the CWND;  $-0.5$  reduces the window size by 50%. The initial CWND size is set to 4KB at the beginning of the episode, as per the transport protocol implementation default.

For the purpose of this study, we capped the CWND size to 50 KB, a value that, if reached, would yield double the throughput that the network can accommodate in the setup we used for training and evaluation. This choice only impacts the very first phases of training, when the agent takes random steps to prime the model. After this phase, which in MARLIN is set to last 10K steps, our experiments have shown that the agent correctly never fills the CWND to 50 KB.

3) *Reward:* We designed a reward function that gives higher rewards to the agent the closer it gets to fully utilizing the available bandwidth:

$$
r_t = -\frac{target_t}{target_t + acked\_kilobytes_t^{cumulative}} 
$$

(1)

where  $target_t$  represents the amount of bytes the agent  
should have delivered up to step  $t$  since the beginning  
of the episode in order to fully utilize the link and  
acked\_kilobytes $^{cumulative}_{t}$  represents the number of kilobytes  
there were acknowledged by the receiver until step  $t$ .

A strictly negative reward function promotes the agent to accumulate the smallest amount of penalties. The penalty received is much smaller the closer the agent it is, at each step, to having utilized the link to the best of its possibilities.

The careful reader might notice that such rewarding system encourages the agent to accumulate acked bytes regardless explicit impact on the RTT, falling into the risk of privileging actions that could produce more acked bytes in the immediate future, with the drawback of causing undesired queuing delays. To prevent such risk, we consider a second formulation of the reward function that introduces a RTT-based penalty coefficient:

$$
r_{t} = -\frac{target_{t}}{target_{t} + acked\_kilobytes_{t}^{cumulative} * (1 - penalties)}
$$

(2)

The term *penalties* depends on the difference between the current RTT and the minimum EMA RTT and is defined as follows:

$$
penalties = \begin{cases} \alpha \frac{rtt_{diff}}{rtt_{min}^{ema}}, & \text{if } \frac{rtt_{diff}}{rtt_{min}^{ema}} < 1\\ 0.99, & \text{otherwise} \end{cases}
$$

(3)
 
In Eq. 3,  $\alpha$  depends on the magnitude of the difference between  $rtt_{diff}$  and  $rtt_{min}^{ema}$  and it is defined as follows:

$$
\alpha = \begin{cases} 1, & \text{if } |\frac{rtt_{diff}}{rtt_{\min}^{ema}}| > 0.6 \\
0.5, & \text{if } 0.1 < |\frac{rtt_{diff}}{rtt_{\min}^{ema}}| \le 0.6 \\
0.3, & \text{if } 0.05 < |\frac{rtt_{diff}}{rtt_{\min}^{ema}}| \le 0.1 \\
0.1, & \text{otherwise} \end{cases} 
$$

(4)

It is worth noticing that, in case  $rtt_{diff} < 0$ , the *penalties* term becomes negative, thus rewarding the agent when RTT improvements are detected.

4) RL Algorithm: MARLIN adopts SAC [11], an off-policy actor-critic algorithm based on the maximum entropy RL framework. SAC augments the maximum reward objective, foundation of RL settings, with an entropy maximization term. Such term acts as a trade-off between exploration and exploitation, so that the agent aims to maximize its return while also acting as random as possible. In circumstances where multiple actions seem equally attractive, i.e. in the case of equal or close Q-Values, the learned policy is encouraged to assign equal probability mass to those actions.

In practice, the effects of the entropy, or temperature, term prompt the agent to discard unsuitable trajectories in favor of more promising ones, as well as to improve the learning speed. The entropy term can be either fixed or optimized/learned as further steps are taken. However, the optimal entropy coefficient varies depending on a series of factors, such as the nature of the task or even the current policy. As a consequence, it is usually considered good practice to avoid fixed values, preferring instead to update the term at the same time actor, critic, and the target networks are optimized [7].

## D. Future Directions

The work described above is the first step of a multi-year research project. We have already identified future steps that follow naturally from our present work. While we proceed describing them sequentially, in reality these steps are much intertwined and we will likely addressed them in subsequent iterations.

We think that MARLIN would benefit from a more expressive reward function. We envision problem and reward formulations that truncate unpromising trajectories that have moved too distant from the optimal. We believe this could significantly speed up the solution convergence. Furthermore, we suspect that MARLIN's current state might present redundant information as well as features of low relevance to the problem. To address this, we plan to investigate smaller and more refined state representations, with the double goal of lowering complexity and improving convergence. We plan

| Hyperparameter                     | Value                       |
|------------------------------------|-----------------------------|
| Training steps                     | $1 \times 10^{6}$           |
| History length                     | 10                          |
| Training episode length            | 200                         |
| Learning rate                      | $3 \times 10^{-4}$          |
| Buffer size                        | $5 \times 10^5$             |
| Warm-up (learning starts)          | $1 \times 10^4$ steps       |
| Batch size                         | 512                         |
| Tau                                | $5 \times 10^{-3}$          |
| Gamma                              | 0.99                        |
| Training Frequency                 | 1/episode                   |
| Gradient Steps                     | -1 (same as episode length) |
| Entropy regularization coefficient | "auto" (Learned)            |
| MLP policy hidden layers           | [400, 300]                  |

Table II: Hyperparameters used in MARLIN.

to train this new agent in diversified networking scenarios, which can capture different traffic patterns and network technologies, to assess the degree of generalizability. Finally, a thorough, automated hyperparameter tuning would further enhance MARLIN's performance and complete a first cycle of improvements.

Further advancements will require an evolution in the agent's design congruent with specific problems that afflict CC. The literature has shown that ML models can accurately distinguish between packet loss attributable to congestion or channel errors. We plan to integrate a similar classifier within MARLIN and investigate the feasibility of an analogous approach to identify variations in end-to-end latency that are caused by changes in the path to destination. The fundamental building block of CC algorithms is *how and when* to change the CWND size. MARLIN currently actively controls the "how", leaving the "when" to a heuristic. We will investigate learning-based approaches to include such decision factor into MARLIN, with the goal of turning it into a more comprehensive and reactive system, able to make rational decisions at its own tempo.

Up until now, we shaped the problem of CC from the perspective of a single RL agent. Nonetheless, the need for CC algorithms in transport protocols originated from the lack of coordination in a multi-agent system, where single entities were acting in a completely self-centered manner. Therefore, we expect that the next step ahead in learning-based CC will come from the application of advancing Multi-Agent Reinforcement Learning (MARL) algorithms that can optimize cooperative and/or competitive agent behavior.

# III. THE MOCKETS TRANSPORT PROTOCOL

One of the main design choices we faced was the transport protocol we would use to train and evaluate MARLIN. The decision fell on a custom transport protocol because it is much simpler to integrate with MARLIN than TCP. Additionally, a custom protocol enables greater flexibility in terms of retrieving the information required to encode the agent state.

For this study, we integrated MARLIN with Mockets, a message-based communication middleware implemented on top of UDP, following a school of thought similar to the one

![](_page_5_Figure_0.jpeg)

Figure 3: The rolling averaged CWND size is plotted against the time from the beginning of the episode, while segment color represents the RTT observed between two actions. This plot was obtained from the best run of the second MARLIN model, i.e. 5b, which completed the transfer in 24.84s.

that drove the design of the QUIC protocol [25] (for additional details on Mockets, the reader can refer to [26]). Mockets implements a very aggressive CC algorithm whose purpose is to fully utilize the available bandwidth in presence of variable communication latency and elevated packet loss. While it presents several advantages over TCP and other transport protocols in degraded environments [27], Mockets' existing CC fails to share link capacity with other communication flows going through the same links and cannot adapt quickly to changes in the available bandwidth. Therefore, Mockets could benefit significantly from MARLIN, which could make it a viable choice also for traditional network scenarios.

To have an accurate representation of the state of the environment within MARLIN, we improved the RTT estimation in Mockets. To do so, the sender keeps track of the transmission time of each packet. When an ACK arrives, the sender computes the RTT by calculating the difference between the current time and the transmission time of the last packet received by the receiver before generating the ACK message and then subtracting the ACK processing time from it. To make this calculation possible, the receiver appends the identifier of the last received packet (a strictly increasing unsigned integer) and the processing time (in microseconds) to all ACK messages.

# IV. EXPERIMENTAL RESULTS

## A. Hardware Involved

The testbed described in Figure 2 is implemented using Commercial Off-The-Shelf (COS) devices. The agent is trained on a workstation equipped with an Intel i7-8700 CPU, 32GB DDR4 RAM, an NVIDIA GeForce RTX 2060 GPU, and an Intel Wireless-AC 9560 Network Interface Card (NIC). The "Receiver Application" resides on a 4GB Raspberry Pi 4 Model B (RPi4B). The two AP used are a TP-Link EAP245 and a Netgear WAC505, connected to two Netgear GS108E switches. A 2015 Apple MacBook Pro with macOS Catalina v10.15.6 generates background traffic, which is sent to the receiving application running on a Dell Latitude 7000 laptop (E7470). Both the desktop machine and E7470 run the Ubuntu 20.04.5 LTS OS. All NIC-related optimizations, e.g., TCP Segmentation Offload (TSO) or Large Receive Offload

(LRO), are enabled, as per default, on all systems. A Ubiquiti Networks EdgeRouter X with EdgeOS routes packets between the two subnetworks.

## B. Training

The agent is trained for 1M steps on an infinite-horizon task with partial episodes lasting 200 steps. When the last step of a partial episode is encountered, a *truncated* signal is emitted and PEB is performed, recalling the agent that additional steps and rewards would actually be available thereafter [12].

Due to time constraints, the optimization steps involved (for actor, critic, target networks, and learned entropy coefficient) happen at the end of every partial episode. Taking training steps between two actions on a real network is a luxury we cannot afford, as we observed it would introduce delays in the order of 800-1000ms. Training at the end of an episode allows us to avoid such effect, limiting the overhead to the sole pipeline, which we report being around 25 to 30 ms.

At the beginning of training, in order to increase exploration, actions coming from a uniform random distribution are taken for 10K steps. Subsequently, SAC starts its normal explorationexploitation strategy, led by its entropy coefficient.

We trained three different models for MARLIN. For the first one, we used Eq  $1$  (5a). For the second model, we used the reward function in Eq.  $2$  (5b). In both cases, the background traffic pattern was fixed, with elephant flows following this order: [100 UDP, 200 TCP, 100 UDP, 50 UDP]. For the last model (5c), we changed the background traffic to use all permutations presented in Section II-B.

The mean training reward shows a promisingly increasing trend in each of the three setting, as shown in Figure 4. It is straightforward to interpret the performance showed during training by taking as a reference our ideal target, which, in this case, would result in -100 mean training reward.

## C. Results

Following training, we tested all the three MARLIN models by transferring a 3MB file on the same network with the background traffic pattern [100 UDP, 200 TCP, 100 UDP, 50 UDP]. It is important to note that file transfer is a significantly different problem from the one that the agent faces during

![](_page_6_Figure_0.jpeg)

Figure 4: Mean training reward over the last 100 partial episodes on the three training runs.

training: here, the agent is evaluated on completing a task longer than the partial episodes seen during training, which are limited to 200 steps, and a faster completion coincides better results.

We repeated the experiment 100 times for each of the three trained agents. Results are then compared to the performance of Secure Copy Protocol (SCP), which in our system uses TCP CUBIC as the transport protocol, on the same file transfer task. Figure 5 shows the optimal behaviour, computed from the a-priori knowledge of the background traffic, along with the batch of results obtained from the trained agents. The average, best, and worst performance of TCP CUBIC are also shown under the same conditions after having repeated the transfer 100 times as well. Figure 3 presents the variation of the CWND size during MARLIN's best run across all the three experiments. The color of each segment represents the RTT of the communication measured during a period of one SRTT immediately following the action that took the CWND size to the value represented by the rightmost end of the segment.

Runs with significant performance degradation were aborted after 80 seconds, with 5a reporting 4 aborted experiments, 5b reporting 20, and 5c reporting 2. TCP CUBIC completes the file transfer in 45.03s in average, with its fastest transfer achieved in 33.97s, and a worst performance of 70.78s. Excluding aborted runs, MARLIN reached the target in 46.1s in average in 5a, 50.31s in 5b, and 46.9s in 5c. All agents' best performance was faster than TCP CUBIC's best. In 5a,  $48\%$  of the experiments achieve a performance equal or better than the average accomplished by TCP CUBIC, 31% in 5b, and 47% in 5c. The fastest transfer was completed by 5b in 24.84s, 27% faster than the fastest SCP transfer and 45% faster than its average. For comparison, the available bandwidth on the link allows file transfers to be completed in 25s (see Figure 5); faster transfers can still occur if the traffic injected by MARLIN causes other flows to slow down.

## D. Discussion

Despite the increased decision making complexity brought by continuous actions in a problem as intricate as CC in a 

real network and a training budget of 1M steps, which is modest when compared to other RL-based CC agents, results are promising and already comparable to TCP CUBIC in our environment. We believe that part of it is due to the sample efficiency of SAC. Another reason can be found in the shaping of the CC problem as an infinite-horizon task with strictly negative rewards, which enables the agent to exploit its acquired experience in tasks longer than the partial episodes seen during training and with different goals.

Permuting the order of the background traffic patterns during training did not deteriorate the performance during evaluation. In fact, evaluation runs exhibited lower variance, a better fastest transfer, and similar average transfer time compared to the experiments shown in 5a. Moreover, fewer experiments had to be aborted. These results suggest that MARLIN's robustness might be further improved by feeding data from diverse scenarios to the algorithm.

Figure 3 shows the CWND size during one of the evaluation runs. Note that most of the time, following an increased RTT reading, the agent has learned to respond by reducing the CWND size or decreasing its growth speed in the next step. This behavior is compatible with many RTT-based CC algorithms.

Finally, although most trajectories obtained during evaluation express promising and valuable results (Figure 5), they also present a significant degree of instability, which warrants additional research. This behavior is particularly evident in 5b, where the model is trained using the function in Eq. 2. This model has had several transfers significantly faster than the ones in 5a and 5c, as well as those obtained with SCP; nonetheless, the model has proved considerably slower in average.

# V. RELATED WORK

Research efforts on the design and optimization of TCP CC have historically been very different from the approach discussed in this paper. Traditional CC algorithms aim at achieving full bottleneck link utilization by applying diverse heuristic strategies that increase the CWND size until a congestion sign emerges, such as a packet loss, e.g., NewReno [28] and CUBIC [29] (the default CC algorithm in modern Linux Kernels and recent Windows operating systems), or an increase in latency, e.g., Vegas [30] and, more recently, BBR [31]. These approaches have been shown to work well under specific network conditions, but underperform or experience other types of issues in other scenarios  $[32, 33, 34]$ .

Due to the exceptional degree of heterogeneity, complexity, and intrinsic dynamism of networking environments, and following the reinvigorated interest in ML that captivated the scientific community in the last two decades, several recent efforts have focused on learning-based transport protocol optimization techniques. Some approaches focus on addressing very specific problem of transport protocols, such as accurately identifying congestion events, but do not aim at replacing CC algorithms. For instance, researchers have successfully built ML-based models that can distinguish between losses caused

by congestion and losses due to channel errors in wireless networks [35, 36] or losses caused by medium contention in optical burst switching networks [37]. While we expect that our approach would benefit from such solutions, as discussed above, the final goal of MARLIN is to train an agent that can take on the tasks of CC algorithms efficiently in a number of different network scenarios.

More interesting studies seek to apply ML models and RL agents directly to CC, by learning policies that can optimally adjust the sending rate [38], the CWND [39], or other parameters that tune the CC algorithm [40]. Comprehensive surveys of applications of ML to CC are given in [9, 10]. Common problems that emerged from the reviewed studies include parameter selection, computational complexity, high memory consumption, low training efficiency, and compatibility and fairness against existing CC heuristics.

The possibility of training a RL agent in a completely autonomous manner makes the case for applying RL to CC optimization very compelling. The blocking nature of widely used RL libraries such as OpenAI Gym, designed to solve RL problems such as the ATARI games [41], which can be blocked while waiting for the next action, constitutes one of the main challenges. Researchers have circumvented this obstacle by training agents in simulated environments and achieved very promising results, such as with DRL-CC [42] and Aurora [43]. However, reproducing those results in real-world networks has proved to be a challenge. Solutions that rely on blocking while the agent computes the next action introduce significant delays, which are detrimental to the overall communication performance, especially in fast networks. In contrast, we designed MARLIN to integrate asynchronously with Mockets, which avoids blocking the transport protocol waiting for the agent to take the next action.

Other researchers propose the usage of non-blocking agents to optimize CC. MVFST-RL [16] proposed a non-blocking agent based on IMPALA [44], a  $C++$  implementation of the QUIC transport protocol, and Pantheon [39] for network emulation. Communication between the agent and the system work in a similar fashion to Park [17], a platform for experimenting with RL agents on computer system problems based on RPCs. MARLIN follows a similar philosophy to avoid the drawbacks of blocking systems. However, to the best of our knowledge, MARLIN, is the only work that uses strictly negative rewards and investigates the use of an off-policy and entropy-regularized RL algorithm, such as SAC, with a continuous action space, trained on a real network in which real background traffic flows compete for bandwidth access. Additionally, we trained MARLIN on a infinite-horizon setting and evaluated the model on a common real-world problem such as transferring a file over a shared link.

# VI. CONCLUSION

This work has shown how effective policies can be obtained by training a RL agent based on a off-policy, entropyregularized algorithm such as SAC. MARLIN shapes the CC problem as a strictly negative rewarded task actuating on

continuous-actions in a real network with competing dynamic background traffic.

We have also presented future research directions that we plan to pursue. These include training in more heterogeneous environments, exploring MARL settings, investigating more expressive reward functions, and designing an agent able to autonomously decide when to take the next action.

![](_page_7_Figure_8.jpeg)

(a) Agent trained on a single traffic pattern.

![](_page_7_Figure_10.jpeg)

(b) Agent trained on a single traffic pattern with RTT penalties.

![](_page_7_Figure_12.jpeg)

(c) Agent trained on every permutation of the traffic flows.

Figure 5: Evaluation of each model on 100 testing experiments.

 REFERENCES

- [1] Josip Lorincz, Zvonimir Klarin, and Julije Ožegović. "A Comprehensive Overview of TCP Congestion Control in 5G Networks: Research Challenges and Future Perspectives". In: Sensors 21.13 (2021). ISSN: 1424-8220. DOI: 10.3390/s21134510. URL: https://www.mdpi.com/1424-8220/21/13/4510.
- [2] Shiyao Ma, Jingjie Jiang, Wei Wang, and Bo-chen Li. "Fairness of Congestion-Based Congestion Control: Experimental Evaluation and Analysis". In: arXiv: Networking and Internet Architecture (2017).
- [3] J. Widmer, R. Denda, and M. Mauve. "A survey on TCP-friendly congestion control". In: IEEE Network 15.3 (2001), pp. 28-37. DOI: 10.1109/65.923938.
- [4] Raffaele Galliera and Niranjan Suri. "Object Detection at the Edge: Off-the-shelf Deep Learning Capable Devices and Accelerators". In: Procedia Computer Science 205 (2022). 2022 International Conference on Military Communication and Information Systems (ICMCIS), pp. 239–248. ISSN: 1877-0509. DOI: https://doi.org/10.1016/j.procs.2022.09.025. URL: https://www.sciencedirect.com/science/article/pii/S1877050922008900. Advances
- [5] Nikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to Walk in Minutes Using Massively Parallel Deep Reinforcement Learning. 2021. DOI: 10.48550/ARXIV.2109.11978. URL: https://arxiv.org/abs/2109.11978.
- [6] OpenAI: Marcin Andrychowicz et al. "Learning dexterous in-hand manipulation". In: The International of Robotics Journal Research 39.1  $(2020),$ рр. 3-20. DOI: 10.1177/0278364919887447. eprint: https://doi.org/10.1177/0278364919887447. URL: https://doi.org/10.1177/0278364919887447.
- [7] Tuomas Haarnoja et al. *Soft Actor-Critic Algorithms and* Applications. 2018. DOI: 10.48550/ARXIV.1812.05905. URL: https://arxiv.org/abs/1812.05905.
- [8] Tobias Jacob., Raffaele Galliera., Muddasar Ali., and Sikha Bagui. "Marine Vessel Tracking using a Monocular Camera". In: Proceedings of the 2nd International Conference on Deep Learning Theory and Applications - DeLTA, INSTICC. SciTePress, 2021, pp. 17–28. ISBN: 978-989-758-526-5. DOI: 10.5220/0010516000170028.
- [9] Wenting Wei, Huaxi Gu, and Baochun Li. "Congestion Control: A Renaissance with Machine Learning". In: IEEE Network 35 (4 July 2021), pp. 262–269. ISSN: 1558156X. DOI: 10.1109/MNET.011.2000603.
- [10] Huiling Jiang et al. When machine learning meets congestion control: A survey and comparison. June 2021. DOI: 10.1016/j.comnet.2021.108033.
- [11] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor". In: CoRR abs/1801.01290 (2018). arXiv: 1801.01290. URL: http://arxiv.org/abs/1801.01290.

- [12] Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. "Time Limits in Reinforcement Learning". In: (2017). DOI: 10.48550/ARXIV.1712.00378. URL: https://arxiv.org/abs/1712.00378.
- [13] Martin L. Puterman. Markov Decision Processes: Dis-crete Stochastic Dynamic Programming. 1st. USA: John Wiley & Sons, Inc., 1994. ISBN: 0471619779.
- [14] Richard S. Sutton and Andrew G. Barto. *Reinforcement*   Learning: An Introduction. Cambridge, MA, USA: A Bradford Book, 2018. ISBN: 0262039249.
- [15] Lei Zhang et al. "Reinforcement Learning Based Congestion Control in a Real Environment". In: 2020 29th International Conference on Computer Communications and Networks (ICCCN). 2020, pp. 1-9. DOI: 10.1109/ICCCN49398.2020.9209750.
- [16] Viswanath Sivakumar et al. "MVFST-RL: An Asynchronous RL Framework for Congestion Control with Delayed Actions". In: (Oct. 2019). URL: http://arxiv.org/abs/1910.04054.
- [17] Hongzi Mao et al. "Park: An Open Platform for Learning-Augmented Computer Systems".Advances in Neural Information Pro-Wallach et al. Systems. Ed. by H. cessing Vol. 32. Curran Associates, Inc., 2019. URL: https://proceedings.neurips.cc/paper/2019/file/f69e505b08403ad229
- [18] Ubiquiti EdgeOS. URL: https://dl.ubnt.com/guides/edgemax/EdgeOS\_UG.pdf.
- [19] Naval Research Laboratory (NRL) PROTocol Engi-neering Advanced Networking (PROTEAN) Research Group. Multi-Generator (MGEN) Network Test Tool. https://www.nrl.navy.mil/Our-Work/Areas-of-Research/Information 2021.
- [20] Antonin Raffin.  $RL$ Baselines3 Zoo. https://github.com/DLR-RM/rl-baselines3-zoo. 2020.
- [21] Antonin Raffin et al. "Stable-Baselines3: Reliable Reinforcement Learning Implementations". In: Journal of Machine Learning Research 22.268 (2021), pp. 1–8. URL: http://jmlr.org/papers/v22/20-1364.html.
- [22] Adam Paszke et al. "PyTorch: An Imperative Style, High-Performance Deep Learning Library". In: Proceedings of the 33rd International Conference on Neural Information Processing Systems. Red Hook, NY, USA: Curran Associates Inc., 2019.
- [23] Greg Brockman et al. *OpenAI Gym.* 2016. eprint: arXiv:1606.01540.
- [24] Nathan Jay, Noga Rotman, Brighten Godfrey, Michael Schapira, and Aviv Tamar. "A Deep Reinforcement Learning Perspective on Internet Congestion Control". In: Proceedings of the 36th International Conference on Machine Learning. Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Vol. 97. Proceedings of Machine Learning Research. PMLR, Sept. 2019, pp. 3050-3059. URL: https://proceedings.mlr.press/v97/jay19a.html.
- [25] Martin Thomson Jana Iyengar. OUIC:  $A$ UDP-Based Multiplexed and Secure Trans-

port.  $RFC$ 9000. IETF, Feb. 2022. URL: https://datatracker.ietf.org/doc/rfc9000/.

- [26] Erika Benvegnù, Niranjan Suri, Mauro Tortonesi, and Tomás Esterrich. "Seamless network migration using the Mockets communications middleware". In: 2010 - MILCOM 2010 MILITARY COMMUNICA-TIONS CONFERENCE. 2010, pp. 2298-2303. DOI: 10.1109/MILCOM.2010.5680364.
- [27] Alessandro Morelli, Michel Provosty, Roberto Fronteddu, and Niranjan Suri. "Performance Evaluation of Transport Protocols in Tactical Network Environments". In: MILCOM 2019 2019  $IEEE$ Military Communications *Conference* (*MILCOM*). 2019, pp. 30–36. DOI: 10.1109/MILCOM47813.2019.9021047.
- [28] Andrei Gurtov, Tom Henderson, Sally Floyd, and  $The$ NewReno Modification Yoshifumi Nishida. TCP's Fast Recovery Algorithm. RFC 6582. to Apr. 2012. DOI: 10.17487/RFC6582. URL: https://www.rfc-editor.org/info/rfc6582.
- [29] Sangtae Ha, Injong Rhee, and Lisong Xu. "CUBIC: A New TCP-Friendly High-Speed TCP Variant". In: SIGOPS Oper. Syst. Rev. 42.5 (July 2008), pp. 64-74. ISSN: 0163-5980. DOI: 10.1145/1400097.1400105. URL: https://doi.org/10.1145/1400097.1400105.
- [30] Lawrence S. Brakmo, Sean W. O'Malley, and Larry L. Peterson. "TCP Vegas: New Techniques for Congestion Detection and Avoidance". In: SIGCOMM. 1994.
- Neal Cardwell, Yuchung Cheng, C. Stephen Gunn, [31] Soheil Hassas Yeganeh, and Van Jacobson. "BBR: Congestion-Based Congestion Control". In: ACM Queue 14, September-October (2016), pp. 20–53. URL: http://queue.acm.org/detail.cfm?id=3022184.
- [32] Philipp Bruhn, Mirja Kuehlewind, and Maciej Muehleisen. "Performance and Improvements TCP CUBIC of in Low-Delay Cellular Networks". In: 2022 IFIP Networking Conference  $(IFIP$ Networking). 2022, 1–9. DOI: pp. 10.23919/IFIPNetworking55013.2022.9829781.
- [33] Marko Šošić and Vladimir Stojanović. "Resolving poor TCP performance on high-speed long distance links — Overview and comparison of BIC, CUBIC and Hybla". In: 2013 IEEE 11th International Symposium on Intelligent Systems and Informatics (SISY). 2013, pp. 325–330. DOI: 10.1109/SISY.2013.6662595.
- [34] Phuong Ha, Minh Vu, Tuan-Anh Le, and Lisong Xu. "TCP BBR in Cloud Networks: Challenges, Analysis, and Solutions". In: 2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS). 2021, pp. 943-953. DOI: 10.1109/ICDCS51616.2021.00094.
- [35] Kimoon Han, Jae Yong Lee, and Byung Chul Kim."Machine-Learning based Loss Discrimination Algorithm for Wireless TCP Congestion Control". In: 2019 International Conference on Electronics, Information,

and Communication (ICEIC). 2019, pp. 1–2. DOI: 10.23919/ELINFOCOM.2019.8706382.

- [36] P. Geurts, I. El Khayat, and G. Leduc. "A machine learning approach to improve congestion control over wireless computer networks". In: Fourth IEEE International Conference on Data Mining (ICDM'04). 2004, рр. 383-386. DOI: 10.1109/ICDM.2004.10063.
- [37] A. Jayaraj, T. Venkatesh, and C. Siva Ram Murthy. "Loss classification in optical burst switching networks using machine learning techniques: improving the performance of TCP". In: IEEE Journal on Selected Areas in Communications 26.6 (2008), pp. 45-54. DOI: 10.1109/JSACOCN.2008.033508.
- Mo Dong, Qingxi Li, Doron Zarchy, P. Brighten God-[38] frey, and Michael Schapira. "PCC: Re-Architecting Congestion Control for Consistent High Performance". In: Proceedings of the 12th USENIX Conference on Networked Systems Design and Implementation. NSDI'15. Oakland, CA: USENIX Association, 2015, pp. 395–408. ISBN: 9781931971218.
- [39] Francis Y. Yan et al. "Pantheon: the training ground for Internet congestion-control research". In: 2018 USENIX Annual Technical Conference (USENIX ATC 18). Boston, MA: USENIX Association, July 2018, pp. 731–743. ISBN: 978-1-939133-01-4. URL: https://www.usenix.org/conference/atc18/presentation/yan-francis.
- [40] Keith Winstein and Hari Balakrishnan.  $"TCP$ Machina: Computer-Generated Congestion Ex Control". In: SIGCOMM Comput. Commun. Rev. 43.4 (Aug. 2013), pp. 123–134. ISSN: 0146-4833. 10.1145/2534169.2486020. DOI: URL: https://doi.org/10.1145/2534169.2486020.
- [41] Volodymyr Mnih et al. "Playing Atari with Deep Reinforcement Learning". In: CoRR abs/1312.5602 (2013). arXiv: 1312.5602. URL: http://arxiv.org/abs/1312.5602.
- Zhiyuan Xu, Jian Tang, Chengxiang Yin, Yanzhi Wang, [42] and Guoliang Xue. "Experience-Driven Congestion Control: When Multi-Path TCP Meets Deep Reinforcement Learning". In: IEEE Journal on Selected Areas in Communications 37.6 (2019), pp. 1325-1336. DOI: 10.1109/JSAC.2019.2904358.
- Nathan Jay, Noga Rotman, Brighten Godfrey, Michael [43] Schapira, and Aviv Tamar. "A Deep Reinforcement Learning Perspective on Internet Congestion Control". In: Proceedings of the 36th International Conference on Machine Learning. Ed. by Kamalika Chaudhuri and Ruslan Salakhutdinov. Vol. 97. Proceedings of Machine Learning Research. PMLR, Sept. 2019, pp. 3050–3059. URL: https://proceedings.mlr.press/v97/jay19a.html.
- [44] Lasse Espeholt et al. "IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures". In: *CoRR* abs/1802.01561 (2018). arXiv: 1802.01561. URL: http://arxiv.org/abs/1802.01561.