1

# Cutting-edge 3D Medical Image Segmentation Methods in 2020: Are Happy Families All Alike?

Jun Ma

**Abstract**—Segmentation is one of the most important and popular tasks in medical image analysis, which plays a critical role in disease diagnosis, surgical planning, and prognosis evaluation. During the past five years, on the one hand, thousands of medical image segmentation methods have been proposed for various organs and lesions in different medical images, which become more and more challenging to fairly compare different methods. On the other hand, international segmentation challenges can provide a transparent platform to fairly evaluate and compare different methods. In this paper, we present a comprehensive review of the top methods in ten 3D medical image segmentation challenges during 2020, covering a variety of tasks and datasets. We also identify the "happy-families" practices in the cutting-edge segmentation methods, which are useful for developing powerful segmentation approaches. Finally, we discuss open research problems that should be addressed in the future. We also maintain a list of cutting-edge segmentation methods at https://github.com/JunMa11/SOTA-MedSeg.

Index Terms—Image Segmentation, Deep Learning, U-Net, Convolutional Neural Networks, Survey, Review.

# INTRODUCTION 1

MEDICAL image segmentation aims to delineate the interested anatomical structures, such as tumors, organs, and tissues, in a semi-automatic or fully automatic way, which has many applications in clinical practice, such as radiomic analysis [1], treatment planning [2], and survival analysis [3], and so on. Currently, medical image segmentation is also an active research topic. Figure 1 presents the word cloud of the paper titles in the 23rd International Conference and Medical Image Computing & Computer Assisted Intervention (MICCAI  $2020$ )<sup>1</sup> that is the largest international event in medical image analysis community. It can found that the term 'segmentation' has very high frequency and putting the top high-frequency words together can form a meaningful phase "image segmentation using  $deep \ learning / network(s)''$ .

![](_page_0_Figure_7.jpeg)

Fig. 1: Word cloud of the paper titles in MICCAI 2020.

Since U-Net [4], the legend medical image segmentation approach, appeared in 2015, there has been numerous new segmentation methods have been proposed for various segmentation tasks [5], [6] in the past five years<sup> $2$ </sup>. With so many segmentation papers on hand, it becomes extremely hard to compare them and identify the methodology progress, because the proposed methods are usually evaluated on different datasets with different dataset splits, metrics, and implementations.

Public segmentation challenges provide a standard platform of getting insights into the current cutting-edge approaches where solutions are evaluated and compared against each other in a transparent and fair way. In MIC-CAI 2020, there are totally ten international 3D medical image segmentation challenges<sup>3</sup>. All these challenges follow the Biomedical Image Analysis ChallengeS (BIAS) Initiative [7]. Specifically, the challenge designs are transparent and standardized, and the proposals (http://miccai.org/ events/challenges/) also have passed the peer review.

Table 1 provides an overview of the 10 segmentation challenges, which can be roughly divided into

- five single-modality image segmentation tasks, including three CT image segmentation tasks and two MR image segmentation tasks;

- five multi-modality image segmentation tasks, in-٠ cluding two bi-modality tasks, two triple-modality tasks, and one four-modality task.

In this paper, we first provide a comprehensive review of the ten 3D medical segmentation challenges and the associated top solutions. we also identify the "happy-families" elements in the top solutions. Finally, we highlight some problems and potential future directions for medical image segmentation.

Jun Ma was with the Department of Mathematics, School of Science, Nanjing University of Science and Technology, Nanjing, China, 210094.

*Manuscript was finished on December 31, 2020. Comments are welcome.* 

<sup>1.</sup> https://miccai2020.org/en/

<sup>2.</sup> U-Net [4] has more than 20000 citations by December, 2020.

<sup>3.</sup> https://www.miccai2020.org/en/MICCAI-2020-CHALLENGES.html

2

TABLE 1: Task overview of ten 3D medical image segmentation challenges. 'Seg. Targets' denotes segmentation targets in each task; '# Class and # Train/Val./Test' denote the number of class and the number of cases in training set, validation set, and testing set, respectively. '-' denotes no validation cases. '+' denotes that additional segmentation metrics (except DSC and HD) are used to evaluate the solutions of challenge participants.

| Name    | Seg. Targets                                              | # Class | # Train/Val./Test | Modality               | Multi-Center | Metrics    |
|---------|-----------------------------------------------------------|---------|-------------------|------------------------|--------------|------------|
| CADA    | Cerebral Aneurysm                                         | 1       | 92/-/23           | CT                     |              | IoU, HD, + |
| ASOCA   | Coronary arteries                                         | 1       | 40/-/20           | CT                     |              | DSC, HD    |
| VerSeg  | Vertebra                                                  | 28      | 100/-/200         | CT                     | Y            | DSC, HD    |
| MMs     | Heart4                                                    | 3       | 150(+25)/-/200    | MR                     | Y            | DSC, HD, + |
| EMIDEC  | Myocardium, infraction, re-flow                           | 3       | 100/-/50          | MR                     |              | DSC, HD, + |
| ADAM    | Intracranial aneurysm                                     | 1       | 113/-/140         | TOF-MRA, structural MR |              | DSC, HD, + |
| HECKTOR | Head/neck tumor                                           | 1       | 203/-/46          | PET, CT                | Y            | DSC        |
| MyoPS   | Scar, edema                                               | 2       | 25/-/20           | LGE, T2, bSSFP         |              | DSC        |
| ABCs    | Task 1: 5 brain structures<br>Task 2: 10 brain structures | 5<br>10 | 45/15/15          | CT, T1, T2             | Y            | DSC, SDSC  |
| BraTS   | Brain tumor5                                              | 3       | 369/125/166       | Flair, T1, T1ce, T2    | Y            | DSC, HD    |

The main contributions of this paper are summarized as follows:

- We provide a comprehensive review of ten recent international 3D medical image segmentation challenges, including the task descriptions, the datasets, and more importantly, the top solutions of participant teams, which represent the cutting-edge segmentation methods at present.

- We identify the widely used "happy-families" com-٠ ponents in the top methods, which are useful for developing powerful segmentation approaches.

- We summarize several unsolved problems and potential research directions, which could promote the developments in medical image segmentation field.

# 2 PRELIMINARIES: WIDELY USED METHODS IN DEEP LEARNING-BASED MEDICAL IMAGE SEGMEN-TATION

## 2.1 Network Architectures

nnU-Net [8], no new net, is a dynamic fully automatic segmentation framework for medical images, which is based on the widely used U-Net architecture [4], [9]. It can automatically configures the preprocessing, the network architecture, the training, the inference, and the post-processing for any new segmentation task. Without manual intervention, nnU-Net surpasses most existing approaches, and achieves the state-of-the-art in 33 of 53 segmentation tasks and otherwise shows comparable performances to the top leaderboard entries. Currently, nnU-Net has been the most popular backbone for 3D medical image segmentation tasks because of its powerful, flexible, out-of-the-box, and open-sourced,

## 2.2 Loss Functions

Loss function is used to guide the network to learn meaningful predictions and dictate how the network is supposed to trade off mistakes. Cross entropy loss and Dice loss [10], [11] are two most popular loss functions in segmentation tasks. Specifically, cross entropy aims to minimize the dissimilarity between two distributions, which is defined by

$$
L_{CE} = -\frac{1}{N} \sum_{c=1}^{C} \sum_{i=1}^{N} g_i^c \log s_i^c,
$$

(1)

where  $g_i^c$  is the ground truth binary indicator of class label  $c$  of voxel *i*, and  $s_i^c$  is the corresponding predicted segmentation probability.

Dice loss can directly optimize the Dice Similarity Coefficient (DSC) which is the most commonly used segmentation evaluation metric. In general, there are two variants for Dice loss, one employs squared terms in the denominator [10], which is defined by

$$
L_{Dice-square} = 1 - \frac{2\sum_{c=1}^{C}\sum_{i=1}^{N}g_{i}^{c}s_{i}^{c}}{\sum_{c=1}^{C}\sum_{i=1}^{N}(g_{i}^{c})^{2} + \sum_{c=1}^{C}\sum_{i=1}^{N}(s_{i}^{c})^{2}}.
$$

(2)

The other does not use the squared terms in the denominator  $[11]$ , which is defined by

$$
L_{Dice} = 1 - \frac{2\sum_{c=1}^{C}\sum_{i=1}^{N}g_{i}^{c}s_{i}^{c}}{\sum_{c=1}^{C}\sum_{i=1}^{N}g_{i}^{c} + \sum_{c=1}^{C}\sum_{i=1}^{N}s_{i}^{c}}.
$$

(3) 

The default loss function in nnU-Net is the unweighted sum  $L_{CE} + L_{Dice}$ .

## 2.3 Evaluation Metrics

Dice Similarity Coefficient (DSC) and Hausdorff Distance (HD) are two widely used segmentation metrics, which can measure the region overlap ratio and boundary distance, respectively. Let  $G$  and  $S$  be the ground truth and the segmentation result, respectively. DSC is defined by

$$
DSC = \frac{2|G \cap S|}{|G| + |S|}. 
$$

(4)

A similar metric IoU (Jaccard) sometimes is used as an alternative, which is defined by

$$
IoU = \frac{|G \cap S|}{|G \cup S|}.
$$

(5)

Let  $\partial G$  and  $\partial S$  are the boundary points of the ground truth and the segmentation, respectively. The Hausdorff Distance is defined by

$$
HD(\partial G, \partial S) = \max(hd(\partial G, \partial S), hd(\partial S, \partial G)),
$$

(6)

where

$$
hd(\partial G, \partial S) = \max_{x \in \partial G} \min_{y \in \partial S} ||x - y||_2,
$$

and

$$
hd(\partial S, \partial G) = \max_{x \in \partial S} \min_{y \in \partial G} ||x - y||_2.
$$

3

To eliminate the impact of the outliers, 95% HD is also widely used, which is based on the calculation of the 95th percentile of the distances between boundary points in  $\partial G$ and  $\partial S$ .

# 3 SINGLE MODALITY IMAGE SEGMENTATION

## 3.1 CADA: Cerebral Aneurysm Segmentation

task in CADA challenge The (https://cada-as. grand-challenge.org/Overview/) is to segment the aneurysms from 3D CT images. The organizers provide 92 cases for training and 23 cases for testing, where the cases are with cerebral aneurysms without vasospasm. The main difficulty in this challenge is the highly imbalanced labels. As shown in Figure 2 (the first row), the aneurysm is very small and most of the voxels are the background in the CT images.

Six metrics are used to quantitatively evaluate the segmentation results, including Jaccard (IoU), Hausdorff distance (HD), mean distance (MD), Pearson correlation coefficient between predicted volume and reference volume of all aneurysms (Volume Pearson R), the mean absolute difference of predicted and reference volume (Volume Bias), and Standard deviation of the difference between predicted and reference volumes (Volume Std). For the ranking, a maximum-minimum normalization is performed according to all participants. In this way, each individual metric takes a value between 0 (worst case among all participants) and 1 (perfect fit between the reference and predicted segmentation). The ranking score is calculated as the average of the normalized metrics.

TABLE 2: Quantitative results of top-2 teams on CADA Challenge Leaderboard. The bold numbers denote the best results.

| Metrics        | Mediclouds | junma [12] |
|----------------|------------|------------|
|                | Rank 1st   | Rank 2nd   |
| IoU            | 0.758      | 0.759      |
| HD             | 2.866      | 4.967      |
| MD             | 1.618      | 3.535      |
| Vol. Pearson R | 0.998      | 0.998      |
| Vol.Bias       | 72.24      | 75.84      |
| Vol.Std        | 106.4      | 110.5      |
| Final Score    | 0.833      | 0.832      |

Table 2 shows the quantitative segmentation results of the top-2 teams on the challenge leaderboard<sup>6</sup>. The team 'junma' achieved the best IoU while the team 'Mediclouds' achieved better performance in the remaining five metrics. However, the final score difference is marginal. The method of the team 'Mediclouds', unfortunately, is not available. Thus, we only present the solution of the team 'junma'. Specifically, Ma and Nie [12] developed their methods based on nnU-Net [8] where the main modification to use a large patch size ( $192 \times 224 \times 192$ ) during training and inference. Five models were trained in five-fold cross-validation and each model was trained on a TITAN V100 32G GPU. Each testing case is predicted by the ensemble of the trained five models.

## 3.2 ASOCA: Automated Segmentation Of Coronary Arteries

The task in ASOCA challenge (https://asoca. grand-challenge.org/Home/) is to segment the coronary arteries from Cardiac Computed Tomography Angiography (CCTA) images. The organizers provide 40 cases for training and 20 cases for testing. The main difficulties in this challenge are the imbalanced problem and appearance variations. On the one hand, the coronary arteries only occupy a small proportion in the whole CT image. On the other hand, the arteries from healthy and unhealthy cases share different appearances. Figure 2 (the second row) presents a visualized example. DSC and HD95 are used to evaluate and rank the segmentation results.

TABLE 3: Quantitative results of top-2 teams on ASOCA Challenge Leaderboard. The bold numbers denote the best results.

| Team Name  | DSC          | HD95         | Final Rank |
|------------|--------------|--------------|------------|
| RuochenGao | <b>0.867</b> | 4.165        | 1          |
| SenYang    | 0.838        | <b>2.336</b> | 2          |

Table 3 shows the quantitative segmentation results of the top-2 teams on the challenge leaderboard<sup>7</sup> during MIC-CAI 2020. The 1st-place team had better DSC while the 2nd-place team obtained better HD95, indicating that the top-2 teams achieved better region overlap and boundary distance, respectively.

The team 'RuochenGao' used nnU-Net [8] as the backbone. The whole pipeline include three independent networks for three tasks: epicardium segmentation, artery segmentation, and scale map regression [13]. The final segmentation results were generated by the ensemble of artery segmentation results and scale map regression results followed by removing the vessels outside the epicardium. The team 'SenYang' proposed an improved 2D U-Net with selective kernel (SK-UNet) where the regular convolution blocks were replaced by SE-Res modules in the encoder. Moreover, the SK-modules [14], including different convolution filters and kernel sizes, were used in the decoder to leverage multiscale information.

## 3.3 VerSe: Large Scale Vertebrae Segmentation Challenge

The segmentation task in VerSe challenge (https://verse2020.grand-challenge.org/) is to segment the vertebrae from CT images. The organizers provide 100 cases for training, 100 cases for public testing (the participants can access the testing cases) and 100 cases for hidden testing (this testing set is not publicly available and participants are required to submitted their solutions with Docker containers) [15], [16]. The annotations consist of 28 different vertebrae but each case may only contain part of the vertebrae. There are several difficulties in this challenge: highly varying fields-of-view (FoV) across cases, large scan sizes, highly correlating shapes of adjacent vertebrae, scan noise, the presence of vertebral fractures, metal implants, and so on [17]. Figure 2 (the third row) presents a visualized

<sup>4.</sup> Myocardium, left and right ventricle

<sup>5.</sup> Whole tumor, enhancing tumor, and tumor core

<sup>6.</sup> https://cada-as.grand-challenge.org/FinalRanking/

<sup>7.</sup> https://asoca.grand-challenge.org/MICCAI\_Ranking/

4

![](_page_3_Figure_1.jpeg)

Fig. 2: Visualized examples in three CT segmentation tasks. The ground truth of the original image (a) in each task is shown in 2D projected onto the raw data (b) and in 3D together with a volume rendering of the raw data (c).

example. DSC and HD are used to evaluate and rank the segmentation results.

Payer et al., the defending champion in VerSe 2019 [17], succeeded in winning this year's challenge again by the SpatialConfiguration-Net [18] and U-Net [4], [9]. Specifically, they proposed a coarse-to-fine approach, including three stages:

- stage 1: localizing the whole spine by a 3D U-Netbased heatmap regression network, which can remove background; The network input size ranged from  $32 \times 32 \times 32$  to  $128 \times 128 \times 128$ .

- stage 2: localizing and identifying all vertebrae landmarks simultaneously via a 3D SpatialConfiguration-Net, which combines local appearance with spatial configuration of landmarks; The network input size ranged from  $64 \times 64 \times 64$  to  $96 \times 96 \times 256$  during training and was up to  $128 \times 128 \times 448$  during inference. To address the missed vertebrae, a MRFbased graphical model was employed to refine the localization results.

- stage 3: segmenting each vertebra individually by a 3D U-Net. The input size was  $128 \times 128 \times 96$ .

Table 4 presents the quantitative segmentation results on the public testing set. The absent results will be added when the challenge summarize paper is released.

TABLE 4: Quantitative vertebrae segmentation results of the winner solution in VerSe 2020. '-' denotes not available currently.

| Testing set | DSC    | HD95 |
|-------------|--------|------|
| Public      | 0.9354 | -    |
| Hidden      | -      | -    |

## 3.4 M&Ms: Multi-Centre, Multi-Vendor & Multi-Disease **Cardiac Image Segmentation Challenge**

The task in M&Ms challenge https://www.ub.edu/mnms/ is to segment the left and right ventricle (LV and RV, respectively) cavities and the left ventricle myocardium (MYO) from multi-center, multi-vendor, and multi-disease cardiac MR images. The organizers provide 175 cases for training, 40 cases for validation, and 160 cases for testing, which are from four scanner vendors. Specifically, the 175 training cases consist of 75 labelled cases from the vendor A, 75 labelled cases from the vendor B, and 25 unlabelled cases from the vendor C. The 40 validation cases consist of 10 cases from each of the four vendors. The 160 testing cases consist of 40 cases from each of the four vendors. The main difficulty in this challenge is the domain shift in testing set, which requires the solutions should be generalizable across different clinical centers, scanner vendors, and patient conditions. It should be noted that both validation cases and testing cases are not publicly available to par-

5

![](_page_4_Figure_0.jpeg)

Fig. 3: Visualized examples in two MR segmentation tasks. The ground truth of the original image (a) in each task is shown in 2D projected onto the raw data (b) and in 3D rendering (c).

ticipants during the challenge. Participants are required to build a Singularity image and shared it with the organizers. Figure 3 (the first row) presents a visualized example. Four metrics are used to evaluate and rank the segmentation results, including DSC, IoU, Average symmetric surface distance (ASSD), and HD.

TABLE 5: Quantitative segmentation results (in terms of DSC and HD) of top-3 teams on M&Ms Challenge Leaderboard. 'ED' and 'ES' denote the end-diastolic and endsystolic phases cardiac MR images. The bold numbers are the best results and the italics numbers are not-significant when compared with the best results ( $p > 0.01$  in T-test).

| Metrics    | Peter M. Full [19]<br>Rank 1st | Yao Zhang [20]<br>Rank 2nd | Jun Ma [21]<br>Rank 3rd |
|------------|--------------------------------|----------------------------|-------------------------|
| LV DSC     | 0.939                          | 0.938                      | 0.935                   |
| LV HD      | 9.10                           | 9.30                       | 9.50                    |
| ED MYO DSC | 0.839                          | 0.830                      | 0.825                   |
| ED MYO HD  | 12.8                           | 12.9                       | 13.3                    |
| RV DSC     | 0.910                          | 0.909                      | 0.906                   |
| RV HD      | 11.8                           | 12.3                       | 12.3                    |
| LV DSC     | 0.886                          | 0.880                      | 0.875                   |
| LV HD      | 9.10                           | 9.50                       | 10.5                    |
| ES MYO DSC | 0.867                          | 0.861                      | 0.856                   |
| ES MYO HD  | 10.6                           | 10.8                       | 11.6                    |
| RV DSC     | 0.860                          | 0.850                      | 0.844                   |
| RV HD      | 12.7                           | 13.0                       | 13.0                    |

The top-3 teams developed their methods based on nnU-Net [8]. Specifically, Full et al. [19], the 1st-place team, handled the domain shift problem by an ensemble of five 

2D and five 3D nnU-Net models that were trained with the batch normalization and extensive data augmentation, such as random rotation, flipping, gamma correction, multiplicative/additive brightness, and so on. Zhang et al. [20], the 2nd-place team, used label propagation to leverage unlabelled cases and exploited the style transfer to reduce the variance among different centers and vendors. The final solution was one single model without using postprocessing and ensemble. Ma [21], the 3rd-place team, addressed the domain shift problem by enlarging the training set with histogram matching, where new training cases were generated by using histogram matching to transfer the intensity distribution of 25 unlabelled cases to existing labelled cases. The final solution was an ensemble of five 3D nnU-Net models. Table 5 presents the quantitative segmentation results of the top-3 teams. It can be found that the differences among them were marginal and not statistically significant, indicating that all (three) roads lead to Rome.

## 3.5 EMIDEC: Automatic Evaluation of Myocardial Infarction from Delayed-Enhancement Cardiac MRI

The task in EMIDEC challenge (http://emidec.com/) is to segment the myocardium, the infarction, and the no-reflow areas from delayed-enhancement cardiac MR images. The organizers provide 100 cases for training and 50 cases for testing [22]. The main difficulties in this challenge are the low contrast, varied short-axis orientations, heterogeneous

6

appearances of myocardium pathology areas, and unbalanced distribution between normal and pathological cases. Figure 3 (the second row) presents a visualized example. The evaluation and ranking metrics include

- clinical metrics: the average errors for the volume of the myocardium (in mm3), the volume (in mm3) and the percentage of infarction and no-reflow area;

- geometrical metrics: the average DSC for the different areas and Hausdorff distance (in 3D) for the myocardium.

Table 6 presents the quantitative segmentation results of the top-3 teams on the final leaderboard<sup>8</sup>. Both Zhang and Ma, the top-2 teams, used a two-stage cascaded framework and developed their methods based on nnU-Net [8]. Specifically, Zhang [23] first used a 2D nnU-Net, focusing on the intra-slice information, to obtain a preliminary segmentation, and then a 3D nnU-Net, focusing on the volumetric spatial information, was employed to refine the segmentation results. The 3D nnU-Net took the combination of the preliminary segmentation and original image as the input. Finally, the scattered voxels in segmentation results were removed in postprocessing step. Ma [24] used the 2D nnU-Net in the two stages. Firstly a 2D U-Net was used to segment the whole heart, including the left ventricle and the myocardium. Then, the whole heart was cropped as a region of interest (ROI). Finally, a new 2D U-Net was trained to segment the infraction and no-reflow areas in the ROI. The final model was an ensemble of five 2D nnU-Net models in each stage. Feng et al.<sup>9</sup> used dilated 2D UNet [25] with rotation-based augmentation, which aim to overcoming the varied short-axis orientations.

TABLE 6: Quantitative results of top-3 teams on EMIDEC Challenge Leaderboard.

| Targets    | Metrics          | Zhang [23]<br>Rank 1st | Ma [24]<br>Rank 2nd | Feng et al.<br>Rank 3rd |
|------------|------------------|------------------------|---------------------|-------------------------|
| Myocardium | DSC              | 0.8786                 | 0.8628              | 0.8356                  |
|            | Vol. Diff.       | 9258                   | 10153               | 15187                   |
|            | HD               | 13.01                  | 14.31               | 33.77                   |
| Infarction | DSC              | 0.7124                 | 0.6224              | 0.5468                  |
|            | Vol. Diff.       | 3118                   | 4874                | 3971                    |
|            | Vol. Diff. Ratio | 2.38%                  | 3.50%               | 2.89%                   |
| Re-flow    | DSC              | 0.7851                 | 0.7776              | 0.7222                  |
|            | Vol. Diff.       | 634.7                  | 829.7               | 883.4                   |
|            | Vol. Diff. Ratio | 0.38%                  | 0.49%               | 0.53%                   |

The methods of Zhang [23] and Ma [24] obtained comparable results for myocardium and re-flow areas, but Zhang achieved significantly better results for infraction, which were 9% and 17% higher than the methods of Ma [24] and Feng et al. in terms of DSC. The major methodology difference is that Zhang used the 3D network in the second stage while Ma and Feng et al. used the 2D network. Thus, one of the possible reasons might be that 3D network can use more image contextual information than 2D network, and also lead to better performance.

# 4 MULTI-MODALITY 3D IMAGE SEGMENTATION

## 4.1 ADAM: Intracranial Aneurysm Detection and Segmentation Challenge

The task in ADAM challenge ( $http://adam.isi.uu.nl/)$  is to segment the aneurysms from TOF-MRA and structural MR images. The organizers provide 113 cases for training and 141 cases for testing. In the 113 training cases, 93 cases contain at least one untreated, unruptured intracranial aneurysm and 20 cases do not have intracranial aneurysms. In the 141 testing cases, 117 cases containing at least one untreated, unruptured intracranial aneurysm, and 26 cases do not have intracranial aneurysms. Each case has two folders:

- 'orig' folder: containing all of the original TOF-MRA images and structural images (T1, T2, or FLAIR). The structural image was aligned to the TOF image by  $\text{elastic}^{10}$ .

- 'pre' folder: All images were preprocess by 'n4biasfieldcorrection'<sup>11</sup> to correct bias field inhomogeneities.

The main difficulty in this challenge is the extremely imbalanced problem. Specifically, the median image size is  $512 \times 512 \times 140$ , while the median aneurysm voxel size is 238, leading to a extremely imbalanced foreground-background ratio of  $6.5 \times 10^{-6}$ . Figure 4 presents the visualized examples. Participants are allowed to use any of the provided images to develop their methods. The testing set is hidden by the organizers and participants should submit their methods with Docker containers.

![](_page_5_Figure_16.jpeg)

Fig. 4: Visualized examples in ADAM Challenge. Ground truth (b) of the intracranial aneurysm is shown in 2D projected onto the TOF-MRA and the structural MR image (a) and in 3D together with a volume rendering of the raw data (c). The red arrows point to the intracranial aneurysm.

Table 7 presents the quantitative results of top-2 teams on ADAM Challenge Leaderboard<sup>12</sup> during MICCAI 2020. Both teams developed their methods based on nnU-Net [\[8\]](#page-9-1). Specifically, to alleviate the imbalanced problem, the team 'junma' trained two group five-fold nnU-Net models with Dice + Cross entropy loss and Dice + TopK loss, respectively [\[26\]](#page-9-1). Only preprocessed TOF-MRA images were used

<sup>8.</sup> http://emidec.com/leaderboard

<sup>9.</sup> http://emidec.com/downloads/papers/paper-24.pdf

<sup>10.</sup> https://elastix.lumc.nl/

<sup>11.</sup> http://stnava.github.io/ANTs/

<sup>12.</sup> http://adam.isi.uu.nl/results/results-miccai-2020/

7

during training. The final model was the ensemble of five best models during cross-validation. To speed up the inference, the default testing time augmentation in nnU-Net (TTA) was disabled during testing. The team 'jocker' modified the default nnU-Net by introducing residual blocks in the encoder and replacing the instance normalization with group normalization. The loss function was  $Dice + TopK$ loss. The final model was the ensemble of four models with different modalities and output classes.

TABLE 7: Quantitative results of top-2 teams on ADAM Challenge Leaderboard. The bold numbers are the best results.

| Team  | DSC  | HD95 | Volumetric Similarity | Rank |
|-------|------|------|-----------------------|------|
| junma | 0.41 | 8.96 | 0.50                  | 1    |
| joker | 0.40 | 8.67 | 0.48                  | 2    |

As shown in Table 7, the team 'junma' achieved the best DSC and Volumetric Similarity and the team 'joker' achieved the best HD95. However, it should be noted that the differences between them are marginal.

## 4.2 HECKTOR: 3D Head and Neck Tumor Segmentation in PET/CT

The task in HECKTOR challenge (http://www.aicrowd. com/challenges/hecktor) is to segment the heck and neck tumor from PET and CT images. The organizers provide 201 training cases from four medical centers in Montreal and 53 testing cases from another medical center in Lausane [27], [28]. The tumor ground truth was delineated for radiotherapy treatment planning on PET and CT. Moreover, the organizers also provided bounding boxes ( $114 \times 114 \times 114$  $mm^3$ ) locating the oropharynx region. The main difficulties are the multi-modality fusion, imbalanced problem, and the unseen testing cases from a new medical center. Figure 5 presents the visualized examples.

![](_page_6_Figure_6.jpeg)

(a) Original Image (b) Ground Truth  (c) 3D Rendering

Fig. 5: Visualized examples in HECKTOR Challenge. Ground truth (b) of the head and neck tumor is shown in 2D projected onto the CT and the PET image (a) and in 3D together with a volume rendering of the raw data (c).

Table 8 presents the quantitative segmentation results of the top-2 teams on HECKTOR Challenge Leaderboard<sup>13</sup>

during MICCAI 2020. Both teams developed their methods based on the two channel 3D U-Net [9]. Specifically, the team 'andrei.iantsen', replaced the batch normalization with squeeze-and-excitation normalization and introduced the residual blocks in the encoder. The loss function was the unweighted sum of Dice loss and focal loss [29]. Four models with leave-one-center-out splits and four additional models with random data splits were trained for 800 epoches using Adam optimizer [30] on two NVIDIA 1090Ti GPUs with a batch size of 2. The final model was an ensemble of the eight models. The team 'junma' [31] firstly trained five 3D nnU-Net models [8] with Dice + TopK loss for five-fold crossvalidation. Then, a segmentation quality score was defined by model ensembles, which can be used to select the cases with high uncertainties. Finally, the high uncertainty cases were refined by a hybrid active contour model with iterative convolution-thresholding methods [32], [33], [34].

Both teams concatenated the PET and CT image as input and model ensembles were used to predicting the testing set. In the loss function, Dice loss was also incorporated in both teams.

TABLE 8: Quantitative results of top-2 teams on HECKTOR Challenge Leaderboard. The bold numbers are the best results.

| Team           | DSC          | Precision    | Recall       | Rank |
|----------------|--------------|--------------|--------------|------|
| andrei.iantsen | <b>0.759</b> | 0.833        | <b>0.740</b> | 1    |
| junma [31]     | 0.752        | <b>0.838</b> | 0.717        | 2    |

The final rank was based on the DSC scores on the testing set. The 1st-place team 'andrei.iantsen' obtained better DSC and Recall while the 2nd-place team 'junma' obtained better Precision. However, the differences between the two teams are marginal, especially for the DSC and Precision. Moreover, both teams achieved significantly higher Precision than Recall, indicating that most of the segmented voxels were real tumor voxels but many tumor voxels were missed by the model.

## 4.3 MyoPS: Multi-sequence CMR based myocardial pathology segmentation challenge

The task in MyoPS challenge (http://www.sdspeople. fudan.edu.cn/zhuangxiahai/0/myops20/) is to segment the myocardial pathology (i.e., scar and edema) from multisequence cardiac MR images, including the late gadolinium enhancement (LGE) sequence, the T2-weighted sequence, and the balanced- Steady State Free Precession (bSSFP) cine sequence. The organizers provide 25 cases for training and 20 cases for testing [35], [36]. The main difficulties are the multi-modality fusion, imbalanced problem, low-contrast and heterogeneous appearances of myocardium lesions.

The winner team, 'Zhai & Gu et al.' [37], proposed a coarse-to-fine framework with weighted ensemble. In the coarse segmentation stage, the whole heart was segmented by a U-Net [4] from three sequence MR images. In the fine segmentation stage, the region of interest (ROI) was cropped according to coarse segmentation results and a  $nnU-Net$  [8] was trained to simultaneously segment the left ventricle, right ventricle, healthy myocardium, scar, and edema from the concatenation of three sequence MR images and coarse

<sup>13.</sup> https://www.aicrowd.com/challenges/miccai-2020-hecktor/ leaderboards

8

![](_page_7_Picture_0.jpeg)

Fig. 6: Visualized examples in MyoPS Challenge. Ground truth is shown in 2D projected onto the multi-sequence MR images (b) and in 3D rendering (c).

segmentation results. Cross-validation results showed that 2D U-Net achieved better performance for the edema while 2.5D U-Net achieved better performance for the scar. To obtained better performance, a weighted method was used for final ensemble. Specifically, the weights for edema and scar prediction channels were 0.8 in 2D and 2.5D U-Net, respectively, while the weights for the other prediction channels were 0.5.

TABLE 9: Quantitative results of the winner team 'Zhai & Gu et al.' [37] in MyoPS challenge.

| Target       | DSC           | Rank |
|--------------|---------------|------|
| Scar         | 0.672 ± 0.244 | 1    |
| Scar + Edema | 0.731 ± 0.109 | 1    |

Table 9 presents the quantitative segmentation results of the winner team on the testing set<sup>14</sup>. Zhai & Gu et al. achieved an average DSC of 0.672  $\pm$  0.244 and 0.731  $\pm$  0.109 for scar and the combination of scar and edema, respectively. The performance was significantly better than the inter-observer variation of manual scar segmentation (DSC:  $0.5243 \pm 0.1578$ ), demonstrating the effectiveness of the proposed method.

## 4.4 ABCs: Anatomical Brain Barriers to Cancer Spread: Segmentation from CT and MR Images

ABCs challenge (https://abcs.mgh.harvard.edu/) included two brain structures segmentation tasks

Task 1: segmenting five brain structures, including falx cerebri, tentorium cerebelli, sagittal and transverse brain sinuses, cerebellum and ventricles, which can be used for automated definition of the clinical target volume (CTV) for radiotherapy treatment.

Task 2: segmenting ten structures, including Brainstem, left and right eyes, left and right optic nerves, left and right optic chiasm, lacrimal glands, and cochleas, which can be used in radiotherapy treatment plan optimization.

The organizers provide 45 cases for training, 15 cases for validation, and 15 cases for testing. Each case consists of one CT image acquired for treatment planning, and two post-operative brain MRI images (i.e., contrast enhanced T1-weighted, T2-weighted FLAIR). The CT and MR images were obtained from two different CT scanners and seven different MRI scanners, respectively. The multi-modality images were co-registered, and re-sampled to the same resolution and size. The main difficulties are the multi-modality fusion, imbalanced labels, and multi-vendor cases. Figure 7 presents the visualized examples in two tasks. Participants are required to submit their segmentation results within 48 hours after the time of download the testing set. DSC and Surface  $DSC^{15}$  at the tolerance of 2  $mm$  are used to evaluate and rank the segmentation results.

![](_page_7_Figure_12.jpeg)

Fig. 7: Visualized examples in ABCs Challenge. Ground  $\text{truth (b)}$  is shown in 2D projected onto the multi-modality images (a) and in 3D together with a volume rendering of the raw data (c).

Table 10 presents the average DSC and SDSC of testing set segmentation results of the top-2 teams on the Challenge Leaderboard<sup>16</sup>. Both teams developed their methods based on nnU-Net [8]. Specifically, the team 'Jarvis' [38] used the

<sup>14.</sup> http://www.sdspeople.fudan.edu.cn/zhuangxiahai/0/ myops20/result.html

<sup>15.</sup> https://github.com/deepmind/surface-distance

<sup>16.</sup> https://abcs.mgh.harvard.edu/index.php/leader-board

9

TABLE 10: Quantitative results of top-2 teams on ABCs Challenge Leaderboard. The bold numbers are the best results.

| Team        | Task 1       |              | Task 2       |              | Rank |
|-------------|--------------|--------------|--------------|--------------|------|
|             | DSC          | SDSC         | DSC          | SDSC         |      |
| Jarvis [38] | <b>0.888</b> | <b>0.980</b> | <b>0.783</b> | 0.936        | 1    |
| HILab       | 0.883        | 0.978        | 0.781        | <b>0.941</b> | 2    |

ResU-Net where residual blocks were introduced in the U-Net encoder. The training process had three main features:

- the training cases '007' and '054' in Task 2 had annotation issues. Thus, the default annotations were replaced with pseudo labels generated by crossvalidation.

- the flipping along x-axis was dropped from the de-. fault data augmentation setting in nnU-Net, because the segmentation targets in Task 2 are sensitive to left and right direction.

- in addition to the default Dice + CE loss in nnU-Net, Tversky loss [26], [39] was also used to train the ResU-Net.

The final model was an ensemble of default nnU-Net, ResU-Net with Dice-CE loss, and ResU-Net with Tversky loss. The team 'HILab' used a coarse-to-fine framework with nnU-Net [8] for both tasks. Specifically,

- in Task 1, an uncovered model was trained to obtain the coarse segmentations with small overfitting. Then, each organ was cropped from the original images and refined by an independent network. The refined organs were combined as the final segmentation results.

- in Task 2, a coarse model was firstly trained to segment all organs. Then, each organ was also cropped from the original images and refined by an independent network. The training process was different from Task 1, where a new data augmentation technique, flipping each organ to other side was introduced to enlarge the training set. The final segmentation results were also the combination of the refined organs.

Both teams fused the three modalities by concatenating them as the network input. Model ensemble was also used by both teams but the ensemble strategies were different. In particular, the team 'Jarvis' used an ensemble of multiple multi-organ segmentation networks while the team 'HILab' used an ensemble of one multi-organ and multiple singleorgan segmentation networks.

## 4.5 BraTS: Brain Tumor Segmentation

The segmentation task in BraTS challenge (https://www. med.upenn.edu/cbica/brats2020/) is to segment the enhancing tumor (ET), the tumor core (TC, the necrotic and non-enhancing tumor core), and the whole tumor (WT) from pre-operative multi-modality MR images. As shown in Figure 8, the whole tumor comprises the enhancing tumor (red), the edema (green), and the tumor core (blue). The organizers provide 369 cases for training, 125 cases for testing, and 166 cases for testing. Each case consists of four

modalities: the native (T1) MR image, the post-contrast T1weighted (T1Gd) MR image, the T2-weighted (T2), and the T2 Fluid Attenuated Inversion Recovery (FLAIR) MR image, which were acquired with different clinical protocols and various scanners from 19 institutions [40], [41], [42], [43], [44]. The main difficulties are the multi-modality fusion, imbalanced labels, low-contrast and heterogeneous appearances of the brain lesion. Participants are required to submit their segmentation results within 48 hours after the time of download the testing set. DSC and HD95 are used to evaluate and rank the segmentation results.

![](_page_8_Figure_13.jpeg)

Fig. 8: Visualized examples in BraTS Challenge. Ground truth is shown in 2D projected onto the multi-sequence MR images and in 3D together with a volume rendering of the raw data.

The winner team 'MIC-DKFZ', leaded by Fabian et al. [45], extended nnU-Net [8] by incorporating BraTSspecific modifications regarding postprocessing, regionbased training, a more aggressive data augmentation, BraTS ranking-based model selection as well as several minor modifications, which can improve the default nnU-Net segmentation performance substantially. Specifically, following BraTS-specific modifications were integrated into nnU-Net's configuration:

- **Region-based training:** replacing the softmax layer with a sigmoid layer and changing the optimization target to the three tumor sub-regions. The default cross entropy loss term was also replaced with a binary cross-entropy where each of the regions was optimized independently;

- **Postprocesing:** removing enhancing tumor entirely if the predicted volume was less than a given threshold. The threshold was optimized on training set cross-validation twice, once via maximizing the mean Dice score and once via minimizing the BraTSlike ranking score;

- **Increased batch size:** increasing the batch size from 2 to 5;

- **Extensive data augmentation:** using more aggressive augmentations, such as increasing the the probability of applying rotation, scaling, the scale range, elastic deformation, and so on;

10

- **Batch normalization:** replacing the default instance normalization with batch normalization;

- **Batch dice:** computing the dice loss over all samples in the batch;

- **BraTS Ranking-based model selection:** selecting the best model with BraTS-like 'rank then aggregate' ranking scheme.

The final model was an ensemble of 25 cross-validation models including three groups of top performing models.

Two tied teams ranked second. Specifically, the team 'NPU\_PITT', leading by Jia et al. [46], proposed a Hybrid High-resolution and Non-local Feature Network (H<sup>2</sup>NF-Net) Four modalities were concatenated as a four-channel input and processed at five different scales in the network. The edema and enhancing tumor were segmented by the single HNF-Net and the tumor core was segmented by he cascaded HNF-Net. and different brain tumor sub-regions were segmented by single and cascaded HNF-Nets. The team 'Radicals', leading by Wang et al. [47], proposed an end-to-end Modality-Pairing learning method with paralleled branches and more layer connections to explore the latent relationship among different modalities. Moreover, a consistence loss was introduced to minimize the prediction variance between branches. The final model was an ensemble of three Modality-Pairing models and three Vanilla nnU-Net [8] models.

TABLE 11: Quantitative results of the top-3 teams on BraTS 2020 Challenge Leaderboard. The bold numbers are the best results.

| Team               | Target          | DSC           | HD95        |
|--------------------|-----------------|---------------|-------------|
| MIC_DKFZ           | Enhancing Tumor | 0.820 ± 0.197 | 17.8 ± 74.9 |
| Fabian et al. [45] | Whole Tumor     | 0.890 ± 0.132 | 8.50 ± 40.7 |
| Rank 1st           | Tumor Core      | 0.851 ± 0.240 | 13.3 ± 69.5 |
| NPU_PITT           | Enhancing Tumor | 0.828 ± 0.177 | 13.0 ± 63.7 |
| Jia et al. [46]    | Whole Tumor     | 0.888 ± 0.119 | 4.53 ± 6.21 |
| Rank 2nd (tie)     | Tumor Core      | 0.854 ± 0.231 | 16.9 ± 69.5 |
| Radicals           | Enhancing Tumor | 0.816 ± 0.197 | 17.8 ± 74.9 |
| Wang et al. [47]   | Whole Tumor     | 0.891 ± 0.112 | 6.2 ± 29.0  |
| Rank 2nd (tie)     | Tumor Core      | 0.842 ± 0.244 | 19.5 ± 74.8 |

Table 11 presents the quantitative segmentation results of the top-3 teams on the testing  $\text{set}^{17}$ . Overall, the performance differences are marginal. The team 'MIC\_DKFZ' achieved the best HD95 for the tumor core and the team 'Radicals' achieved the best DSC for the whole tumor. The team 'NPU\_PITT' achieved the best performance in the remaining four metrics.

# 5 DISCUSSION

## 5.1 What are the "happy-families" elements in the top methods?

As the Anna Karenina principle  $goes^{18}$ : "All happy families are alike.", there are also some common components in the top methods.

nnU-Net [8] backbone All the top methods used U-Net  $[4]$ ,  $[9]$  like architectures in the ten 3D segmentation challenges. Remarkably, nnU-Net was used by the top teams 

in nine out of ten challenges, because it is open-sourced, powerful, flexible, and out-of-the-box. Participants can easily integrate their new methods into nnU-Net.

**Dice-related loss functions** Loss function is one of the most important elements in deep learning-based segmentation methods. nnU-Net used Dice + cross entropy as the default loss function. For extremely imbalanced segmentation tasks, modifying the loss function can obtain better performance. For example, the winner in HECKTOR challenge used Dice + Focal loss. Both the winner and the runner up used Dice + TopK loss in ADAM challenge. For a more detailed analysis of segmentation loss functions, please refer to [26].

**Cascaded/coarse-to-fine framework** Cropping the region-of-interest (ROI) can eliminate the unrelated background tissues and reduce the computational burden. Thus, one can firstly trained a model to obtain the coarse segmentation and then crop the ROI. After that, training a new model with the ROI image (concatenated with the coarse segmentation) to refine the segmentation results. This strategy is quite effective for myocardial pathology and small organ segmentation tasks, which was used by both the winners in EMIDEC and MyoPS challenge, and the runner up in ABCs challenge.

**Model Ensembles** Ensemble is an effective way to fuse the performance of multiple single models. All the top teams used more than one models in their final solutions. The models were usually trained with different data splits, data augmentation techniques, networks, or loss functions, and then combined by averaging the predictions, majority vote, or cascaded frameworks.

**Concatenated input fusion in multi-modality segmentation tasks** How to fuse multiple different images is a key question in multi-modality segmentation tasks. Common deep-learning based image fusion methods include inputlevel fusion, feature-level fusion, and output-level fusion. In five multi-modality segmentation challenges, four out of five winner teams used input-level fusion, which directly concatenated multiple images as network inputs. The winner team in ADAM challenge only use one modality but the runner up, achieving similar performance, also used the concatenation strategy to fuse different modalities.

## 5.2 Problems and Opportunities

Based on the summary of the ten segmentation challenges, it can be found that deep learning has achieved unprecedented or even human-level performance on many medical image segmentation tasks, but there still remains several problems. Following, we introduce some of the problems and also opportunities that can promote the further development of medical image segmentation methods.

Standardized method reports Many challenge organizers required the participants to submit a short paper to describe their methods. However, these papers are usually structured with their own way and some necessary details might be missed. Currently, the challenge quality has been greatly improved with the Biomedical Image Analysis ChallengeS (BIAS) initiative [7], where a checklist is used to standardize the review process and raise interpretability and reproducibility of challenge results. Thus, there is also

<sup>17.</sup> https://www.med.upenn.edu/cbica/brats2020/rankings.html

<sup>18.</sup> https://en.wikipedia.org/wiki/Anna\_Karenina\_principle

11

a high demand for the challenge method reports quality control. The winner team in MICCAI Hackathon Challenge provided an initial attempt (https://github.com/JunMa11/ MICCAI-Reproducibility-Checklist) at dealing with the method reproducibility with a checklist, but more efforts are required to make this checklist more complete and acceptable by our community.

**Publicly available baseline models** nnU-Net has been proved to be a strong baseline. When starting with a new 3D segmentation challenge, most of the participants will train nnU-Net baseline models, which usually cost 72-120 GPU hours (depending on the computational facilities). This could be a great waste of energy and time, because participants are repeatedly doing the same thing. There is a strong demand for publicly available (trained) baseline models when a new segmentation challenge is launched. In this way, participants can pay more attention to developing new methods without spending energy and time on training the baseline models.

**Fast and memory efficient models** There is no doubt that accuracy (e.g., DSC, HD) is an important factor for segmentation methods. However, the running time and the GPU memory requirement of the segmentation methods are also critical when deploying the trained models in clinical practice. Currently, most of the top methods use model ensembles, which could be time-consuming and require very high computing resources. In order to promote the deep learning-based medical image segmentations to be clinically applicable, it is necessary to pay more attention to the models' running efficiency.

**Theoretical foundations of segmentation models** Current theoretical studies of deep learning usually have strong assumptions [48], [49], [50], such as smoothness, infinite width, and so on. However, when it comes to real practice, many open problems remain unsolved. For example, what is the theoretical principle of designing segmentation network architectures? is there a generalization gap? how should we to estimate it? what does the loss function landscape look like? does the training process converge to a good solution? How fast? how much data do we need when start with a new segmentation task?

**Diverse datasets and generalizable segmentation models** Collecting diverse datasets is critical for developing generalizable segmentation models, because clinical practice requires the trained models can be applied to many (unseen) medical centers. According to the challenge results (e.g., M&Ms, BraTS, HECKTOR), we found that the segmentation performances have a significant drop when testing sets include unseen cases from new medical centers. Thus, it is important to have diverse datasets to evaluate the models' generalization ability when organizing segmentation challenges. Currently, building generalizable models that can be applied consistently across medical centres, diseases, and scanner vendors is still an unsolved and challenging problem.

**Extremely imbalanced target segmentation Imbalanced** segmentation has been a long-term problem in medical image segmentation, especially when the size of target foreground region is several orders of magnitude less than the background size. Recently studies have made some progress [10], [51], [52], however, the extremely imbalanced segmen-

tation still remains very difficult. For example, in ADAM challenge, the median target size is 238, which occupies  $6.5 \times 10^{-6}$  of the median image size. The winner method achieved a DSC score of 0.41, which has large rooms for further improvements.

## 5.3 Limitations

There are also some important but not mentioned topics in this paper. For example, a summary of 2D medical image segmentation challenges [53], [54], [55] has not been included in this paper, because we only found three 2D international image segmentation challenges in 2020, including thyroid nodule segmentation in ultrasound images (https://tn-scui2020. grand-challenge.org/Home/), optic disc and cup segmentation in fundus images (https://refuge.grand-challenge. org/Home2020/), and cataract segmentation in surgical videos (https://cataracts-semantic-segmentation2020. grand-challenge.org/Home/). Thus, the findings would be biased with limited challenge samples and we will provide a similar summary for 2D medical image segmentation methods when there are many ( $\geq 10$ ) international challenges. Moreover, this paper only covers cutting-edge fully supervised segmentation methods, while semi-supervised learning [56], [57], [58], weakly-supervised learning [59], [60], and continual learning [61], [62], [63]-based segmentation methods are not mentioned. This is because, currently, there are few benchmarks or challenges [64], [65] for these topics in the medical image segmentation field.

# 6 CONCLUSION

Challenges provide an open and fair platform for various research groups to test and validate their segmentation methods on common datasets acquired from the clinical environment. In this paper, we have summarized ten 3D medical image segmentation challenges and the corresponding top methods. In addition, we also identify the widely involved "happy-families" elements in the top methods and give potential future research directions in medical image segmentation. Moreover, we also maintain a public GitHub repository (https://github.com/JunMa11/SOTA-MedSeg) to collect the cutting-edge segmentation methods based on various international segmentation challenges. We expect that this review of the cutting-edge 3D image segmentation methods will be beneficial to both early-stage and senior researchers in related fields.

# ACKNOWLEDGMENTS

The authors would like to thank all the organizers for creating the public datasets and holding the great challenges. The authors also highly appreciate Ruochen Gao (the winner in ASOCA), Ran Gu (the winner in MyoPS), Wenhui Lei (the winner in MyoPS and the runner up to winner in ABCs), Munan Ning (the winner in ABCs), and Yixin Wang (the runner up to winner in BraTS), Yao Zhang (the runner up to winner in M&Ms), Yichi Zhang (the winner in EMIDEC) for valuable discussions.

12

# REFERENCES

- [1] R. J. Gillies, P. E. Kinahan, and H. Hricak, "Radiomics: images are more than pictures, they are data," Radiology, vol. 278, no. 2, pp. 563-577, 2016.
- E. Rietzel, G. T. Chen, N. C. Choi, and C. G. Willet, "Four-[2] dimensional image-based treatment planning: Target volume segmentation and dose calculation in the presence of respiratory motion," International Journal of Radiation Oncology Biology Physics, vol. 61, no. 5, pp. 1535-1550, 2005.
- [3] K. Zhang, X. Liu, J. Shen, Z. Li, Y. Sang, X. Wu, Y. Cha, W. Liang, C. Wang, K. Wang et al., "Clinically applicable ai system for accurate diagnosis, quantitative measurements and prognosis of covid-19 pneumonia using computed tomography," Cell, 2020.
- [4] O. Ronneberger, P. Fischer, and T. Brox, "U-net: Convolutional networks for biomedical image segmentation," in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2015, pp. 234-241.
- [5] G. Litjens, T. Kooi, B. E. Bejnordi, A. A. A. Setio, F. Ciompi, M. Ghafoorian, J. A. Van Der Laak, B. Van Ginneken, and C. I. Sánchez, "A survey on deep learning in medical image analysis," Medical Image Analysis, vol. 42, pp. 60-88, 2017.
- S. Minaee, Y. Boykov, F. Porikli, A. Plaza, N. Kehtarnavaz, and [6] D. Terzopoulos, "Image segmentation using deep learning: A survey," arXiv preprint arXiv:2001.05566, 2020.
- L. Maier-Hein, A. Reinke, M. Kozubek, A. L. Martel, T. Arbel, [7] M. Eisenmann, A. Hanbury, P. Jannin, H. Müller, S. Onogur et al., "Bias: Transparent reporting of biomedical image analysis challenges," Medical Image Analysis, vol. 66, p. 101796, 2020.
- [8] F. Isensee, P. F. Jäger, S. A. Kohl, J. Petersen, and K. H. Maier-Hein, "nnu-net: a self-configuring method for deep learning-based<br>biomedical image segmentation," *Nature Methods*, 2020.
- Ö. Çiçek, A. Abdulkadir, S. S. Lienkamp, T. Brox, and O. Ron-[9] neberger, "3d u-net: learning dense volumetric segmentation from sparse annotation," in International Conference on Medical Image *Computing and Computer-Assisted Intervention, 2016, pp. 424–432.*
- [10] F. Milletari, N. Navab, and S.-A. Ahmadi, "V-net: Fully convolutional neural networks for volumetric medical image segmentation," in 2016 fourth international conference on 3D vision (3DV), 2016, pp. 565-571.
- [11] M. Drozdzal, E. Vorontsov, G. Chartrand, S. Kadoury, and C. Pal, "The importance of skip connections in biomedical image segmentation," in Deep Learning and Data Labeling for Medical Applications, 2016, pp. 179–187.
- [12] J. Ma and Z. Nie, "Exploring large context for cerebral aneurysm segmentation," arXiv preprint arXiv:2012.15136, 2020.
- [13] Y. Wang, X. Wei, F. Liu, J. Chen, Y. Zhou, W. Shen, E. K. Fishman, and A. L. Yuille, "Deep distance transform for tubular structure segmentation in ct scans," in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 3833–3842.
- [14] X. Li, W. Wang, X. Hu, and J. Yang, "Selective kernel networks," in Proceedings of the IEEE conference on computer vision and pattern recognition, 2019, pp. 510–519.
- [15] M. T. Löffler, A. Sekuboyina, A. Jacob, A.-L. Grau, A. Scharr,<br>M. El Husseini, M. Kallweit, C. Zimmer, T. Baum, and J. S. Kirschke, "A vertebral segmentation dataset with fracture grading," Radiology: Artificial Intelligence, vol. 2, no. 4, p. e190138, 2020.
- [16] A. Sekuboyina, M. Rempfler, A. Valentinitsch, B. H. Menze, and J. S. Kirschke, "Labeling vertebrae with two-dimensional reformations of multidetector ct images: An adversarial approach for incorporating prior knowledge of spine anatomy," Radiology: Artificial Intelligence, vol. 2, no. 2, p. e190074, 2020.
- [17] A. Sekuboyina, A. Bayat, M. E. Husseini, M. Löffler, M. Rempfler, J. Kukačka, G. Tetteh, A. Valentinitsch, C. Payer, M. Urschler et al., "Verse: A vertebrae labelling and segmentation benchmark," arXiv preprint arXiv:2001.09193, 2020.
- [18] C. Payer, D. Štern, H. Bischof, and M. Urschler, "Integrating spatial configuration into heatmap regression based cnns for landmark localization," Medical image analysis, vol. 54, pp. 207-219, 2019.
- [19] P. M. Full, F. Isensee, P. F. Jäger, and K. Maier-Hein, "Studying robustness of semantic segmentation under domain shift in cardiac mri," arXiv preprint arXiv:2011.07592, 2020.
- [20] Z. Yao, Y. Jiawei, H. Feng, L. Yang, W. Yixin, T. Jiang, Z. Cheng, Z. Yang, and H. Zhiqiang, "Semi-supervised cardiac image segmentation via label propagation and style transfer," arXiv preprint arXiv:2012.14785, 2020.

- [21] J. Ma, "Histogram matching augmentation for domain adaptation with application to multi-centre, multi-vendor and multi-disease cardiac image segmentation," arXiv preprint arXiv:2012.13871, 2020.
- [22] A. Lalande, Z. Chen, T. Decourselle, A. Qayyum, T. Pommier, L. Lorgis, E. de la Rosa, A. Cochet, Y. Cottin, D. Ginhac et al., "Emidec: A database usable for the automatic evaluation of myocardial infarction from delayed-enhancement cardiac mri," Data, vol. 5, no. 4, p. 89, 2020.
- [23] Y. Zhang, "Cascaded convolutional neural network for automatic myocardial infarction segmentation from delayed-enhancement cardiac mri," arXiv preprint arXiv:2012.14128, 2020.
- [24] J. Ma, "Cascaded framework for automatic evaluation of myocardial infarction from delayed-enhancement cardiac mri," arXiv *preprint arXiv:2012.14556, 2020.*
- [25] X.-Y. Zhou, J.-Q. Zheng, P. Li, and G.-Z. Yang, "Acnn: a full resolution dcnn for medical image segmentation," in 2020 IEEE International Conference on Robotics and Automation (ICRA), 2020, pp. 8455-8461.
- [26] M. Jun, "Segmentation loss odyssey," arXiv preprint arXiv:2005.13449, 2020.
- [27] V. Andrearczyk, V. Oreiller, M. Vallières, J. Castelli, H. Elhalawani, M. Jreige, S. Boughdad, J. O. Prior, and A. Depeursinge, "Automatic segmentation of head and neck tumors and nodal metastases in pet-ct scans," in Medical Imaging with Deep Learning, 2020, pp. 33-43.
- [28] V. Andrearczyk, V. Oreiller, M. Vallières, M. Jreige, J. O. Prior, and A. Depeursinge, "Overview of the hecktor challenge at miccai 2020: Automatic head and neck tumor segmentation in pet/ct," in Lecture Notes in Computer Science (LNCS) Challenges, 2021.
- [29] P. Goyal and H. Kaiming, "Focal loss for dense object detection," IEEE Transactions on pattern Analysis and machine intelligence, vol. 39, pp. 2999-3007, 2018.
- [30] D. P. Kingma and J. Ba, "Adam: A method for stochastic optimization," in Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2014.
- [31] M. Jun and Y. Xiaoping, "Combining cnn and hybrid active contours for head and neck tumor segmentation in ct and pet images," arXiv preprint arXiv:2012.14207, 2020.
- [32] D. Wang, H. Li, X. Wei, and X.-P. Wang, "An efficient iterative thresholding method for image segmentation," Journal of Computational Physics, vol. 350, pp. 657-667, 2017.
- [33] D. Wang and X.-P. Wang, "The iterative convolutionthresholding method (ictm) for image segmentation," arXiv *preprint arXiv:1904.10917, 2019.*
- [34] J. Ma, D. Wang, X.-P. Wang, and X. Yang, "A fast algorithm for geodesic active contours with applications to medical image segmentation," arXiv preprint arXiv:2007.00525, 2020.
- [35] X. Zhuang, "Multivariate mixture model for cardiac segmentation from multi-sequence mri," in International Conference on Medical Image Computing and Computer-Assisted Intervention, 2016, pp. 581-588
- -, "Multivariate mixture model for myocardial segmentation [36] . combining multi-source images," IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 12, pp. 2933-2946, 2018.
- [37] S. Zhai, R. Gu, W. Lei, and G. Wang, "Myocardial edema and scar segmentation using a coarse-to-fine framework with weighted ensemble," in Myocardial Pathology Segmentation Combining Multi-Sequence Cardiac Magnetic Resonance Images, X. Zhuang and L. Li, Eds., 2020, pp. 49–59.
- [38] N. Munan, B. Cheng, Y. Chenglang, and Z. Yefeng, "Ensembled resunet for anatomical brain barriers segmentation," arXiv preprint arXiv:2012.14567, 2020.
- [39] S. S. M. Salehi, D. Erdogmus, and A. Gholipour, "Tversky loss function for image segmentation using 3d fully convolutional deep networks," in International Workshop on Machine Learning in Medical Imaging, 2019, pp. 379–387.
- [40] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest et al., "The multimodal brain tumor image segmentation benchmark (brats)," IEEE Transactions on Medical Imaging, vol. 34, no. 10, pp. 1993-2024, 2014.
- [41] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. S. Kirby, J. B. Freymann, K. Farahani, and C. Davatzikos, "Advancing the cancer genome atlas glioma mri collections with expert segmenta-

13

tion labels and radiomic features,” Scientific data, vol. 4, p. 170117,
2017.

- [42] B. Spyridon, A. Hamed, S. Aristeidis, B. Michel, R. Martin, K. Justin, F. John, F. Keyvan, and D. Christos, "Segmentation labels and radiomic features for the pre-operative scans of the tcga-gbm collection," The Cancer Imaging Archive, 2017.
- [43] -, "Segmentation labels and radiomic features for the preoperative scans of the tcga-lgg collection," The Cancer Imaging Archive, 2017.
- [44] S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempfler, A. Crimi, R. T. Shinohara, C. Berger, S. M. Ha, M. Rozycki, and et al., "Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge," arXiv preprint arXiv:1811.02629, 2018.
- [45] F. Isensee, P. F. Jaeger, P. M. Full, P. Vollmuth, and K. H. Maier-Hein, "nnu-net for brain tumor segmentation," arXiv preprint arXiv:2011.00848, 2020.
- [46] J. Haozhe, C. Weidong, H. Heng, and X. Yong, "H2nf-net for brain tumor segmentation using multimodal mr imaging: 2nd place solution to brats challenge 2020 segmentation task," arXiv preprint *arXiv:2012.15318, 2020.*
- [47] Y. Wang, Y. Zhang, F. Hou, Y. Liu, J. Tian, C. Zhong, Y. Zhang, and Z. He, "Modality-pairing learning for brain tumor segmentation," arXiv preprint arXiv:2010.09277, 2020.
- [48] T. Poggio, A. Banburski, and Q. Liao, "Theoretical issues in deep networks," Proceedings of the National Academy of Sciences, vol. 117, no. 48, pp. 30 039-30 045, 2020.
- [49] E. Weinan, C. Ma, S. Wojtowytsch, and L. Wu, "Towards a mathematical understanding of neural network-based machine learning: what we know and what we don't," arXiv preprint arXiv:2009.10713, 2020.
- [50] H. Fengxiang and T. Dacheng, "Trecent advances in deep learning theory," arXiv preprint arXiv:2012.10931, 2020.
- [51] J. Ma, Z. Wei, Y. Zhang, Y. Wang, R. Lv, C. Zhu, G. Chen, J. Liu, C. Peng, L. Wang, Y. Wang, and J. Chen, "How distance transform maps boost segmentation cnns: An empirical study," in Medical Imaging with Deep Learning, vol. 121, 2020, pp. 479–492.
- [52] H. Kervadec, J. Bouchtiba, C. Desrosiers, E. Granger, J. Dolz, and I. Ben Ayed, "Boundary loss for highly unbalanced segmentation," *Medical Image Analysis*, vol. 67, p. 101851, 2021.
- [53] N. Kumar, R. Verma, D. Anand, Y. Zhou, O. F. Onder, E. Tsougenis, H. Chen, P.-A. Heng et al., "A multi-organ nucleus segmentation challenge," IEEE Transactions on Medical Imaging, vol. 39, no. 5, pp. 1380–1391, 2019.
- [54] T. Roß, A. Reinke, P. M. Full, M. Wagner, H. Kenngott, M. Apitz, H. Hempe, D. M. Filimon, P. Scholz, T. N. Tran et al., "Comparative validation of multi-instance instrument segmentation in endoscopy: results of the robust-mis 2019 challenge," Medical *Image Analysis*, p. 101920, 2020.
- [55] H. Fu, F. Li, X. Sun, X. Cao, J. Liao, J. I. Orlando, X. Tao, Y. Li, S. Zhang, M. Tan, C. Yuan, C. Bian, R. Xie, J. Li, X. Li, J. Wang, L. Geng, P. Li, H. Hao, J. Liu, Y. Kong, Y. Ren, H. Bogunović, X. Zhang, and Y. Xu, "Age challenge: Angle closure glaucoma evaluation in anterior segment optical coherence tomography," *Medical Image Analysis*, vol. 66, p. 101798, 2020.
- [56] V. Cheplygina, M. de Bruijne, and J. P. Pluim, "Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis," Medical Image Analysis, vol. 54, pp. 280  $-296.2019.$
- [57] J. E. Van Engelen and H. H. Hoos, "A survey on semi-supervised learning," Machine Learning, vol. 109, no. 2, pp. 373-440, 2020.
- [58] G.-J. Qi and J. Luo, "Small data challenges in big data era: A survey of recent progress on unsupervised and semi-supervised methods," IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.
- [59] N. Tajbakhsh, L. Jeyaseelan, Q. Li, J. N. Chiang, Z. Wu, and X. Ding, "Embracing imperfect datasets: A review of deep learning solutions for medical image segmentation," Medical Image Analysis, p. 101693, 2020.
- [60] D. Karimi, H. Dou, S. K. Warfield, and A. Gholipour, "Deep learning with noisy labels: exploring techniques and remedies in medical image analysis," Medical Image Analysis, vol. 65, p. 101759, 2020.
- [61] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, "Continual lifelong learning with neural networks: A review," Neural Networks, vol. 113, pp. 54-71, 2019.

- [62] A. Soltoggio, K. O. Stanley, and S. Risi, "Born to learn: the inspiration, progress, and future of evolved plastic artificial neural networks," *Neural Networks*, vol. 108, pp. 48–67, 2018.
- [63] S. C. Hoi, D. Sahoo, J. Lu, and P. Zhao, "Online learning: A comprehensive survey," *arXiv* preprint *arXiv*:1802.02871, 2018.
- [64] J. Ma, Y. Wang, X. An, C. Ge, Z. Yu, J. Chen, Q. Zhu, G. Dong, J. He, Z. He, C. Tianjia, Z. Yuntao, N. Ziwei, and Y. Xiaoping, "Towards data-efficient learning: A benchmark for covid-19 ct<br>lung and infection segmentation," *Medical Physics*, 2020.
- [65] J. Ma, Y. Zhang, S. Gu, Y. Zhang, C. Zhu, Q. Wang, X. Liu, X. An, C. Ge, S. Cao, Q. Zhang, S. Liu, Y. Wang, Y. Li, C. Wang, J. He, and X. Yang, "Abdomenct-1k: Is abdominal organ segmentation a solved problem?" arXiv preprint arXiv:2010.14808, 2020.