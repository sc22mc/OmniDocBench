# Sub-Gaussian Error Bounds for Hypothesis Testing

Yan Wang

Department of Statistics, Iowa State University 

Ames, IA 50011, USA

Email: wangyan@iastate.edu

Abstract—We interpret likelihood-based test functions from a geometric perspective where the Kullback-Leibler (KL) divergence is adopted to quantify the distance from a distribution to another. Such a test function can be seen as a sub-Gaussian random variable, and we propose a principled way to calculate its corresponding sub-Gaussian norm. Then an error bound for binary hypothesis testing can be obtained in terms of the sub-Gaussian norm and the KL divergence, which is more informative than Pinsker's bound when the significance level is prescribed. For  $M$ -ary hypothesis testing, we also derive an error bound which is complementary to Fano's inequality by being more informative when the number of hypotheses or the sample size is not large.

# I. INTRODUCTION

Hypothesis testing is one central task in statistics. One of its simplest forms is the binary case: given  $n$  independent and identically distributed (i.i.d.) random variables  $X_1^n \equiv (X_1,\ldots,X_n)$ , one wants to infer whether the null hypothesis  $H_0: X_i \sim P_0$  or the alternative hypothesis  $H_1: X_i \sim P_1$  is true. The binary case serves as an important starting point from which further results can be established, in the settings of both classical and quantum hypothesis testing [\[1\]](#), [\[2\]](#). With  $X_1^n$ , one can construct the empirical distribution  $\hat{P}_n = \frac{1}{n} \sum_{i=1}^n \delta_{X_i}$ , where  $\delta_X$  is the Dirac measure that puts unit mass at X. Adopting the Kullback-Leibler (KL) divergence as a distance from  $\hat{P}_n$  to  $P_0$  or  $P_1$ , one can construct a test function as

$$
\Phi(X_1^n) = I\{D_{\text{KL}}(\dot{P}_n \| P_0) - D_{\text{KL}}(\dot{P}_n \| P_1) > c\}, 
$$

(1)

where  $I\{\cdot\}$  is the indicator function,  $c \geq 0$  serves as a threshold beyond which the decision that  $\hat{P}_n$  is closer to  $P_1$  than to  $P_0$  is made, and  $D_{\mathrm{KL}}(P||Q) = \int \ln(dP/dQ)dP$  is the KL divergence from probability  $P$  to probability  $Q$  if  $P \ll Q$ . Conventionally, if  $P$  is not absolutely continuous with respect to  $Q$ , then  $D_{\mathrm{KL}}(P||Q) \equiv \infty$ . Note  $\hat{P}_n$  is discrete; hence if both  $P_0$  and  $P_1$  are discrete with the same support, (1) is well defined. Denote the densities of  $P_0$  and  $P_1$  with respect to the counting measure as  $p_0$  and  $p_1$ , respectively, and we have

$$
D_{\mathrm{KL}}(\hat{P}_n \| P_0) - D_{\mathrm{KL}}(\hat{P}_n \| P_1) = \frac{1}{n} \ln \left( \frac{\prod_{i=1}^n p_1(X_i)}{\prod_{i=1}^n p_0(X_i)} \right).
$$

(2)

In fact, in this case,  $(1)$  is equivalent to the test function for the likelihood ratio test [4]

$$
\Phi_{\mathrm{lrt}}(X_1^n) = I\left\{\frac{\prod_{i=1}^n p_1(X_i)}{\prod_{i=1}^n p_0(X_i)} > c'\right\},
$$

(3)

This research was supported in part by the US National Science Foundation under grant HDR: TRIPODS 19-34884.

where  $c' = e^{cn}$ . In the case that both  $P_0$  and  $P_1$  are continuous, the KL divergence difference  $D_{\text{KL}}(\hat{P}_n || P_0) - D_{\text{KL}}(\hat{P}_n || P_1)$  is not well defined. Nonetheless, the technically tricky part is the term “ $\int \hat{p}_n \ln(\hat{p}_n) d\mu$ ,” where we use  $\hat{p}_n$  to denote the density of  $\hat{P}_n$  with respect to the Lebesgue measure  $\mu$  as if it had one. But it appears twice and is cancelled out formally. We might conveniently *define* the KL divergence difference in this case as  $(2)$ , and still find the equivalence between  $(1)$  and  $(3)$ . Using the KL divergence in the context of hypothesis testing can be beneficial. Firstly, it provides a clear geometric meaning to the likelihood ratio test, as well as to the general idea underlying hypothesis testing. Secondly, it also offers a geometric, or even physical, interpretation of the lower bound for the resulting statistical errors, as shown below.

Under the null hypothesis  $H_0$ , the type I error rate (or the significance level)  $\alpha$  that is incurred by applying (1) for a fixed  $c$  is

$$
\alpha = \mathbb{E}_{X_1^n \sim P_\circ^{\otimes n}} \Phi(X_1^n),
$$

(4)

where  $P_0^{\otimes n}$  is the product probability measure for  $X_1^n$  under  $H_0$ . In practice, by prescribing the significance level, for example, letting  $\alpha = 0.05$ , one can derive the corresponding  $c$  and determine the desired test function. However, in this work, our focus is *not* to find a test function at given  $\alpha$ , we mainly deal with the case that  $c$  is fixed, and  $\alpha$  is obtained in a somewhat passive way. Thanks to the Neyman-Pearson lemma [3], the likelihood ratio test is known to be optimal in the sense of statistical power. Hence, given the incurred  $\alpha$ , test function (1) has the minimal type II error rate  $\beta$  among all possible test functions with the corresponding type I error rate no greater than  $\alpha$ :

$$
\beta = 1 - \mathbb{E}_{X_1^n \sim P_1^{\otimes n}} \Phi(X_1^n),
$$

(5)

where  $P_1^{\otimes n}$  is the product probability measure for  $X_1^n$  under the alternative hypothesis  $H_1$ .

Controlling statistical errors is of practical importance; however, typically one cannot suppress both types of error simultaneously. Under our i.i.d. setting, a classical result, based on Pinsker's inequality, concerning the error bound for any (measurable) test function is that [4]

$$
\alpha + \beta \ge 1 - \sqrt{\frac{n}{2}D_{\mathrm{KL}}(P_1 \| P_0)}.
$$

(6)

This result is striking in that without going into the details of calculating  $\alpha$  and  $\beta$ , one can have a *nontrivial* lower 

bound of their sum in terms of the KL divergence between two candidate probabilities, as long as the right-hand side of (6) is greater than 0. For a fixed  $n$ , this bound is solely determined by  $D_{\text{KL}}(P_1||P_0)$ , which reflects the “distance” from  $P_1$  to  $P_0$ . This result also has a significant physical meaning. At a nonequilibrium steady state, if  $P_1$  denotes the probability associated with observing a stochastic trajectory in the forward process, and  $P_0$  in the backward process, then the theory of stochastic thermodynamics tells us that  $D_{\text{KL}}(P_1 \| P_0)$ is equivalent to the average entropy production  $\Delta S$  in the forward process, which is always nonnegative [5], [6]. Hence, if one wishes to infer the arrow of time based on observations, then Pinsker's result (6) implies that the chance of making an error is high if  $\Delta S$  is small. In fact, we know that  $\Delta S = 0$ at equilibrium, and one cannot tell the arrow of time at all; hypothesis testing is just random guess in this case.

While (6) is useful, can we have a tighter and thus more informative bound? In this work, we will show that by taking advantage of the sub-Gaussian property of  $\Phi(X_1^n)$  [\[7\]](#page-9-1), [\[8\]](#page-9-1), one can derive a bound (15) on statistical errors in terms of its sub-Gaussian norm (as well as the KL divergence from  $P_1$  to  $P_0$ ). We name such an error bound as "sub-Gaussian" to highlight this fact. It turns out that it is a tighter bound than (6) in the sense that it provides a greater lower bound for  $\alpha + \beta$  (or for  $\beta$  at any given  $\alpha \ne 0.5$ ). In practice, a small  $\alpha$  is commonly set as the significance level, and our result can hopefully be more relevant. Moreover, in the case of M-ary hypothesis testing where  $M > 2$  hypotheses are present, we also derive a bound (19) for making incorrect decisions, which is complementary to the celebrated Fano's inequality [\[9\]](#page-9-1) when the number of hypotheses  $M$  or the sample size  $n$  is not large. The error bounds presented in this work are universal and easily applicable. We hope these findings can help better quantify errors in various statistical practices involving hypothesis testing.

# II. MAIN RESULTS

We will first introduce the sub-Gaussian norm of  $\Phi(X_1^n)$ . Then error bounds in the binary and  $M$ -ary cases are established, respectively.

## A. Sub-Gaussian norm of $\Phi(X_1^n)$

Sub-Gaussian random variables are natural generalizations of Gaussian ones. The so-called sub-Gaussian property can be defined in several different but equivalent ways [7], [8]. In this work, we pick one that suits most for our purposes.

**Definition 1.** A random variable  $X$  with probability law  $P$  is called sub-Gaussian if there exists  $\sigma > 0$  such that its central moment generating function satisfies

$$
\mathbb{E}_P e^{s(X - \mathbb{E}_P X)} \le e^{\sigma^2 s^2/2}, \ \forall s \in \mathbb{R}.
$$

**Definition 2.** The associated sub-Gaussian norm  $\sigma_{XP}$  of  $X$  with respect to  $P$  is defined as

$$
\sigma_{XP} \equiv \inf \{ \sigma > 0 : \mathbb{E}_P e^{s(X - \mathbb{E}_P X)} \le e^{\sigma^2 s^2/2}, \ \forall s \in \mathbb{R} \}.
$$

**Remark 1.**  $\sigma_{XP}$  is a well defined norm for the centered variable  $X - \mathbb{E}_P X$  [\[6\]](#page-9-1). It is the same for a location family of random variables that have different means but are otherwise identical. Also,  $\sigma_{XP}$  is equal to the  $\psi_{2}$ -Orlicz norm of  $X - \mathbb{E}_P X$  up to a numerical constant factor.

Lemma 1. A bounded random variable is sub-Gaussian. In particular, if  $X \in [a, b]$  almost surely with respect to P, then  $\sigma_{XP} \leq (b-a)/2$ .

*Proof.* This is a well known result that can be found in, for example, [7], [8].  $\square$ 

Test function  $(1)$  is an indicator function and takes on values in  $\{0, 1\}$ ; hence it is bounded. No matter what the law of  $X_1^n$  is,  $\Phi(X_1^n)$  is always sub-Gaussian by Lemma 1 with a uniform upper bound of its sub-Gaussian norm that

$$
\sigma_{\Phi P} \le 0.5. 
$$

(7)

However, if  $\alpha$  is fixed as a result of some  $c$  being used in (1), then a more informative sub-Gaussian norm for  $\Phi(X_1^n)$  can be obtained under the situation that  $X_1^n \sim P_0^{\otimes n}$ . In this case, by (4),

$$
\Pr(\Phi(X_1^n) = 1 | H_0) = \mathbb{E}_{X_1^n \sim P_0^{\otimes n}} \Phi(X_1^n) = \alpha,
$$

and one can explicitly write

$$
\mathbb{E}_{X_1^n \sim P_0^{\otimes n}} e^{s[\Phi(X_1^n) - \alpha]}
$$

$$
= \Pr(\Phi = 1 | H_0) e^{s(1-\alpha)} + \Pr(\Phi = 0 | H_0) e^{s(0-\alpha)}
$$

$$
= \alpha e^{s(1-\alpha)} + (1-\alpha)e^{-s\alpha} \equiv e^f.
$$

Using  $f$ , one can rewrite the sub-Gaussian property as

$$
h \equiv f - \frac{1}{2}\sigma^2 s^2 \le 0, \ \forall s \in \mathbb{R}.
$$

 (8)

Since  $\Phi$  is sub-Gaussian, there exists  $\sigma$  such that at any  $\alpha$ , we have  $h(s = 0) = 0$ , which is the maximal value of  $h$ . This fact implies  $\partial h/\partial s|_{s=0} = 0$  and  $\partial^2 h/\partial s^2|_{s=0} \leq 0$ . The latter poses a constraint on  $\sigma$ 's under which (8) holds:

$$
\frac{\partial^{2}h}{\partial s^{2}}\big|_{s=0} \leq 0 \Longrightarrow \sigma^{2} \geq \frac{\partial^{2}f}{\partial s^{2}}\big|_{s=0} = \alpha(1-\alpha).
$$
 (9)

Since  $\alpha(1-\alpha) \leq 0.25$ , we know the minimal universal  $\sigma$  for all  $\alpha$  is 0.5, consistent with (7).

For a specific  $\alpha$ , the minimal  $\sigma$  that makes (8) valid is denoted as  $\sigma_{\Phi_0}(\alpha)$ , which is defined to be the sub-Gaussian norm of  $\Phi(X_1^n)$  under the law  $\Phi_{\#}P_0^{\otimes n}$ , the push forward probability measure of  $P_0^{\otimes n}$  induced by  $\Phi$ . We may also simply state that  $\sigma_{\Phi_0}(\alpha)$  is the sub-Gaussian norm of  $\Phi(X_1^n)$  under  $H_0$ . The norm  $\sigma_{\Phi_0}(\alpha)$  can be numerically obtained in a principled way, as summarized in the following theorem.

**Theorem 1.** For  $\alpha \neq 0.5$ , besides the trivial solution  $(\sigma, 0)$  with any  $\sigma > 0$ , the equations

$$
\begin{cases} f = \frac{1}{2}\sigma^2 s^2, \\ \frac{\partial f}{\partial s} = \sigma^2 s, \end{cases}
$$

(10)

have only one nontrivial solution  $(\sigma^*, s^*)$  where  $s^* \neq 0$ . The sub-Gaussian norm of  $\Phi(X_1^n)$  under  $H_0$  is  $\sigma_{\Phi_0} = \sigma^*$ . For  $\alpha = 0.5, \sigma_{\Phi_0} = 0.5$ .

![](_page_2_Figure_0.jpeg)

Fig. 1. Assuming  $\alpha = 0.05$ , we show the main idea underlying Theorem 1, and numerically calculate  $\sigma_{\Phi 0}$ , which is the minimal  $\sigma$  such that  $h(s)$  is no greater than 0 for all  $s$  due to the sub-Gaussian property (8).

*Proof.* We will consider three cases based on the value of  $\alpha$ . *Case I:*  $\alpha = 0.5$ . In this case,  $\sigma_{\Phi 0}$  can be obtained directly by noticing

$$
\exp[f] = \cosh\left(\frac{s}{2}\right) = \sum_{n=0}^{\infty} \frac{(s/2)^{2n}}{(2n)!} \le \sum_{n=0}^{\infty} \frac{(s/2)^{2n}}{2^n n!}
$$

$$
= \exp\left(\frac{1}{2} \times 0.5^2 \times s^2\right).
$$

Hence by direct inspection,  $\sigma_{\Phi 0} = 0.5$ .

Case II:  $0 < \alpha < 0.5$ . Before diving into the proof, we briefly address the main idea first. Given  $\alpha$ , the function *h* depends on both *s* and  $\sigma$ . Requiring its maximum to be no greater than 0 at some  $\sigma$  naturally leads to two conditions that  $h(s,\sigma) = 0$  and  $\partial h(s,\sigma)/\partial s = 0$ , which are just (10). It is expected that  $\sigma_{\Phi 0}$  can be obtained from the corresponding nontrivial solutions, since it is the minimal  $\sigma$  that satisfies (8). Fig. 1 confirms this intuition, where  $\alpha = 0.05$  is assumed for illustration. By tuning  $\sigma$  to some  $\sigma^*$ , one can see the maximum of *h* at some  $s^* > 0$  can be exactly equal to 0, i.e.,  $h(s^*, \sigma^*) =$ 0. Also at this  $s^*$ , *h* is tangent to the *s*-axis, indicating that  $\partial h(s,\sigma^*)/\partial s|_{s=s^*}=0$ . Hence  $\sigma_{\Phi 0}=\sigma^*$ .

Now we turn to the proof. It is trivial that for any  $\alpha$ ,  $h$  attains its maximal value 0 at  $s = 0$ , no matter what  $\sigma > 0$  is. This does not provide much useful information of  $\sigma_{\Phi_0}$ . To proceed, we need a nontrivial local maximum of  $h(s)$  at some  $s \neq 0$ . Our first observation is that when  $0 < \alpha < 0.5$ , there is no local maximum achieved for  $s < 0$ , because  $\partial h/\partial s > 0$  for all  $s < 0$ . To see this, let  $a \equiv e^{-|s|(1-\alpha)}$ ,  $b \equiv e^{|s|\alpha}$ , and  $\delta = 0.5 - \alpha$ , then we have that

$$
\begin{split} \frac{\partial h}{\partial s} &= -\alpha (1-\alpha) \frac{b-a}{\alpha a + (1-\alpha)b} + \sigma^2 |s| \\ &= -\alpha (1-\alpha) \frac{1-e^{-|s|}}{(0.5+\delta) + (0.5-\delta)e^{-|s|}} + \sigma^2 |s| \\ & > -2\alpha (1-\alpha) \tanh(|s|/2) + \sigma^2 |s| \\ & > [\sigma^2 - \alpha (1-\alpha)] \times |s| \ge 0, \end{split}
$$

![](_page_2_Figure_8.jpeg)

Fig. 2. The sub-Gaussian norm  $\sigma_{\Phi 0}$  is plotted as a function of type I error  $\alpha$ . Since  $\sigma_{\Phi 0}$  is the same for  $\alpha$  and  $1-\alpha$ , we only plot the result for  $\alpha \in (0, 0.5]$ .

where  $\alpha < 0.5$  (hence  $\delta > 0$ ) is used in the first inequality, the second inequality is due to  $\tanh(x) < x$  for  $x > 0$ , and the last inequality is given by (9) since we have already known  $\Phi(X_1^n)$  is sub-Gaussian. This result indicates that the nontrivial maximum, if any, can only be found at some  $s > 0$ . 

For  $s > 0$ , following similar steps, we obtain

$$
\frac{\partial h}{\partial s} = \frac{\alpha(1-\alpha)}{\frac{1}{2}\coth\left(\frac{s}{2}\right) - \left(\frac{1}{2}-\alpha\right)} - \sigma^2 s
$$

and the condition  $\partial h/\partial s = 0$  then implies

$$
g(s) \equiv \frac{s}{2} \coth\left(\frac{s}{2}\right) = \left(\frac{1}{2} - \alpha\right)s + \frac{\alpha(1-\alpha)}{\sigma^2} \equiv l(s,\sigma).
$$

It is straightforward to check that  $g(s) \ge 1$  is a positive, monotonically increasing, and strongly convex function. Hence it can intersect the straight line  $l(s, \sigma)$  at no more than two points. Note  $g(0^+) = 1$ , and  $g'(0^+) = 0$ . The intercept of  $l(s, \sigma)$  is  $\alpha(1-\alpha)/\sigma^2 \in (0, 1)$  by (9), and the slope is greater than 0. Hence by tuning  $\sigma$ , it is always possible to make  $g(s)$  and  $l(s, \sigma)$  intersect twice. Denote these two points as  $s_1(\sigma)$  and  $s_2(\sigma)$ , respectively, with  $h(s_1) < h(s_2)$ . As shown in Fig. 1,  $h(s_1)$  is the minimum between two maxima  $h(s = 0)$  and  $h(s_2)$ . Then further requiring  $h(s_2(\sigma)) = 0$  at some  $\sigma^*$ , which is attainable since  $\Phi$  is known to be sub-Gaussian, we obtain  $\sigma_{\Phi 0} = \sigma^*$ , and Theorem 1 for the  $0 < \alpha < 0.5$  part is proved. 

*Case III:*  $0.5 < \alpha < 1$ . Note  $f(s)$  or  $h(s)$  is invariant under the transformations  $\alpha \leftrightarrow 1-\alpha$  and  $s \leftrightarrow -s$ . Hence  $\sigma_{\Phi 0}$  is the same for  $\alpha$  and  $1-\alpha$ .

Combining all three cases, we have proved Theorem 1.  $\square$ 

One can calculate  $\sigma_{\Phi 0}$  in a principled way with given  $\alpha$ , without knowing  $P_0$  or  $P_1$  or the constant  $c$  in the test function. We summarize the relation between  $\alpha$  and  $\sigma_{\Phi 0}$  for (1) in Fig. 2. Error bounds for hypothesis testing can now be established based on  $\sigma_{\Phi 0}$ .

## B. Sub-Gaussian bound for binary hypothesis testing

**Lemma 2.** Consider two general probability measures  $\nu$  and  $\mu$  on a common measurable space. Suppose  $\nu \ll \mu$ , and let 

$g \equiv d\nu/d\mu$  be the density of  $\nu$  with respect to  $\mu$ . Let Y be a sub-Gaussian random variable which is a function of  $X$  that has law  $\mu$  or  $\nu$ . Then we have

$$
|\mathbb{E}_{X\sim\nu}Y - \mathbb{E}_{X\sim\mu}Y| \leq \sigma_{Y_{\#}\mu}\sqrt{2D_{\mathrm{KL}}(\nu\|\mu)},
$$

(11)

where  $\sigma_{Y_{\#}\mu}$  denotes the sub-Gaussian norm of Y with respect to the push forward measure  $Y_{\#}\mu$ .

Recently, there have been several works with findings similar to Theorem 2, in the context of nonequilibrium statistical physics [6], data exploration or model bias analysis [10], [11], or uncertainty quantification for stochastic processes [12]. They can, however, be analyzed in a unified way based on the spirit in [13].

*Proof.* We have assumed  $\nu \ll \mu$  and  $g \equiv d\nu/d\mu$ . The associated entropy functional of g with respect to  $\mu$  is defined as  $\text{Ent}_{\mu}(g) = \int g \ln g d\mu$ . It is straightforward to find that

$$
\text{Ent}_{\mu}(g) = \int \frac{d\nu}{d\mu} \ln\left(\frac{d\nu}{d\mu}\right) d\mu = \int \ln\left(\frac{d\nu}{d\mu}\right) d\nu
$$

$$
= D_{\text{KL}}(\nu \|\mu). 
$$

(12)

On the other hand, by the variational representation of  $\text{Ent}_{\mu}(g)$ , we have that

$$
\text{Ent}_{\mu}(g) = \sup_{\eta} \int \eta g d\mu, \text{ with } \int e^{\eta} d\mu \le 1, 
$$

(13)

where  $\eta$  is a measurable function. We have  $\int \eta g d\mu = \mathbb{E}_{\mu} \eta g = \mathbb{E}_{\nu}\eta$  and  $\int e^{\eta}d\mu = \mathbb{E}_{\mu}e^{\eta}$ .

By assumption,  $Y(X)$  is sub-Gaussian. If  $X \sim \mu$ , then the sub-Gaussian norm of  $Y$  under the push forward measure  $Y_{\#\mu}$  is  $\sigma_{Y_{\#\mu}}$ . Let us construct  $\eta$  as

$$
\eta = s[Y(X) - \mathbb{E}_{X \sim \mu} Y(X)] - \frac{1}{2} \sigma_{Y_{\#}\mu}^2 s^2.
$$

It is clear that  $\mathbb{E}_{\mu}e^{\eta} \leq 1$  can be satisfied. Combining  $\eta$  with  $(12)$  and  $(13)$ , we arrive at

$$
\begin{split} D_{\mathrm{KL}}(\nu \| \mu) &\geq \mathbb{E}_{X \sim \mu} \eta(Y(X)) g \\ &= \mathbb{E}_{X \sim \mu} g \left[ s(Y - \mathbb{E}_{X \sim \mu} Y) - \frac{1}{2} \sigma_{Y \# \mu}^2 s^2 \right] \\ &= \mathbb{E}_{X \sim \nu} \left[ s(Y - \mathbb{E}_{X \sim \mu} Y) - \frac{1}{2} \sigma_{Y \# \mu}^2 s^2 \right] \\ &= s(\mathbb{E}_{X \sim \nu} Y - \mathbb{E}_{X \sim \mu} Y) - \frac{1}{2} \sigma_{Y \# \mu}^2 s^2, \end{split}
$$

which holds for any  $s \in \mathbb R$ . Hence, Lemma 2 is proved:

$$
\left|\mathbb{E}_{X\sim\nu}Y - \mathbb{E}_{X\sim\mu}Y\right| \leq \left[\inf_{|s|} \frac{D_{\mathrm{KL}}(\nu||\mu)}{|s|} + \frac{1}{2}\sigma_{Y_{\#\mu}}^2|s|\right]
$$

$$
= \sigma_{Y_{\#\mu}}\sqrt{2D_{\mathrm{KL}}(\nu||\mu)}.
$$

 $\Box$ 

**Theorem 2.** Suppose  $P_1 \ll P_0$ , and denote the sub-Gaussian norm of test function (1) under the null hypothesis  $H_0$  as  $\sigma_{\Phi 0}$ . Then we have

$$
\begin{split} |\mathbb{E}_{X_1^n \sim P_0^{\otimes n}} \Phi(X_1^n) - \mathbb{E}_{X_1^n \sim P_1^{\otimes n}} \Phi(X_1^n)| \\ &\leq \sigma_{\Phi 0} \sqrt{2n D_{\text{KL}}(P_1 \| P_0)}. \end{split} 
$$

(14)

*Proof.* Let  $X = X_1^n$ ,  $\nu = P_1^{\otimes n}$  and  $\mu = P_0^{\otimes n}$ , and due to the i.i.d. setting,  $D_{\text{KL}}(P_1^{\otimes n} || P_0^{\otimes n}) = n D_{\text{KL}}(P_1 || P_0)$ . Then the proof is completed by letting  $Y = \Phi(X_1^n)$  in Lemma 2.  $\Box$ 

Corollary 1. One has

$$
\alpha + \beta \ge 1 - \sigma_{\Phi 0} \sqrt{2n D_{\text{KL}}(P_1 \| P_0)}. 
$$

(15)

*Proof.* Insert definitions  $(4)$  and  $(5)$  into  $(14)$  and then simplify to obtain the result. □

**Remark 2.** Corollary 1 can be relaxed by replacing the sub-Gaussian norm  $\sigma_{\Phi_0}$  with one of its upper bounds. In fact, if we use the universal upper bound provided by  $(7)$ , then Corollary 1 reduces to Pinsker's classical result (6). However, our bound is always stronger in general. In particular, when controlling  $\alpha$  is more important than controlling  $\beta$ , one might set  $c > 0$  to put more emphasis on it. Hence for the same sample size n, the larger c is, the smaller  $\alpha$  and  $\sigma_{\Phi_0}$  are, resulting in a *tighter bound for*  $\beta$ .

**Remark 3.** There is another inequality from Theorem 2 that  $\alpha + \beta \leq 1 + \sigma_{\Phi 0} \sqrt{2nD_{\text{KL}}(P_1||P_0)}$ . But it is somewhat trivial because the bound is greater than  $1$  and in general does not provide much useful information. For example, one can always accept  $H_0$ , and for this trivial decision rule,  $\alpha = 0$ , but  $\beta \leq 1$  by definition. Hence  $\alpha + \beta \leq 1$ , and the extra term  $\sigma_{\Phi 0} \sqrt{2n D_{\text{KL}}(P_1 || P_0)}$  is not informative at all.

**Remark 4.** Suppose also  $P_0 \ll P_1$ , which is the usual case in hypothesis testing. Then by symmetry, it is straightforward to have

$$
\alpha + \beta \ge 1 - \sigma_{\Phi 1} \sqrt{2n D_{\text{KL}}(P_0 \| P_1)},
$$

(16)

where  $\sigma_{\Phi_1}$  is the sub-Gaussian norm of  $\Phi(X_1^n)$  under  $H_1$ , and it is a function of  $\beta$ . This result is nontrivially different than (15), not only because different norms are applied, but also because the KL divergence is not symmetric in two involved probabilities. Given (16), we can either bound  $\alpha$  when  $\beta$  is given or bound  $\beta$  in an implicit way when  $\alpha$  is given.

**Remark 5.** Similar to (6), our bound is also nonasymptotic in nature as it holds for any finite  $n$ . The expense we pay for this, however, is that in the large  $n$  and small  $\alpha$  limit, our bound for  $\beta$  is not as tight as Stein's lemma which states that  $\beta \sim e^{-nD_{\text{KL}}(P_0||P_1)}$  [\[4\]](#page-9-1).

## C. Sub-Gaussian bound for $M$ -ary hypothesis testing

A generalization of our result to the  $M$ -ary hypothesis testing can be obtained. Suppose there are  $M$  hypotheses, represented by the corresponding probability distributions  $\{P_1,\ldots,P_M\}$ . Suppose from one of such distributions  $P_{i_0}$ ,  $n$  data points  $X_1^n$  are drawn independently. Our task is to infer the hypothesis index  $i_0$  from data. Similar to (1), let us consider the test function for the  $i$ -th hypothesis as

$$
\varphi_i(X_1^n) = \prod_{j \neq i} I\{D_{\text{KL}}(\hat{P}_n || P_j) - D_{\text{KL}}(\hat{P}_n || P_i) > c_i\},
$$

with  $\varphi = 1 - \Phi$  in the binary case. We will consider the case  
that  $c_i = 0$  for all  $i \in \{1, ..., M\}$ . Unlike in the binary case

where  $c > 0$  can be adopted to intentionally render a small  $\alpha$ , the test function here is purely likelihood-based without any prescribed preference over any particular hypothesis. It is known that this approach minimizes  $\alpha + \beta$  in the binary case (the Bayes classifier). From  $M$  such test functions  $\varphi_i$ , one can construct a random vector  $\boldsymbol{\varphi} = (\varphi_1, \ldots, \varphi_M)$ . Assume there always exists a single index  $i_0$  such that

$$
D_{\mathrm{KL}}(\hat{P}_n \| P_j) - D_{\mathrm{KL}}(\hat{P}_n \| P_{i_0}) > 0
$$

holds for all  $j \neq i_0$ . In this case,  $\varphi_{i_0} = 1$ , and  $\varphi_j = 0$  for  $j \neq i_0$ . Since  $X_1^n$  is random, we expect that  $i_0$  may differ for each realization. However, it is almost surely with respect to all  $P_i$ 's that

$$
\sum_{i=1}^{M} \varphi_i(X_1^n) = 1.
$$

(17)

Under  $M$  hypotheses, we can construct a matrix, denoted  $\mathbb{E}\varphi$ , that encodes the error incurred in testing:

$$
\mathbb{E}\boldsymbol{\varphi} \equiv \left( \begin{array}{ccc} \mathbb{E}_{1}\varphi_{1} & \cdots & \mathbb{E}_{1}\varphi_{M} \\ \vdots & \ddots & \vdots \\ \mathbb{E}_{M}\varphi_{1} & \cdots & \mathbb{E}_{M}\varphi_{M} \end{array} \right),
$$

(18)

where the matrix element  $\mathbb{E}_i \varphi_j \equiv \mathbb{E}_{X_i^n \sim P_i^{\otimes n}} \varphi_j$ . By (17), the row sum of  $\mathbb{E}\varphi$  is 1. The diagonal elements of  $\mathbb{E}\varphi$  are actually the probabilities that the underlying hypothesis is correctly identified. In other words, the probability of making an incorrect decision when the data are generated from the *i*th hypothesis is  $\alpha_i \equiv 1 - \mathbb{E}_i \varphi_i$ . We denote  $\alpha_{\max} \equiv \max_i \alpha_i$ . The following theorem provides a lower bound to  $\alpha_{\text{max}}$  that is complementary to Fano's inequality.

**Theorem 3.** Suppose  $P_i \ll P_j$  for all  $i, j \in \{1, ..., M\}$ . For any  $j \in \{1, ..., M\}$ , we have

$$
\alpha_{\max} \ge 1 - \frac{1}{M} - \frac{1}{M} \sum_{i=1}^{M} \sigma_{\varphi_i} \sqrt{2nD_{\mathrm{KL}}(P_j \| P_i)},
$$

(19)

where  $\sigma_{\varphi_i}$  is the sub-Gaussian norm of  $\varphi_i$  with respect to the *i*th hypothesis.

*Proof.* First note that  $\varphi_i$  is sub-Gaussian since it takes on values in  $\{0,1\}$ . If  $\alpha_i$  is fixed, then the sub-Gaussian norm  $\sigma_{\varphi_i}$  can be calculated similarly as in the binary case. Even  $\alpha_i$  is unknown, by  $(14)$ , we can formally have

$$
\mathbb{E}_{i}\varphi_{i} \leq \sigma_{\varphi_{i}}\sqrt{2nD_{\mathrm{KL}}(P_{j}\|P_{i}) + \mathbb{E}_{j}\varphi_{i}}.
$$

(20)

Summing over  $i$  and combining (17), we find

$$
M(1 - \alpha_{\max}) \leq \sum_{i=1}^{M} \mathbb{E}_{i} \varphi_{i} \leq 1 + \sum_{i=1}^{M} \sigma_{\varphi_{i}} \sqrt{2nD_{\mathrm{KL}}(P_{j}||P_{i})}.
$$

Finally, we arrive at (19) by rearranging the terms. Hence the proof is completed.  $\Box$ 

If we aim at lower bounding  $\alpha_{\text{max}}$ , then using the sub-Gaussian norm  $\sigma_{\varphi_i}$  in Theorem 3 seems not useful practically, since  $\sigma_{\varphi_i}$  itself depends on  $\alpha_i$ . Nonetheless, due to the

universal upper bound  $(7)$ , we can have a relaxed version of  $(19)$  as in the corollary below.

**Corollary 2.** For any  $j \in \{1, ..., M\}$ , we have

$$
\alpha_{\max} \ge 1 - \frac{1}{M} - \frac{1}{M} \sum_{i=1}^{M} \sqrt{\frac{n}{2} D_{\text{KL}}(P_j \| P_i)}, 
$$

(21)

or, using the mean square root of KL divergences, we have

$$
\alpha_{\max} \ge 1 - \frac{1}{M} - \frac{1}{M^2} \sum_{i,j=1}^{M} \sqrt{\frac{n}{2} D_{\text{KL}}(P_j \| P_i)}. 
$$

(22)

*Furthermore, if*  $D_{KL}(P_i||P_j) \leq \delta$  *holds for each pair of i and j, then*

$$
\alpha_{\max} \ge 1 - \frac{1}{M} - \sqrt{\frac{n}{2}}\delta.
$$

(23)

**Remark 6.** It is interesting to compare (23) with Fano's inequality [\[9\]](#page-9-1), which, under the same assumption that all KL divergences are uniformly bounded by  $\delta$ , states that

$$
\alpha_{\text{max}}^{\text{Fano}} \ge 1 - \frac{n\delta + \ln 2}{\ln(M-1)}.
$$

(24)

As evidenced by the scalings of  $M$  and  $n$  in (23) and (24), respectively, there is a region that our result outperforms Fano's in the sense that it provides a greater lower bound for  $\alpha_{\text{max}}$ . Qualitatively, this happens when at least one of the number of hypotheses  $M$  and the sample size  $n$  is not large. For example, when  $M = 3$ , Fano's inequality is trivial, while our result can still work nontrivially.

# III. CONCLUSION AND DISCUSSION

In this work, by using the sub-Gaussian property of test functions, we uncover two universal error bounds in terms of the sub-Gaussian norm and the Kullback-Leibler divergence. In the case of binary hypothesis testing, our bound (15) is always tighter than Pinsker's bound (6) for any given  $\alpha \neq$  0.5. In the case of  $M$ -ary hypothesis testing, our result (19) is complementary to Fano's inequality (24) by providing a more informative bound when the number of hypotheses or the sample size is not large.

Given the universality of our results, we hope, with possible generalizations, they can find potential applications in fields ranging from clinical trials to quantum state discrimination. In particular, the quantum extension of these bounds is of special interest. Due to the experimental cost, it may be important to quantify statistical errors in the presence of a *limited* number of observations, and nonasymptotic rather than asymptotic results are thus more relevant. Both our bounds hold for any finite sample size, and can hopefully be helpful in such cases.

# ACKNOWLEDGMENT

YW gratefully thank Prof. Dan Nettleton for helpful discussions that stimulated this work and Prof. Huaiqing Wu for a careful review of the manuscript and insightful comments.

# REFERENCES

- [1] R. W. Keener, Theoretical Statistics: Topics for a Core Course. New York, NY, USA: Springer, 2010.
- [2] M. Hayashi, Quantum Information Theory: A Mathematical Foundation, 2nd ed. New York, NY, USA: Springer-Verlag, 2017.
- [3] J. Neyman and E. S. Pearson, IX, "On the problem of the most efficient tests of statistical hypotheses," Phil. Trans. R. Soc. London, vol. A231, pp. 289-337, April 1933.
- [4] T. M. Cover and J. A. Thomas, Elements of Information Theory. Hoboken, NJ, USA: John Wiley & Sons, 1991.
- [5] U. Seifert, "Entropy production along a stochastic trajectory and an integral fluctuation theorem," Phys. Rev. Lett., vol. 95, no.4, July 2005, Art. no. 040602.
- [6] Y. Wang, "Sub-Gaussian and subexponential fluctuation-response inequalities," Phys. Rev. E, vol. 102, no. 5, November 2020, Art. no. 052105.
- [7] M. J. Wainwright, High-Dimensional Statistics: A Non-Asymptotic Viewpoint. Cambridge, U.K.: Cambridge University Press, 2019.
- [8] R. Vershynin, High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge, U.K.: Cambridge University Press. 2018.
- [9] R. Fano, Transmission of Information: A Statistical Theory of Communications. Cambridge, MA, USA: M.I.T. Press, 1961.
- [10] D. Russo and J. Zou, "How much does your data exploration overfit? Controlling bias via information usage," IEEE Trans. Inf. Theory, vol. 66, no. 1, pp. 302–323, January 2020.
- [11] K. Gourgoulias, M. A. Katsoulakis, L. Rey-Bellet, and J. Wang, "How biased is your model? Concentration inequalities, information and model bias," IEEE Trans. Inf. Theory, vol. 66, no. 5, pp. 3079-3097, May 2020.
- [12] J. Birrell and L. Rey-Bellet, "Uncertainty quantification for Markov processes via variational principles and functional inequalities," SIAM/ASA J. Uncertain. Quantif. vol. 8, no. 2, pp. 539-572, April 2020.
- [13] S. G. Bobkov and F. Götze, "Exponential integrability and transportation cost related to logarithmic Sobolev inequalities," J. Funct. Anal., vol. 163, no. 1, pp. 1-28, April 1999.