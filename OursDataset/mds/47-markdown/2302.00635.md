# Shared SAT Solvers and SAT Memory in Distributed Business Applications

Sergejs Kozlovičs

Institute of Mathematics and Computer Science, University of Latvia 

Raina bulv. 29, Riga, Latvia, LV-1459 

sergejs.kozlovics@lumii.lv

 **Abstract**  We propose a software architecture where SAT solvers act as a shared network resource for distributed business applications. There can be multiple parallel SAT solvers running either on dedicated hardware (a multi-processor system or a system with a specific GPU) or in the cloud. In order to avoid complex message passing between network nodes, we introduce a novel concept of the shared SAT memory, which can be accessed (in the read/write mode) from multiple different SAT solvers and modules implementing the business logic. As a result, our architecture allows for the easy generation, diversification, and solving of SAT instances from existing high-level programming languages without the need to think about the network. We demonstrate our architecture on the use case of transforming the integer factorization problem to SAT.

**Keywords:** SAT  $\cdot$  distributed applications  $\cdot$  software architecture  $\cdot$  integer factorization

# 1 Introduction

The Boolean satisfiability problem (SAT) has many practical applications such as circuit design, model generation and verification, planning, software package management, program analysis, and other constraint satisfaction problems  $[7,22,9,13]$ . Since SAT is an NP-complete problem, a polynomial reduction exists for any other problem from the NP class, making SAT a "silver bullet" for the  $\text{NP class}^1$ . Even if some NP problem is not known to be NP-complete (such as integer factorization or graph isomorphism), reducing it to SAT can be a reasonable temporary measure until a specific efficient algorithm is found (if it  $\text{exists}.$ 

While no polynomial algorithm is known for SAT, and we do not know whether it exists (the  $P=NP$  problem), numerous techniques have been proposed to solve SAT efficiently. They include heuristics, conflict-driven clause learning, backjumping, random restarts, message passing, and machine learning  $[18,6,19]$ . As a result, state-of-the-art SAT solvers are able to find variable assignments

 $1$  Though, finding the most direct low-degree polynomial reduction to SAT can be a challenge.

 2 Sergejs Kozlovičs

for SAT instances with tens of thousands of variables and clauses, meaning that many practical SAT applications are now tractable.

While some solvers are optimized to run on a single-core CPU (e.g., Glusoce and GSAT), others take advantage specific of hardware, e.g., HordeSAT utilizes multiple CPU cores [3,11,4]. At  $IMCS^2$ , we have developed our own solver QuerySAT, which uses graph neural networks internally and requires a  $\text{GPU}^3$ for better performance  $[21]$ .

Solvers that use branching (e.g., Maple $SAT^4$  and Lingeling<sup>5</sup>) can be used together with parallel solvers that can diversify a single SAT instance into multiple subtasks to be executed in parallel. Besides, multiple different solvers can be launched in parallel on the same SAT instance, hoping that one of them will find the solution faster. Furthermore, we can combine solvers, resulting in hybrid ones, e.g., we can replace the query mechanism in our QuerySAT with a third-party solver.

Since SAT solvers are computationally intense and often require specific hardware, we consider the task of making SAT solvers available as a shared resource deployed either to the dedicated on-premise hardware on to the cloud servers. In this paper, we cover the following tasks:

- 1. Integrating SAT solvers (as a shared resource) into distributed business software.

- 2. Optimizing the communication between multiple parallel SAT solvers (e.g., for diversification and learned clause exchange).

Our idea is to introduce the shared SAT memory, which can be accessed from all involved modules (SAT solvers and business software components).

The next section provides the terminology and introduces the web kernel concept- an OS kernel analog for distributed applications. The web kernel will act as the communication broker between SAT solvers, business logic units, and SAT memory. We continue by providing essential implementation details. In particular, we show how to implement SAT memory and manage SAT solvers on multiple shared hardware units. In Section 5, we provide a usage scenario based on integer factorization. Finally, we discuss related work and sketch further research directions.

# 2 Definitions

A distributed application is a program that runs on more than one autonomous computer that communicate over a network. Usually, it consists of two separate programs: the back-end (server-side software) and the front-end

<sup>2</sup> Institute of Mathematics and Computer Science, University of Latvia

<sup>3</sup>  such as Nvidia T4 16GB GPU (used in our experiments)

<sup>4</sup> MapleSAT has multiple configurations. It is based on MiniSAT [10];

see https://sites.google.com/a/gsd.uwaterloo.ca/maplesat/maplesat.

<sup>5</sup>  https://github.com/arminbiere/lingeling

Shared SAT Solvers and SAT Memory in Distributed Business Applications 3

(client-side) software. However, multiple server and client nodes are also possible.

**The web computer** is an abstraction that simplifies the development of distributed applications by providing the illusion of a single target computer  $[15,16]$ . The web computer factors out the network and multi-user/multi-process management. As a result, distributed applications can be created like traditional desktop applications, i.e., by focusing on just one program, one target computer, and one user.

The web computer consists of the following main parts:

- $-$  **data memory** (or web memory), which is shared among network nodes (e.g., the client and the server); since it is constantly being automatically and transparently synchronized, each network node can access web memory as if it was directly attached:

- $-$  the **instruction memory** (or code space), where executable/interpretable code is placed; the particular code is delivered only the nodes being able to execute/interpret it (e.g., Java and Python code is installed at the serverside and executed there, while JavaScript code can be delivered and executed at the client browser as well as at the server-side by means of a JavaScript  $\text{engine such as node.is)}$

- $-$  web processors, which are software modules that can execute certain types of instructions from the code space;

- **web**  $I/O$  devices, which can be either physical devices (e.g., printers) directly attached to particular physical nodes and available via specific APIs, or virtual devices implemented as software running either on a single node or in a cluster (e.g., databases or file systems).

Notice that due to security considerations, the web computer separates instruction memory from data memory. Thus, it corresponds to the Harvard architecture (as opposed to von Neumann architecture). That protects server-side code from being able to execute code from the shared data memory, which the client can modify (and we should not trust the client) [2].

Although the web computer still requires multiple physical network nodes to operate, web applications do not access them directly but via the intermediate layer, making the single computer illusion possible. Since this intermediate layer acts as an operating system analog, its reference implementation is called  $webAppOS$  (available at http://webappos.org).

The **web kernel** is the abstraction layer used by webAppOS applications to access web memory, to invoke code from the instruction memory (the code will be executed by some web processor), and to access web  $I/O$  devices (Figure 1). The web kernel factors out the network communication and provides the illusion that all web computer components are located at the same network node as the calling application code. Since code invocations may require network communication, they are called **web calls**. Web calls are enqueued and forwarded to the corresponding web processors by the web kernel. All web calls are asynchronous (however, the async/await capabilities of modern programming languages can be used as a syntax sugar to simplify asynchronous code).

4 Sergejs Kozlovičs

![](_page_3_Figure_1.jpeg)

 $Fig. 1.$  webAppOS web kernel as an abstraction that factors out the network

In fact, web memory is a graph-like structure for storing objects, their states (attributes), and the links between these objects. Besides, web memory also stores the meta-level information (object classes, inheritance relations, etc.). Thus, web memory resembles the OOP memory used by the Java virtual machine. In web memory, multiple inheritance is supported.

The code space, in its turn, stores executable code as **web methods**, which are either functions or methods (depending on a programming language). Each web method is identified by an implementation-agnostic fully-qualified name (e.g., ClassName.methodName). Thus, a web call is specified by a fully-qualified method name, the web memory slot, the web object reference in that slot (a this or self analog), and a JSON object as an argument. Each web call must also return a JSON object (at least an empty JSON {}, if no result is expected). Errors (including network errors) are returned in the error field.

Different methods of the same class can be written in different programming languages (we call that approach **per-method granularity**). Obviously, there must be web processors (among all nodes) that are able to execute different types of code used in the given webAppOS application.

Since methods can be executed by different web processors (which can be located on different network nodes), it is not possible to implement static linking. The process or substituting a particular web call with the corresponding method implementation is called **web linking**. Due to dynamic and distributed nature of web linking, we use the **duck typing** mechanism to define the *implements* relationship between web objects and interfaces. $^6$ 

Each webAppOS application can be used by multiple users, and each user can have multiple concurrent sessions with the same application. Each such application instance is called a **web process**; it has a dedicated web memory slot identified by  $\mathbf{webPid}$  (in essence, UUID).

<sup>6</sup> If interface  $I$  is a set of web methods  $m_1,m_2,\ldots,$  and the object  $o$  is included in web memory classes  $C_1, C_2, \ldots$ , then o implements I, iff  $\forall m_i \exists C_j$ :  $\text{ClassName}(C_j).\text{methodName}(m_i) \in \text{Code Space}.$ 

Shared SAT Solvers and SAT Memory in Distributed Business Applications 5

Web  $I/O$  devices are implemented as code libraries that define device-specific classes and provide implementations for the device-specific methods. For real hardware, these methods access the device via native drivers or OS calls; for virtual devices, they provide software implementation. Since web  $I/O$  devices are shared among all web processes, there is a dedicated web memory slot called the **root web memory**, where web  $I/O$  devices (with their states) are stored. Thus, device-specific methods can be invoked via the web kernel in the same way as invoking ordinary web calls. Internally, however, device-specific web calls are  $\text{implemented like software interrupts}$  — they take precedence over ordinary web calls in web processors. This is done intentionally since we may need to interrupt the previous command sent to the device or to release the device for other users as soon as possible.<sup> $7$ </sup>

The web kernel is already available as part of webAppOS. In our architecture, it will act as the communication broker between SAT solvers, business logic units, and the SAT memory, all of which can be located at different network nodes.

# 3 Integrating SAT Solvers

Each SAT solver is integrated as a virtual web  $I/O$  device (i.e., implemented as a software unit). In the root web memory, different types of SAT solvers are represented as subclasses of the SatSolver superclass, while each particular solver instance is represented as an object of the corresponding subclass. Typically, the number of solver instances will correspond to the number of free CPU cores (i.e., not occupied by web processors); however, the solver type and the required hardware (e.g., GPU) can also affect the number of solvers at each node. In any case, all available solvers among all nodes can be easily detected by traversing the root web memory.

## 3.1 Per-Solver Concurrency 

The SAT instance can be a standalone SAT problem, a subtask of a larger problem, or its diversified variant. Since solving SAT is a computationally intensive task, we intentionally limit each solver to processing only one SAT instance at a time. However, since the solver can be invoked by different users concurrently, it has to be implemented in a thread-safe way, returning an error attribute when  $necessary.$ 

Each solver has to provide the following web methods ( $=$ web I/O device  $\text{interrupts})$ :

 $-$  solve({satMemoryUrl, timeout, diversification})  $-$  starts the solver on a SAT instance stored in the given SAT memory using the given diversification guidelines (the SAT memory is discussed in the next section, while diversification is discussed in the following subsection).

<sup>7</sup>  This implies that web I/O devices must be implemented in a thread-safe way.

 6 Sergejs Kozlovičs

The solver gets the clauses from the SAT memory, diversifies itself, solves the SAT instance, and returns the result attribute equal to SAT, UNSAT, or UNKNOWN. The latter value is returned when neither the satisfying variables assignment, nor the contradiction could be devised (e.g., for heuristics-based or randomized solvers). The UNKNOWN value is also returned when the solver has been interrupted. In case of SAT, the variable assignments are returned in the model attribute (a JSON array).

Suppose the solver is already solving another SAT instance. In that case, the solve method must return the BUSY flag in the error attribute or wait at most timeout seconds for the solver to become available.

- $pause({})$  requests the solver to interrupt the search as soon as possible with the ability to resume the search (e.g., the call stack has to be  $\text{maintained});$

- $\text{resume}(\{\})$  resumes the previously interrupted search;

- $-$  cancel( $\{\}$ ) interrupts the search for good for the SAT instance currently being processed. As a result, the currently running solve web call must return UNKNOWN.

We assume that in case of any error (e.g., when a non-paused solver is requested to resume, or when the memory/CPU limits have been exceeded), the  $\texttt{error}$  attribute is returned. Besides, since the web calls above are web I/O device interrupts, webPid of the caller web process is always implicitly passed. That allows the solver to verify, for example, whether the pause web call originated from the same web process that had invoked solve,  $-$  a helpful security precaution.

## 3.2 Diversification

The following diversification settings can be passed to the solve web call. We borrowed them from the HordeSAT portfolio-based solver and transformed them to the JSON syntax to comply with the requirements of the web kernel [4]. Each solver type can implement diversification differently.

- $-$  rank the index of this particular solver among all solvers working on the current SAT problem; as a trivial example, rank could be used to initialize the random number generator seed;

- $-$  size the number of solvers working on the current SAT problem;

- $-$  phases an optional JSON object with attributes named  $xi$ , e.g.,  $x5$ ,  $1 \leq$  $i \leq \#(\text{SAT variables})$ ; for variable  $x_i$ , phases  $[x_i]$  denotes the Boolean value to try first during the search (the variable "phase").

Notice that although each solver in the root web memory is treated as a single instance, it can rely on multiple parallel solvers (e.g., multiple GPU subprograms) inside. Such "internals solvers" are not visible to the web kernel and cannot be diversified in the way described above.

Shared SAT Solvers and SAT Memory in Distributed Business Applications 7

## 3.3 Joining Parallel Solvers

A solver can divide the task into subtasks and forward them to multiple parallel solvers via the **parallelize**, a built-in web call provided by the web kernel. The **parallelize** web call takes a list  $L$  of child web calls to be invoked in parallel, when possible. Technically, parallilize stores a counter in web memory and appends the join method (also built-in) invocation to each web call from  $L$ . The join method increments the counter and stores the return value of the child web call. When the counter reaches the length of  $L$ , parallelize returns the list of child return values, which can be processed by the parent solver.

# 4 Implementing the SAT Memory 

Like SAT solvers, each SAT memory instance is also represented as a virtual web  $I/O$  device. The reason for such design choice is to avoid introducing a new memory type, which could complicate the web kernel architecture. Unlike web memory, SAT memory is not used in all web processes. Besides, depending on the normal form used to represent the clauses (e.g., CNF or ANF), there could be different types of SAT memory<sup>8</sup>. In addition, certain web methods (such as converters between normal forms) may need multiple types of SAT memory at the same time.

In this section, we focus on SAT memory with clauses given in the conjunctive normal form  $(CNF)$  since it is used in the DIMACS file format, a *de facto* standard supported by the majority of SAT solvers. Besides, it is used at SAT  $\text{competitions}^9$ .

SAT memory instances with CNF clauses are represented as instances of the SatCnf web class in the root web memory. Unlike instances of SatSolver, which are preconfigured in advance, SatCnf objects can be created and deleted dynamically. Such an approach resembles the process of attaching and detaching external data storage (e.g.,  $USB$  drives) to the PC.

The CNF SAT memory consists of free variables  $x_1, x_2, \ldots, x_n$  and CNF clauses in the form  $(l_1, l_2, \ldots, l_k)$ , where each  $l_i, 1 \leq l_i \leq n$ , is an integer representing a literal:

$$
\begin{array}{c} x_{l_i} \text{ if } l_i > 0\\ \neg x_{l_i} \text{ if } l_i < 0 \end{array}
$$

The number of free variables can be increased at runtime. Additional variables can be introduced, for example, to avoid exponential formula growth while converting Boolean formulas to CNF (resulting in an equisatisfiable formula, not equivalent). Besides, specific formula fragments can be easier expressible by introducing helper variables.

SatCnf objects are created by specifying the initial number of Boolean variables.

<sup>8</sup>  In this paper, we cover CNF only.

<sup>9</sup> https://satcompetition.github.io

8 Sergejs Kozlovičs

The **SatCnf** web class implements the following web methods (in a thread- $\text{safe way}$ :

- $-$  addVariable( $\{\}$ ) adds a new free variable and returns its index (a positive integer);

- $-$  addClause({clause}) adds a new CNF clause as an array of integers  $(l_1, l_2, \ldots)$  representing literals, where  $1 \leq |l_i| \leq \#$  (free variables). In order to avoid duplicate clauses, we sort literals before adding the clause. However, a Bloom filter could also be used instead.

- $-$  clauses ( $\{\}$ ) returns the array of clauses currently stored in the SAT memory, e.g.,  $[[1,-2,3],[4,-5]]$ ;

- $fork(\{detach\})$  returns a new  $SatCnf$  instance that retains the clauses of the current SAT memory with the ability to add new clauses to the forked SAT memory only.<sup>10</sup> If detach=true, further changes to the original SAT are not reflected in the fork Otherwise, the fork will share free variables with the origin and will get clause updates.

Forks are helpful for defining SAT subtasks that can be passed to solve. For example, specific variable assignments can be added as single-literal clauses. Learned clauses (in CDCL-based solvers) can also be added to the forked memory.

Notice that we do not have methods to read and write the assignments of the variables in SAT memory. Although counter-intuitive, that protects SAT memory from possible collisions between different variable assignments by different solvers (if the solution is not unique, or in case of a partial solution).

While the methods above are convenient to constructing the initial SAT instance, invoking them via web calls involves certain serialization/deserialization overhead, which is undesirable when multiple solvers access SAT memory. That would also negatively impact portfolio-based solvers, which need to exchange learned clauses at runtime.

In order to minimize the web kernel overhead, we introduce *direct access* to SAT memory, resembling the DMA<sup>11</sup> feature in traditional hardware computing systems. We implement it by means of web sockets $^{12}$ .

Wes sockets of a SAT memory instance can be accessed by the address available in the **directUrl** attribute. All web socket connections are managed by an internal hub of the SatCnf class. When a new variable or clause is added via one web socket, the hub forwards them to other web sockets. This way, new variables and clauses can be efficiently exchanged between all other nodes using the same  $\text{SAT}$  memory.

In a nutshell, the protocol used in web sockets uses binary messages corresponding to the SatCnf methods. However, addVariable and addClauses can

<sup>8</sup> Sergejs Kozlovičs

 $^{10}$  Technically, in order to avoid copying of clauses, the common part with the original SAT memory is factored out and stored only once.

 $^{11}$  Direct memory access

 $^{12}$  the extension of the HTTP protocol for highly-efficient bi-directional communication via the network

Shared SAT Solvers and SAT Memory in Distributed Business Applications 9

be both sent and received. When received, the client shall treat them as variable and clause synchronization messages from other solvers.

In addition to the addVariable message, we introduce also the messages  $\text{(sent by the client) for:}$ 

- $-$  locking variables (thus, the last used variable index stays fixed);

- $-$  adding multiples free variables;

- $-$  unlocking variables.

These methods skip multiple round-trips when multiple free variables have to be added, and their indices used to construct clauses. Variables should be locked only for a short period of time since addVariable calls sent by other SAT memory users will wait until variables are unlocked.

# 5 Usage Example

In this section, we consider the integer factorization problem and its transformation to the SAT problem as a use case. For simplicity, we consider products of exactly two integer factors  $(> 1)$  of the same length *l*. If both factors are primes, our transformation generates a SAT instance having the unique solution.

The input: the length  $l$  and  $2l$  bits of the product.

The output: A SAT instance in the SAT memory.$ 

When the SAT memory is filled, the solve web method of the first available portfolio-based SAT solver (found in the root web memory) is invoked to obtain the SAT assignment. The first  $2l$  Boolean values of the assignment will correspond to the bits of the two factors in question.

## 5.1 The Karatsuba algorithm

In order to asymptotically minimize the number of generated variables and clauses, we apply the Karatsuba multiplication algorithm, which is based on the recursive "divide and conquer" approach [14].

If  $u = (u_{2n-1} ... u_1 u_0)$  and  $v = (v_{2n-1} ... v_1 v_0)$  are 2*n*-bit integers, they can be written as

$$
u = 2^n U_1 + U_0
$$

and  

$$
v = 2^n V_1 + V_0
$$

(the most significant bits are on the left). Then

$$
uv = (2^{2n} + 2^n)U_1V_1 + 2^2(U_1 - U_0)(V_0 - V_1) + (2^n + 1)U_0V_0.
$$
 (1)

The Karatsuba algorithm is applied recursively to each of the three multiplications of *n*-bit numbers from  $(1)$ . Since the algorithm expects the even number of bits at each level, we start with u and v represented as  $2^k$ -bit numbers (lacking leading zeroes are prepended).

10 Sergejs Kozlovičs

## 5.2 Initializing web memory

In web memory, an  $m$ -bit integer will be represented as an ordered list of  $m$ literals: for a positive literal  $x_i$ , the *i*-th bit value will correspond to the  $x_i$ value (0 for false and 1 for true); for a negative literal  $\neg x_i$ , the *i*-th bit will correspond to the negation of  $x_i$ . Thus, one variable can be re-used as a positive or negative literal.

Since we are given the number of bits in each factor ( $l = 2^k$ ) by the reasons explained above), we initialize a new SAT memory instance with  $2l = 2^{k+1}$  free variables. Thus, factors  $u$  and  $v$  are represented as the literal lists  $x_{l-1}, \ldots x_2, x_1$  and  $x_{2l-1}, \ldots x_{l+1}, x_l$ .

## 5.3 Generating the SAT formula

The SAT formula generation process introduces additional integers (additional free variables are added when needed). Those additional integers result from existing integers by applying transformation functions.

Transformation functions are constructed as Boolean expressions that bind literals of the source and target integers. Boolean expressions are built from Boolean primitives And, Or, and Not, as well as from auxiliary Boolean Xor, Majority (of  $3$  elements), Implication, and Equivalence, which can be easily implemented through the Boolean primitives.

Here is the list of transformation functions used as the building blocks.  $L_1, L_2, \ldots$  are lists of literals used to represent the corresponding source or target integers denoted as  $I_1, I_2, \ldots$ 

- $L_1$ .negation() $\rightarrow L_2$ : transforms  $I_1$  to  $I_2 = -I_1$  represented as the two's complement in the  $|L_1|$ -bit notation. Technically, we inverse all bits of  $I_1$  (by negating all literals of  $L_1$ ) and add 1: the *i*-th bit of  $I_1$  is added to the *i*-th carry bit by means of the half adder formula (from digital logic).

- $L_1$ .sum\_with $(L_2) \rightarrow L_3$  (assume  $|L_1| = |L_2|$ ): transforms  $I_1$  and  $I_2$  to  $I_3 =$  $I_1 + I_2$  in the  $|L_1|$ -bit two's complement notation. The overflow bit of  $I_3$  (if any) is ignored. Technically, we use 1 half adder formula and  $L_1 - 1$  adder formulas.

- $L_1.\texttt{product\_with}(L_2) \rightarrow L_3 \text{ (assume } |L_1| = |L_2| = 2^k, |L_3| = 2 \cdot 2^k)$ : transforms two  $2^k$ -bit integers  $I_1$  and  $I_2$  (in the  $2^k$ -bit two's complement notation) to a  $2^{k+1}$ -bit integer  $I_3 = I_1 \cdot I_2$  in the  $2^{k+1}$ -bit two's complement notation according to Equation  $(1)$ . Some technical nuances are:

  - $\bullet$  representing the difference of 2 numbers by means of sum\_with and negation:

  - determining the sign of the inner Karatsuba product  $(U_1 U_0)(V_0 V_1)$  $\bullet$ from Equation  $(1)$ ;

  - propagating the sign bit from the  $2^k$ -bit to the  $2^{k+1}$ -bit two's complement notation.

By applying the transformation functions above we can construct a (non-CNF) SAT formula that binds the initial  $2l$  free variables with the  $2l$  literals 

Shared SAT Solvers and SAT Memory in Distributed Business Applications 11

of the product (the topmost transformation, obviously, will be **product\_with** on two initial lists  $x_{l-1}, \ldots, x_2, x_1$  and  $x_{2l-1}, \ldots, x_{l+1}, x_l$ ). Our implementation of transformation functions introduces additional variables and re-uses literals for recurring formulas.

After constructing the formula, we convert it to CNF using the following patterns:

- $-$  re-phrasing auxiliary Boolean functions (such as Xor and Majority) using And. Or. and Not:

- $-$  applying De Morgan's laws in order to get rid of factored-out negations;

- $-$  introducing new variables and equivalences for inner conjunctions. For example, given the formula  $(x_1 \vee (x_2 \& x_3 \& x_4))$ , we replace  $(x_2 \& x_3 \& x_4)$  with a new variable  $x_5$ , and add the equivalence  $x_5 \equiv x_2 \& x_3 \& x_4$ ;

- introducing new variables for equivalences (from the previous pattern) hav- ing more than two literals on the right side, e.g.,  $x_6$  to represent  $x_3 \& x_4$  (with the corresponding equivalence  $x_6 \iff x_3 \& x_4$ ;

- $-$  constructing CNF clauses from 3-literal equivalences (such as  $x_5 \iff x_2 \& x_6$  and  $x_6 \iff x_3 \& x_4$  above) with the help of the truth table.

Afterward, we extend the CNF by appending a conjunction of literals corresponding to the product bits. The corresponding single-literal clause is added if the product bit is 1, and the literal is negated if the bit is  $0$ .

In order to exclude the case when one the factors is 1, we add two clauses  $(x_{l-1} \vee \ldots x_3 \vee x_2)$  and  $(x_{2l-1} \vee \ldots x_{l+2} \vee x_{l+1})$ , meaning that, for each factor, there must be at least one bit set to 1, not counting the right-most bit  $(x_1$  or  $x_l)$ .

We exclude negative factors by adding two single-literal clauses  $\neg x_{l-1}$  and  $\neg x_{2l-1}$ , meaning that the most-significant bit of each factor is 0 (it must be 0 for non-negative numbers in two's complement notation).

In order to specify the unique solution when the product consists of two primes, we use the condition  $u - v \geq 0$ . The clauses for it can be obtained in a similar manner as for the main SAT formula.

The transformation functions listed above (applied to integers represented as lists of literals) can be easily mapped to any OOP-based programming language. Access to web memory and SAT solvers (by means of web methods of the corresponding web  $I/O$  devices) is also OOP-based. Thus, SAT instances can be generated, placed into web memory, and then passed to a solver in a truly OOP way, without the need to think about the network. Furthermore, our building blocks can serve as a basis for building larger transformation functions.

The current version of the SAT generator (work-in-progress) is available at the IMCS GitHub page https://github.com/LUMII-Syslab/sat-generator.

# 6 Related Work

The invention of advanced heuristics, search space pruning, and intelligent preprocessing of SAT clauses have drastically increased the performance of SAT

12 Sergejs Kozlovičs

solvers intended to run on a single CPU. Such solvers are primarily based on the  $\text{DPLL}^{13}$  variants  $\text{CDCL}^{14}$  and  $\text{VSIDS}^{15}$ . Much research has also been done in the parallelization direction, where the three main approaches are 1) running multiple sequential SAT solvers with different settings, 2) partitioning the search space into (disjoint) subtasks and running multiple solver instances on them, and 3) the portfolio-based approach, where different types of solvers are working in parallel, with the ability to diversify them and exchange learned clauses between them  $[12,4]$ .

In our approach, each SAT solver is viewed as a serial one (even if it internally uses parallelization). However, multiple solvers (of different types or the same) are also possible, and the web kernel (a part of  $webAppOS$ ) allows integrating them to support different types of parallelization.

SAT memory is our innovative solution to provide distributed memory tailored to solving the SAT problem. In our implementation of the SAT memory, we use the centralized approach (as opposed to the distributed one) to deal with the coherence problem [17]. However, our bi-directional web sockets allow web memory clients to exchange clauses, a step towards a hybrid approach. Besides, that also allows us to avoid callbacks. The direct access to SAT memory resembles how shared character devices (from the /dev directory) are accessed in Linux and BSD systems.

Our APIs for solvers and the SAT memory resemble Portfolio Solver Interface used by HordeSAT, as well as MiniSAT's external interface [4]. However, we separate methods related to SAT solving from the ones used to access clauses in web memory. Besides, both our APIs (for solvers and the SAT memory) follow the pure OOP principles [23]. Furthermore, HordeSat uses the same setSolverInterrupt call for both pausing and canceling the search (MiniSAT interface does not have such capability). In contrast, we use distinct methods, since in the former case, the call stack has to be maintained, while, in the latter case, we can free all the resources used by the solver.

The integer factorization problem is one of the approaches used to generate hard SAT instances. Tought $SAT^{16}$  is a collection of SAT generators, which includes a generator for SAT instances for the integer factorization problem. It has been used in SAT Competition 2019 [5]. Sadly, although the generator generates succinct SAT instances, it emits errors on some factors and generates SAT instances with a non-unique solution.

A more generic approach to SAT instance generation is to use SAT compilers. Picat-SAT,  $FznTiny$ , and  $MiniZinc-SAT$  are good representatives [24,13,20]. Although some SAT compilers use binary log encoding for integers, none of the known generic SAT compilers use the Karatsuba algorithm to represent integer products.

<sup>13</sup>  the Davis–Putnam–Logemann–Loyeland algorithm

<sup>14</sup>  Conflict-driven clause-learning

<sup>15</sup> Variable State Independent Decaying Sum

<sup>16</sup> In 2022, it is available at https://github.com/joebebel/toughsat.git; older references point to https://toughsat.appspot.com/, which is no more available.

Shared SAT Solvers and SAT Memory in Distributed Business Applications 13

Our conversion to CNF utilizes the pattern of constructing CNF clauses used in the proof that SAT is reducible to  $3\text{-}\text{CNF-SAT}$  [1].

# 7 Conclusion

We have proposed a software architecture where SAT solvers are shared in the distributed environment by means of the webAppOS web kernel. In our approach, any type of SAT solver can be used as a shared solver; the only requirement is to represent it as SatSolver subclass and implement the API (web methods) from Section 3.1 (usually, glue code is sufficient for that). Our design is suitable for a single shared SAT solver, as well as for multiple SAT solvers launched with different settings or on parallel subtasks.

Since SAT solvers are represented as independent webAppOS  $I/O$  devices, they can be either on-premise (e.g., installed on specific hardware) or cloudbased. That opens opportunities to commercial SAT solvers available as a service.

In our architecture, a novel component is shared SAT memory, which can be accessed either as a webAppOS virtual  $I/O$  device or directly via web sockets. While we have considered SAT memory for storing CNF clauses, the architecture can be generalized to support clauses in the disjunctive normal form (DNF) and algebraic normal form (ANF). Having multiple types of SAT memory, our architecture allows the developers to create web methods that use multiple SAT memories simultaneously (e.g., for data conversion).

We also envisage further extensions of SAT memory. For example, belief propagation and survey propagation-based SAT solvers need to store real weights (called "warnings") associated with SAT variables and clauses [8].

**Acknowledgements** Research supported by the Latvian Council of Science, Project No.  $2021/1-0479$  "Combinatorial Optimization with Deep Neural Networks".

This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this contribution is published in CCIS vol. 1598, "Digital Business and Intelligent Systems: 15th International Baltic Conference, Baltic DB&IS 2022, Riga, Latvia, July  $4-6$ , 2022, Proceedings", and is available online at https://doi.org/10.1007/978-3-031-09850-5\_14.

# References

- 1. Aho, A.V., Hopcroft, J.E., Ullman, J.D.: The design and analysis of computer algorithms. Addison-Wesley series in computer science and information processing, Addison-Wesley Pub. Co, Reading, Mass (1974)
- 2. Andrews, M., Whittaker, J.A.: How to break web software: Functional and security testing of web applications and web services. Addison-Wesley Professional (2006)
- 3. Audemard, G., Simon, L.: On the Glucose SAT Solver. International Journal on Artificial Intelligence Tools  $27(01)$ , 1840001 (Feb 2018)

$14$ Sergejs Kozlovičs

- 4. Balyo, T., Sanders, P., Sinz, C.: HordeSat: A massively parallel portfolio SAT solver (May 2015). https://doi.org/10.1007/978-3-319-24318-4\_12
- 5. Bebel, J.: Harder SAT Instances from Factoring with Karatsuba and Espresso. In: Proceedings of SAT Race 2019. University of Helsinki (2019)
- 6. Biere, A., Heule, M., Maaren, H.v. (eds.): Handbook of satisfiability. No. volume 336 in Frontiers in Artificial Intelligence and Applications, IOS Press, Amsterdam ; Washington, DC, second edition edn. (2021), oCLC: on1250382566
- 7. Boulanger, J.L. (ed.): Formal methods applied to complex systems: implementation of the B Method. Computer engineering series, ISTE; Wiley, London: Hoboken, New Jersey  $(2014)$
- 8. Braunstein, A., Mezard, M., Zecchina, R.: Survey propagation: An algorithm for satisfiability. Random Struct. Algorithms 27, 201–226 (Sep 2005)
- 9. Eggersglüss, S., Eggersglüß, S., Drechsler, R.: High quality test pattern generation and boolean satisfiability. Springer, New York  $(2012)$
- 10. Eén, N., Sörensson, N.: An extensible SAT-solver. In: Giunchiglia, E., Tacchella, A. (eds.) Theory and applications of satisfiability testing. pp. 502–518. Springer Berlin Heidelberg, Berlin, Heidelberg (2004)
- 11. Folino, G., Pizzuti, C., Spezzano, G.: Parallel hybrid method for SAT that couples genetic algorithms and local search. IEEE Transactions on Evolutionary Computation  $5(4)$ , 323–334 (Aug 2001)
- 12. Hamadi, Y., Jabbour, S., Sais, L.: ManySAT: a parallel SAT solver. JSAT 6, 245–  $262 \text{ (Jun 2009)}$
- 13. Huang, J.: Universal booleanization of constraint models. In: Proceedings of the 14th international conference on principles and practice of constraint programming. pp. 144-158. CP '08, Springer-Verlag, Berlin, Heidelberg (2008)
- 14. Knuth, D.E.: The art of computer programming, Volume 2: Seminumerical Algorithms, vol. 2. Addison-Wesley, Reading, Mass, 3rd ed edn. (1997)
- 15. Kozlovičs, S.: The Web Computer and Its Operating System: A New Approach for Creating Web Applications. In: Proceedings of the 15th International Conference on Web Information Systems and Technologies  $(2019)$
- 16. Kozlovičs, S.: webAppOS: Creating the illusion of a single computer for web application developers. In: Bozzon, A., Domínguez Mayo, F.J., Filipe, J. (eds.) WEBIST 2019: Web information systems and technologies. pp.  $1-21$ . Springer International Publishing  $(2020)$
- 17. Li, K., Hudak, P.: Memory coherence in shared virtual memory systems. ACM Transactions on Computer Systems  $7(4)$ ,  $321-359$  (Nov 1989)
- 18. Malik, S., Zhang, L.: Boolean satisfiability: from theoretical hardness to practical success. Communications of the ACM  $52(8)$ , 76–82 (Aug 2009)
- 19. Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., Malik, S.: Chaff: engineering an efficient SAT solver. In: Proceedings of the 38th design automation conference (IEEE cat. No.01CH37232). pp.  $530-535$  (2001)
- 20. Nethercote, N., Stuckey, P.J., Becket, R., Brand, S., Duck, G.J., Tack, G.: MiniZinc: Towards a standard CP modelling language. In: Bessiere, C. (ed.) Proceedings of the 13th international conference on the principles and practice of constraint programming. pp.  $529 - 543$ . Springer-Verlag London Ltd., Germany (2007)
- 21. Ozolins, E., Freivalds, K., Draguns, A., Gaile, E., Zakovskis, R., Ko- $S.:$  Goal-Aware Neural SAT Solver. In: 2022 International  $z$ Joint Conference on Neural Networks (IJCNN). pp. 1-8. IEEE, Padua, https://doi.org/10.1109/IJCNN55064.2022.9892733, Italy  $(Jul)$ 2022). https://ieeexplore.ieee.org/document/9892733/

Shared SAT Solvers and SAT Memory in Distributed Business Applications 15

- 22. Rintanen, J.: Planning as satisfiability: Heuristics. Artificial Intelligence  $\mathbf{193}, 45\text{--}86$  $(Dec 2012)$
- 23. West, D.: Object thinking. Microsoft professional, Microsoft, Redmond, WA (2004)
- 24. Zhou, n.f., Kjellerstrand, H.: The Picat-SAT compiler. vol. 9585, pp. 48–62 (Jan  $2016)$