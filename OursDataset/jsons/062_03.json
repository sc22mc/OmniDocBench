{
    "layout_dets": [
        {
            "category_type": "header",
            "poly": [
                205,
                100.00000000000001,
                1072.5,
                100.00000000000001,
                1072.5,
                137.5,
                205,
                137.5
            ],
            "ignore": false,
            "order": 1,
            "anno_id": 1,
            "text": "JOURNAL OF LATEX CLASS FILES,vol. 14, NO.8, AUGUST 2022",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        205,
                        100.00000000000001,
                        1072.5,
                        100.00000000000001,
                        1072.5,
                        137.5,
                        205,
                        137.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "header",
            "poly": [
                2312.5,
                97.5,
                2355,
                97.5,
                2355,
                135,
                2312.5,
                135
            ],
            "ignore": false,
            "order": 2,
            "anno_id": 1,
            "text": "3",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        2312.5,
                        97.5,
                        2355,
                        97.5,
                        2355,
                        135,
                        2312.5,
                        135
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                197.5,
                235,
                1252.5,
                235,
                1252.5,
                470,
                197.5,
                470
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": " negative obstacle (points lower than the road plane). Moreover, since water bodies cannot be detected by LiDAR due to refraction and reflection, the authors propose a technique to detect potholes filled with water by scanning the image for large areas of missing data.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        197.5,
                        235,
                        1252.5,
                        235,
                        1252.5,
                        470,
                        197.5,
                        470
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                202.5,
                490,
                1257.5,
                490,
                1257.5,
                970,
                202.5,
                970
            ],
            "ignore": false,
            "order": 4,
            "anno_id": 1,
            "text": " Shuo et al. [29] improves the aforementioned method by projecting the points on the camera plane and interpolating the depth values of the projected points to receive a depth image. They use both horizontal and vertical histograms to coarsely detect the road area and refine it respectively. Although they state their method as sensor fusion between the monocular camera and LiDAR, they do not utilize the color values of the camera images. Both works [15] and [29] use the KITTI dataset as a benchmark and achieve great results, comparable to machine learning methods.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        202.5,
                        490,
                        1257.5,
                        490,
                        1257.5,
                        970,
                        202.5,
                        970
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                202.5,
                982.5,
                1257.5,
                982.5,
                1257.5,
                1262.5,
                202.5,
                1262.5
            ],
            "ignore": false,
            "order": 5,
            "anno_id": 1,
            "text": " Other techniques for pothole detection may include laser scanning, ground penetrating radar, ultrasonic sensor, as well as multi-sensor fusion, especially concerning fusion with imaging information. An extensive review of such techniques falls beyond the scope of this article. However, an interested reader may be referred to the survey in  $[31]$ .",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        202.5,
                        982.5,
                        1257.5,
                        982.5,
                        1257.5,
                        1262.5,
                        202.5,
                        1262.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                200,
                1277.5,
                1252.5,
                1277.5,
                1252.5,
                2272.5,
                200,
                2272.5
            ],
            "ignore": false,
            "order": 6,
            "anno_id": 1,
            "text": " 2) Point cloud saliency: One of main challenges in techniques utilizing point clouds is the inherent noise and the increased computational cost due to the unordered data structure of point clouds. To address such challenges, saliency map extraction has been proposed as a powerful step in point cloud processing to reduce noise and data dimensionality, leading to more robust solutions and computational efficiency [32] [33]. Yet, the use of local saliency in pothole detection has not been sufficiently examined. Saliency maps were constructed from point clouds obtained from Mobile Laser Scanning (MLS) in [34] for road crack detection. MLS point clouds contain spatial information (i.e., Euclidean coordinates) and intensity information, and thus the extracted features could leverage both height and intensity information. Feature saliency was estimated by calculating the distances from the normal of each point to the principal normal of the input point clouds. In a similar setting, Wang *et al.* [35] extracted saliency maps in MLS point clouds by projecting the distance of each point's normal vector to the point cloud's dominant normal vector into a hyperbolic tangent function space.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        200,
                        1277.5,
                        1252.5,
                        1277.5,
                        1252.5,
                        2272.5,
                        200,
                        2272.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                197.5,
                2275,
                1255,
                2275,
                1255,
                2972.5,
                197.5,
                2972.5
            ],
            "ignore": false,
            "order": 7,
            "anno_id": 1,
            "text": " 3) Cooperative driving: While significant advances have been made for single-agent perception, many applications require multiple sensing agents and cross-agent communication for more accurate results. Objects, captured by the single-agent's sensor devices, may be heavily occluded or far away from the sensors' view, resulting in sparse observations. Nevertheless, failing to detect and predict the accurate position or moving intention of these occluded or \"hard-to-see\" objects might have harmful consequences in safety-critical situations, and especially if the reaction time is very narrow [36]. The development of multi-agent solutions can lead to collaborative perception and, through information sharing, may improve the driving performance and experiences, providing endless possibilities for safe driving.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        197.5,
                        2275,
                        1255,
                        2275,
                        1255,
                        2972.5,
                        197.5,
                        2972.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                200,
                2980,
                1257.5,
                2980,
                1257.5,
                3117.5,
                200,
                3117.5
            ],
            "ignore": false,
            "order": 8,
            "anno_id": 1,
            "text": " Recently, cooperative autonomous driving has been considered as a possible solution to improve the performance and safety of autonomous vehicles [37]. Cooperative perception for ",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        200,
                        2980,
                        1257.5,
                        2980,
                        1257.5,
                        3117.5,
                        200,
                        3117.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1290,
                2722.5,
                2355,
                2722.5,
                2355,
                3115,
                1290,
                3115
            ],
            "ignore": false,
            "order": 12,
            "anno_id": 1,
            "text": " 4) Situational awareness and AR infotainment: In the case of semi-autonomous vehicles, where the operator/driver may be asked to take manual control of the car at any moment, it is of great importance [46] to implement notification paradigms that direct the operator's, possibly reduced, attention to the event that triggered the take-over request [47], [48]. Recently, the automotive industry started to invest funds and efforts into AR technology and its integration with In-Vehicle Information",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1290,
                        2722.5,
                        2355,
                        2722.5,
                        2355,
                        3115,
                        1290,
                        3115
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1292.5,
                1730,
                2352.5,
                1730,
                2352.5,
                2715,
                1292.5,
                2715
            ],
            "ignore": false,
            "order": 11,
            "anno_id": 1,
            "text": " Chen et al. [41] proposed a point cloud feature-based cooperative perception framework for connected autonomous vehicles to increase object detection precision. The features are selected to be rich enough for the training process, and at the same time have an intrinsically small size to achieve realtime edge computing. Guo *et al.* [42] proposed a cooperative fusion method to combine spatial feature maps for achieving a higher 3D object detection performance. Yuan *et al.* [43] proposed a 3D keypoints feature fusion scheme for cooperative driving detection to remedy the problem of low bounding box localization accuracy. Fang et al. [44] presented an iterated split covariance intersection filter-based cooperative localization strategy with a decentralized framework. In addition, they adopted a point cloud registration method to obtain the relative pose estimation using mutually shared information from neighbour vehicles. Kim and Liu [45] presented the concept of cooperative autonomous driving using mirror neuron-inspired intention awareness and cooperative perception, providing information on the upcoming traffic situations ahead, even beyond line-of-sight and field-of-view.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1292.5,
                        1730,
                        2352.5,
                        1730,
                        2352.5,
                        2715,
                        1292.5,
                        2715
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1292.5,
                880,
                2355,
                880,
                2355,
                1722.5,
                1292.5,
                1722.5
            ],
            "ignore": false,
            "order": 10,
            "anno_id": 1,
            "text": "  $Xu \\text{ et al. } [38]$  presented the first open dataset and used it to benchmark fusion strategies for V2V (vehicle-to-vehicle) perception. They also plan to extend the dataset with more tasks as well as sensor suites and investigate more multimodal sensor fusion methods in the V2V and V2I (vehicle-toinfrastructure) settings. Arnold *et al.* [37] proposed a system that produces a perception of complex road segments (e.g., complex T-junctions and roundabouts) using a network of roadside infrastructure sensors with fixed positions. Chen et al. [39] studied the raw-data level cooperative perception for enhancing the detection ability of self-driving systems. They fuse the sensor data collected from different positions and angles of connected vehicles, relying on LiDAR 3D point clouds. Liu et al. [40] addressed the collaborative perception problem, where one agent is required to perform a perception task and can communicate and share information with other agents on the same task.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1292.5,
                        880,
                        2355,
                        880,
                        2355,
                        1722.5,
                        1292.5,
                        1722.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1292.5,
                232.5,
                2350,
                232.5,
                2350,
                872.5,
                1292.5,
                872.5
            ],
            "ignore": false,
            "order": 9,
            "anno_id": 1,
            "text": " 3D object detection can be performed via early or late fusion of information, i.e., combination of multiple sensing points of view or fusion of object detection results, respectively.Both fusion approaches can extend the perception of the sensing system, however, only the early fusion approach can actually exploit complementary information. A major challenge that arises regarding cooperative perception is how to effectively merge sensors' data received from different vehicles to obtain a precise and comprehensive perception outcome. Additionally, despite the attention that cooperative driving has attracted recently, the absence of a suitable open dataset for benchmarking algorithms has made it difficult to develop and assess cooperative perception technologies.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1292.5,
                        232.5,
                        2350,
                        232.5,
                        2350,
                        872.5,
                        1292.5,
                        872.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        }
    ],
    "extra": {
        "relation": []
    },
    "page_info": {
        "page_attribute": {},
        "page_no": 629,
        "height": 3300,
        "width": 2550,
        "image_path": "62_3_png.jpg"
    }
}