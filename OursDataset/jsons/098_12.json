{
    "layout_dets": [
        {
            "category_type": "header",
            "poly": [
                450,
                114,
                1228,
                114,
                1228,
                152,
                450,
                152
            ],
            "ignore": false,
            "order": 1,
            "anno_id": 1,
            "text": "Published as a conference paper at ICLR 2024",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        450,
                        114,
                        1228,
                        114,
                        1228,
                        152,
                        450,
                        152
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                441.9999999999998,
                346,
                2112,
                346,
                2112,
                2096,
                441.9999999999998,
                2096
            ],
            "ignore": false,
            "order": 2,
            "anno_id": 1,
            "text": "- Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. - Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. *arXiv* preprint *arXiv*:2212.10560, 2022. - Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing the emergent cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration. *arXiv* preprint *arXiv*:2307.05300, 2023. - Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. *arXiv* preprint arXiv:2109.01652, 2021. - Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. *Advances in* neural information processing systems, 35:24824–24837, 2022. - David Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models. arXiv preprint arXiv:2210.03162, 2022. - Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. - Yadong Zhang, Shaoguang Mao, Tao Ge, Xun Wang, Yan Xia, Man Lan, and Furu Wei. K-level reasoning with large language models. *arXiv* preprint arXiv:2402.01521, 2024. - Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning. arXiv preprint arXiv:2402.04833, 2024. - Lin Zheng, Chong Wang, and Lingpeng Kong. Linear complexity randomized self-attention mechanism. In *International Conference on Machine Learning*, 2022.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        441.9999999999998,
                        346,
                        2112,
                        346,
                        2112,
                        2096,
                        441.9999999999998,
                        2096
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                440.0000000000001,
                2298,
                2098,
                2298,
                2098,
                2386,
                440.0000000000001,
                2386
            ],
            "ignore": false,
            "order": 4,
            "anno_id": 1,
            "text": "We show how to perform pretraining with the text continuation objective and instruction fine-tuning in Figure 7 and 8.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        440.0000000000001,
                        2298,
                        2098,
                        2298,
                        2098,
                        2386,
                        440.0000000000001,
                        2386
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                444,
                2412,
                2104,
                2412,
                2104,
                2506,
                444,
                2506
            ],
            "ignore": false,
            "order": 5,
            "anno_id": 1,
            "text": "We train the ICAE on 8 Nvidia A100 GPUs (80GB). The hyperparameters for pretraining and fine-tuning ICAE are presented in Table 8. We by default train the ICAE with bf16.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        444,
                        2412,
                        2104,
                        2412,
                        2104,
                        2506,
                        444,
                        2506
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                445.9999999999999,
                2194,
                1292,
                2194,
                1292,
                2240,
                445.9999999999999,
                2240
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": "## A MODEL TRAINING CONFIGURATION",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        445.9999999999999,
                        2194,
                        1292,
                        2194,
                        1292,
                        2240,
                        445.9999999999999,
                        2240
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "table_caption",
            "poly": [
                962,
                2556,
                1590,
                2556,
                1590,
                2598,
                962,
                2598
            ],
            "ignore": false,
            "order": 6,
            "anno_id": 1,
            "text": " Table 8: Hyperparameters for training</table>",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        962,
                        2556,
                        1590,
                        2556,
                        1590,
                        2598,
                        962,
                        2598
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "table",
            "poly": [
                796,
                2646,
                1752,
                2646,
                1752,
                3014,
                796,
                3014
            ],
            "ignore": false,
            "order": 7,
            "anno_id": 1,
            "text": "",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        796,
                        2646,
                        1752,
                        2646,
                        1752,
                        3014,
                        796,
                        3014
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": "",
                "table_layout": "",
                "language": ""
            },
            "html": "<table>\n<thead>\n<tr>\n<th>Hyperparameter</th>\n<th>Value</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Optimizer</td>\n<td>AdamW</td>\n</tr>\n<tr>\n<td>learning rate</td>\n<td>1e-4 (pretrain); 5e-5 (fine-tuning)</td>\n</tr>\n<tr>\n<td>batch size</td>\n<td>256</td>\n</tr>\n<tr>\n<td>warmup</td>\n<td>300</td>\n</tr>\n<tr>\n<td>#updates</td>\n<td>200k (pretrain); 30k (fine-tuning)</td>\n</tr>\n<tr>\n<td>clip norm</td>\n<td>2.0</td>\n</tr>\n</tbody>\n</table>",
            "latex": ""
        },
        {
            "category_type": "footer",
            "poly": [
                1250,
                3130,
                1294,
                3130,
                1294,
                3168,
                1250,
                3168
            ],
            "ignore": false,
            "order": 8,
            "anno_id": 1,
            "text": "12",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1250,
                        3130,
                        1294,
                        3130,
                        1294,
                        3168,
                        1250,
                        3168
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        }
    ],
    "extra": {
        "relation": []
    },
    "page_info": {
        "page_attribute": {},
        "page_no": 1015,
        "height": 3300,
        "width": 2550,
        "image_path": "98_12_png.jpg"
    }
}