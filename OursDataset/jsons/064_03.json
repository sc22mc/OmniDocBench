{
    "layout_dets": [
        {
            "category_type": "header",
            "poly": [
                517.5,
                187.5,
                1970,
                187.5,
                1970,
                240,
                517.5,
                240
            ],
            "ignore": false,
            "order": 1,
            "anno_id": 1,
            "text": "QR-CLIP:Introducing Explicit Open-World Knowledge for Location and Time Reasoning",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        517.5,
                        187.5,
                        1970,
                        187.5,
                        1970,
                        240,
                        517.5,
                        240
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "figure",
            "poly": [
                222.5000000000001,
                272.5,
                2252.5,
                272.5,
                2252.5,
                1042.5,
                222.5000000000001,
                1042.5
            ],
            "ignore": false,
            "order": 2,
            "anno_id": 1,
            "text": "",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        222.5000000000001,
                        272.5,
                        2252.5,
                        272.5,
                        2252.5,
                        1042.5,
                        222.5000000000001,
                        1042.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                225,
                1375,
                835,
                1375,
                835,
                1425,
                225,
                1425
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": " ## 2.2. Location and Time Reasoning",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        225,
                        1375,
                        835,
                        1375,
                        835,
                        1425,
                        225,
                        1425
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                227.5,
                1455,
                1210,
                1455,
                1210,
                2405,
                227.5,
                2405
            ],
            "ignore": false,
            "order": 4,
            "anno_id": 1,
            "text": "e models achieved significant success on a wide range of tasks that require an understanding of language (Kenton & Toutanova, 2019; Lewis et al., 2020). Also, the vision models (He et al., 2016; Dosovitskiy et al., 2020; Liu et al., 2021) can predict the correct class label of an image from thousands of options. But they still struggle with many reasoning tasks, such as discerning the abstractive meanings (e.g., time, location, event) of images (Yang et al., 2020; Tahmasebzadeh et al., 2021) or performing mathematical calculations (Lewkowycz et al., 2022) and science deduction (Degrave et al., 2022). However, humans can do these things well because real brains are more powerful than artificial neural networks in many ways and actively learn to conduct abstract reasoning (Schmidhuber, 2015). We focus on location and time reasoning (Fu et al.,  $2022$ ), which needs a model to think and reason beyond the actual content of an image. Compared to other reasoning tasks, it differs in that most of the time there aren't enough visual cues to make inferences, so auxiliary knowledge is required.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        227.5,
                        1455,
                        1210,
                        1455,
                        1210,
                        2405,
                        227.5,
                        2405
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                225,
                2467.5,
                490,
                2467.5,
                490,
                2515,
                225,
                2515
            ],
            "ignore": false,
            "order": 5,
            "anno_id": 1,
            "text": " # 3. Approach",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        225,
                        2467.5,
                        490,
                        2467.5,
                        490,
                        2515,
                        225,
                        2515
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                227.5,
                2550,
                520,
                2550,
                520,
                2597.5,
                227.5,
                2597.5
            ],
            "ignore": false,
            "order": 6,
            "anno_id": 1,
            "text": " ## 3.1. Preliminary",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        227.5,
                        2550,
                        520,
                        2550,
                        520,
                        2597.5,
                        227.5,
                        2597.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                222.5,
                2632.5,
                1210,
                2632.5,
                1210,
                2925,
                222.5,
                2925
            ],
            "ignore": false,
            "order": 7,
            "anno_id": 1,
            "text": " Task Background. Current AI methods are comparably weak in deducing the abstract information hidden behind an image. The goal of this paper is to let the model reason the location and time based on image input (Fu et al.,  $2022$ ): Given an image I, we need the model  $(M(I))$  to predict the location ( $Pred_l$ ) and time ( $Pred_t$ ).",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        222.5,
                        2632.5,
                        1210,
                        2632.5,
                        1210,
                        2925,
                        222.5,
                        2925
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                1277.4999999999998,
                2712.5,
                1667.4999999999998,
                2712.5,
                1667.4999999999998,
                2757.5,
                1277.4999999999998,
                2757.5
            ],
            "ignore": false,
            "order": 12,
            "anno_id": 1,
            "text": " ## 3.2. Quantity Module",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1277.4999999999998,
                        2712.5,
                        1667.4999999999998,
                        2712.5,
                        1667.4999999999998,
                        2757.5,
                        1277.4999999999998,
                        2757.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1282.5,
                2790,
                2272.5,
                2790,
                2272.5,
                2992.5,
                1282.5,
                2992.5
            ],
            "ignore": false,
            "order": 13,
            "anno_id": 1,
            "text": " Additional [CLS]. We employ CLIP (Radford et al., 2021) as our basic architecture. In vanilla CLIP, [CLS] is used to distinguish the input token that represents the whole input features which is a common practice in other trans-",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1282.5,
                        2790,
                        2272.5,
                        2790,
                        2272.5,
                        2992.5,
                        1282.5,
                        2992.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1280,
                1890,
                2262.5,
                1890,
                2262.5,
                2660,
                1280,
                2660
            ],
            "ignore": false,
            "order": 11,
            "anno_id": 1,
            "text": " Therefore, we should simultaneously consider the quantity and relevance of information to achieve a higher correlation.  Our Pipeline. Following the human reasoning process, this paper proposes QR-CLIP. It is based on the contrastive language-image pretraining (CLIP) model (Radford et al., 2021), which was pre-trained with 400M internet data. The QR-CLIP is made up of the quantity module (Sec  $3.2$ ) and the relevance module (Sec  $3.3$ ). The quantity module aims to add diversity to the model's outputs. Here, we introduce the additional [CLS] method, which aims to generate multiple distinct  $[CLS]_i$  to imitate different perspectives of a single image. The relevance module uses a scoring mechanism to weigh the retrieved open-world knowledge and image features to ensure the most relevant information and combine them for final prediction.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1280,
                        1890,
                        2262.5,
                        1890,
                        2262.5,
                        2660,
                        1280,
                        2660
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "isolate_formula",
            "poly": [
                1520,
                1802.5,
                2007.5,
                1802.5,
                2007.5,
                1860,
                1520,
                1860
            ],
            "ignore": false,
            "order": 9,
            "anno_id": 1,
            "text": " $$ Correlation  \\uparrow \\leftrightarrow Q \\uparrow \\times R \\uparrow .  $$",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1520,
                        1802.5,
                        2007.5,
                        1802.5,
                        2007.5,
                        1860,
                        1520,
                        1860
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "formula_caption",
            "poly": [
                2205,
                1804.9999999999998,
                2247.5,
                1804.9999999999998,
                2247.5,
                1844.9999999999998,
                2205,
                1844.9999999999998
            ],
            "ignore": false,
            "order": 10,
            "anno_id": 1,
            "text": "(1)",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        2205,
                        1804.9999999999998,
                        2247.5,
                        1804.9999999999998,
                        2247.5,
                        1844.9999999999998,
                        2205,
                        1844.9999999999998
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1280,
                1377.5,
                2262.5,
                1377.5,
                2262.5,
                1767.5,
                1280,
                1767.5
            ],
            "ignore": false,
            "order": 8,
            "anno_id": 1,
            "text": " Horn's QR Theory. In cognitive research (Allott, 2013), it is believed that human reasoning is the process of obtaining the best correlations. The OR theory (Horn, 1984) builds a more clear definition of the above correlations, where  $Q$ stands for the quantity principle, which requires the maximization of valuable content, and R means the relevance principle that requires the minimization of form and extracts most related information:",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1280,
                        1377.5,
                        2262.5,
                        1377.5,
                        2262.5,
                        1767.5,
                        1280,
                        1767.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "figure_caption",
            "poly": [
                222.5,
                1062.5,
                2270,
                1062.5,
                2270,
                1282.5,
                222.5,
                1282.5
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": " Figure 2. The pipeline of QR-CLIP. It has two modules: Quantity module (Sec 3.2) and Relevance module (Sec 3.3). In step 1, we add additional [CLS] to mimic different perspectives of different individuals and design the local and global loss to guide the location/time fine-tuning. Then, we freeze the fine-tuned CLIP-V and CLIP-T and use them to search for open-world knowledge from our OWK dataset (Sec 4.1). In the Relevance module, we use a scoring mechanism to weights the most valuable information from CLIP-T and CLIP-V. After multiplying the scoring weights for vision and language separately, we add them for final similarity calculation.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        222.5,
                        1062.5,
                        2270,
                        1062.5,
                        2270,
                        1282.5,
                        222.5,
                        1282.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        }
    ],
    "extra": {
        "relation": []
    },
    "page_info": {
        "page_attribute": {},
        "page_no": 658,
        "height": 3300,
        "width": 2550,
        "image_path": "64_3_png.jpg"
    }
}