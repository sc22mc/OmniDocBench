{
    "layout_dets": [
        {
            "category_type": "header",
            "poly": [
                452.5,
                110,
                1225,
                110,
                1225,
                152.5,
                452.5,
                152.5
            ],
            "ignore": false,
            "order": 1,
            "anno_id": 1,
            "text": " Published as a conference paper at ICLR 2024",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        452.5,
                        110,
                        1225,
                        110,
                        1225,
                        152.5,
                        452.5,
                        152.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                442.5,
                349.9949999999999,
                2112.5,
                349.9949999999999,
                2112.5,
                3050.125,
                442.5,
                3050.125
            ],
            "ignore": false,
            "order": 2,
            "anno_id": 1,
            "text": "- Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. *arXiv* preprint arXiv:2303.17651, 2023.  - OpenAI. Gpt-4 technical report. *arXiv* preprint *arXiv*:2303.08774, 2023.  - Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are nlp models really able to solve simple math word problems? *arXiv* preprint *arXiv*:2103.07191, 2021.  - Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904, 2023.  - Silviu Pitis, Michael R Zhang, Andrew Wang, and Jimmy Ba. Boosted prompt ensembles for large language models. *arXiv* preprint *arXiv*:2304.05970, 2023.  - Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with \"gradient descent\" and beam search. *arXiv* preprint *arXiv*:2305.03495, 2023.  - Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. In Proc. Conference on Empirical Methods in Natural Language Processing, pp. 4222–4235, 2020.  - Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv* preprint *arXiv*:2307.09288, 2023.  - Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In Proc. International Conference on Learning Representations, 2022.  - Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. *Advances in Neural Information Processing Systems*, 35:24824–24837, 2022.  - Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. Large language models are better reasoners with self-verification. In *Proc. Conference on Empirical Methods in Natural Language Processing*, pp. 2550–2575, 2023.  - Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. In *Proc. Advances in Neural Information Processing Systems*, volume 36, 2024.  - Chenrui Zhang, Lin Liu, Jinpeng Wang, Chuyuan Wang, Xiao Sun, Hongyu Wang, and Mingchen Cai. Prefer: Prompt ensemble learning via feedback-reflect-refine. *arXiv* preprint *arXiv*:2308.12033, 2023a.  - Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. Cumulative reasoning with large language models. *arXiv* preprint *arXiv*:2308.04371, 2023b.  - Zhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. In *Proc. International Conference on Learning Representations*, 2022.  - Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, and Qizhe Xie. Automatic model selection with large language models for reasoning. *arXiv* preprint *arXiv*:2305.14333, 2023.  - Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. *arXiv* preprint *arXiv*:2304.09797, 2023.  - Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. arXiv preprint arXiv:2308.07921, 2023a.  - Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning in large language models. In *Proc. International Conference on Learning Representations*, 2023b.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        442.5,
                        349.9949999999999,
                        2112.5,
                        349.9949999999999,
                        2112.5,
                        3050.125,
                        442.5,
                        3050.125
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "footer",
            "poly": [
                1247.5,
                3122.5,
                1295,
                3122.5,
                1295,
                3175,
                1247.5,
                3175
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": "11",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1247.5,
                        3122.5,
                        1295,
                        3122.5,
                        1295,
                        3175,
                        1247.5,
                        3175
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        }
    ],
    "extra": {
        "relation": []
    },
    "page_info": {
        "page_attribute": {},
        "page_no": 2,
        "height": 3300,
        "width": 2550,
        "image_path": "100_11_png.jpg"
    }
}