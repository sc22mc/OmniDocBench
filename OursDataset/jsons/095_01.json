{
    "layout_dets": [
        {
            "category_type": "header",
            "poly": [
                624,
                10,
                2064,
                10,
                2064,
                144,
                624,
                144
            ],
            "ignore": false,
            "order": 1,
            "anno_id": 1,
            "text": "This CVPR paper is the Open Access version, provided by the Computer Vision Foundation.  Except for this watermark, it is identical to the accepted version;  the final published version of the proceedings is available on IEEE Xplore.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        624,
                        10,
                        2064,
                        10,
                        2064,
                        144,
                        624,
                        144
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "title",
            "poly": [
                380,
                428,
                2168,
                428,
                2168,
                566,
                380,
                566
            ],
            "ignore": false,
            "order": 2,
            "anno_id": 1,
            "text": "# OmniDocBench: Benchmarking Diverse PDF Document Parsing with Comprehensive Annotations",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        380,
                        428,
                        2168,
                        428,
                        2168,
                        566,
                        380,
                        566
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                272,
                610,
                2312,
                610,
                2312,
                864,
                272,
                864
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": "Linke Ouyang<sup>1\\*</sup> Yuan Qu<sup>1\\*</sup> Hongbin Zhou<sup>1\\*</sup> Jiawei Zhu<sup>1\\*</sup> Rui Zhang<sup>1\\*</sup> Qunshu Lin<sup>2\\*</sup> Bin Wang<sup>1\\*†</sup> Zhiyuan Zhao<sup>1</sup> Man Jiang<sup>1</sup> Xiaomeng Zhao<sup>1</sup> Jin Shi<sup>1</sup> Fan Wu<sup>1</sup> Pei Chu<sup>1</sup> Minghao Liu<sup>3</sup> Zhenxiang Li<sup>1</sup> Chao Xu<sup>1</sup> Bo Zhang<sup>1</sup> Botian Shi<sup>1</sup> Zhongying Tu<sup>1</sup> Conghui He<sup>1‡</sup>  <sup>1</sup>Shanghai AI Laboratory <sup>2</sup>Abaka AI <sup>3</sup>2077AI",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        272,
                        610,
                        2312,
                        610,
                        2312,
                        864,
                        272,
                        864
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                242.00000000000017,
                1100,
                1224,
                1100,
                1224,
                2228,
                242.00000000000017,
                2228
            ],
            "ignore": false,
            "order": 5,
            "anno_id": 1,
            "text": "Document content extraction is a critical task in computer vision, underpinning the data needs of large language models (LLMs) and retrieval-augmented generation (RAG) systems. Despite recent progress, current document parsing methods have not been fairly and comprehensively evaluated due to the narrow coverage of document types and the simplified, unrealistic evaluation procedures in existing benchmarks. To address these gaps, we introduce OmniDocBench, a novel benchmark featuring high-quality annotations across nine document sources, including academic papers, textbooks, and more challenging cases such as handwritten notes and densely typeset newspapers. OmniDocBench supports flexible, multi-level evaluations—ranging from an end-to-end assessment to the task-specific and attribute-based analysis—using 19 layout categories and 15 attribute labels. We conduct a thorough evaluation of both pipeline-based methods and endto-end vision-language models, revealing their strengths and weaknesses across different document types. OmniDocBench sets a new standard for the fair, diverse, and fine-grained evaluation in document parsing. Dataset and code are available at https://github.com/ opendatalab/OmniDocBench.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        242.00000000000017,
                        1100,
                        1224,
                        1100,
                        1224,
                        2228,
                        242.00000000000017,
                        2228
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                239.99999999999994,
                2424,
                1234,
                2424,
                1234,
                2810,
                239.99999999999994,
                2810
            ],
            "ignore": false,
            "order": 7,
            "anno_id": 1,
            "text": "As large language models  $[1, 28, 39, 44]$  increasingly rely on high-quality, knowledge-rich data, the importance of accurate document parsing has grown substantially. Document parsing, a core task in computer vision and document intelligence, aims to extract structured, machine-readable content from unstructured documents such as PDFs. This task is particularly critical for ingesting academic papers, technical reports, textbooks, and other rich textual sources",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        239.99999999999994,
                        2424,
                        1234,
                        2424,
                        1234,
                        2810,
                        239.99999999999994,
                        2810
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1312.0000000000002,
                2072,
                2306.0000000000005,
                2072,
                2306.0000000000005,
                2358,
                1312.0000000000002,
                2358
            ],
            "ignore": false,
            "order": 10,
            "anno_id": 1,
            "text": "into large language models, thereby enhancing their factual accuracy and knowledge grounding [19, 42, 45, 47, 52]. Moreover, with the emergence of retrieval-augmented generation (RAG) systems [12, 22], which retrieve and generate answers conditionally with external documents, the demand for precise document understanding has further intensified.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1312.0000000000002,
                        2072,
                        2306.0000000000005,
                        2072,
                        2306.0000000000005,
                        2358,
                        1312.0000000000002,
                        2358
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1314,
                2372,
                2308,
                2372,
                2308,
                2860,
                1314,
                2860
            ],
            "ignore": false,
            "order": 11,
            "anno_id": 1,
            "text": "To address this challenging task, two main paradigms have emerged: 1) Pipeline-based approaches that decompose the task into layout analysis, OCR, formula/table recognition, and reading order estimation [34, 42]; and 2) End-to-end vision-language models (VLMs) that directly output structured representations (e.g., Markdown)  $[3, 7, 8,$ 29, 45, 46, 48]. Although both approaches have demonstrated promising results, conducting a broad comparison of their effectiveness remains challenging due to the absence of a comprehensive and unified evaluation benchmark.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1314,
                        2372,
                        2308,
                        2372,
                        2308,
                        2860,
                        1314,
                        2860
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1316.0000000000002,
                2878,
                2312,
                2878,
                2312,
                2970,
                1316.0000000000002,
                2970
            ],
            "ignore": false,
            "order": 12,
            "anno_id": 1,
            "text": " As shown in Table 1, for pipeline-based document parsing systems, dedicated benchmarks [10, 26, 54] have been",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1316.0000000000002,
                        2878,
                        2312,
                        2878,
                        2312,
                        2970,
                        1316.0000000000002,
                        2970
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "footer",
            "poly": [
                1222,
                3114,
                1326,
                3114,
                1326,
                3160,
                1222,
                3160
            ],
            "ignore": false,
            "order": 14,
            "anno_id": 1,
            "text": "24838",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1222,
                        3114,
                        1326,
                        3114,
                        1326,
                        3160,
                        1222,
                        3160
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                238,
                2328,
                560,
                2328,
                560,
                2382,
                238,
                2382
            ],
            "ignore": false,
            "order": 6,
            "anno_id": 1,
            "text": "# 1. Introduction",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        238,
                        2328,
                        560,
                        2328,
                        560,
                        2382,
                        238,
                        2382
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                640,
                989.9999999999999,
                834,
                989.9999999999999,
                834,
                1030,
                640,
                1030
            ],
            "ignore": false,
            "order": 4,
            "anno_id": 1,
            "text": "Abstract",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        640,
                        989.9999999999999,
                        834,
                        989.9999999999999,
                        834,
                        1030,
                        640,
                        1030
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "figure_caption",
            "poly": [
                1312,
                1874,
                2302,
                1874,
                2302,
                1964,
                1312,
                1964
            ],
            "ignore": false,
            "order": 9,
            "anno_id": 1,
            "text": "Figure 1. Results of End-to-End Text Recognition on OmniDocBench across 9 PDF page types.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1312,
                        1874,
                        2302,
                        1874,
                        2302,
                        1964,
                        1312,
                        1964
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "figure",
            "poly": [
                1388,
                994,
                2238,
                994,
                2238,
                1832,
                1388,
                1832
            ],
            "ignore": false,
            "order": 8,
            "anno_id": 1,
            "text": "",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1388,
                        994,
                        2238,
                        994,
                        2238,
                        1832,
                        1388,
                        1832
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "footer",
            "poly": [
                292.00000000000006,
                2854,
                980,
                2854,
                980,
                2970,
                292.00000000000006,
                2970
            ],
            "ignore": false,
            "order": 13,
            "anno_id": 1,
            "text": "<sup>\\*</sup> The authors contributed equally.  <sup>†</sup> Project lead.  <sup>‡</sup> Corresponding author (heconghui@pjlab.org.cn).",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        292.00000000000006,
                        2854,
                        980,
                        2854,
                        980,
                        2970,
                        292.00000000000006,
                        2970
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        }
    ],
    "extra": {
        "relation": []
    },
    "page_info": {
        "page_attribute": {},
        "page_no": 982,
        "height": 3300,
        "width": 2550,
        "image_path": "95_1_png.jpg"
    }
}