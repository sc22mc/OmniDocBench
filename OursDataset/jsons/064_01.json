{
    "layout_dets": [
        {
            "category_type": "title",
            "poly": [
                477.5,
                365.00000000000006,
                2022.5,
                365.00000000000006,
                2022.5,
                500.00000000000006,
                477.5,
                500.00000000000006
            ],
            "ignore": false,
            "order": 2,
            "anno_id": 1,
            "text": " # QR-CLIP: Introducing Explicit Open-World Knowledge for Location and Time Reasoning",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        477.5,
                        365.00000000000006,
                        2022.5,
                        365.00000000000006,
                        2022.5,
                        500.00000000000006,
                        477.5,
                        500.00000000000006
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                507.5,
                657.5,
                1970,
                657.5,
                1970,
                712.5,
                507.5,
                712.5
            ],
            "ignore": false,
            "order": 3,
            "anno_id": 1,
            "text": " Weimin Shi<sup>1</sup> Mingchen Zhuge<sup>†2</sup> Zhong Zhou<sup>1</sup> Dehong Gao<sup>3</sup> Deng-Ping Fan<sup>4</sup>",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        507.5,
                        657.5,
                        1970,
                        657.5,
                        1970,
                        712.5,
                        507.5,
                        712.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                625,
                810,
                822.5,
                810,
                822.5,
                855,
                625,
                855
            ],
            "ignore": false,
            "order": 4,
            "anno_id": 1,
            "text": " Abstract",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        625,
                        810,
                        822.5,
                        810,
                        822.5,
                        855,
                        625,
                        855
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                307.49999999999994,
                877.5,
                1125,
                877.5,
                1125,
                1910,
                307.49999999999994,
                1910
            ],
            "ignore": false,
            "order": 5,
            "anno_id": 1,
            "text": " Daily images may convey abstract meanings that require us to memorize and infer profound information from them. To encourage such humanlike reasoning, in this work, we teach machines to predict where and when it was taken rather than performing basic tasks like traditional segmentation or classification. Inspired by Horn's OR theory (Horn, 1984), we designed a novel  $\\mathbf{QR}$ -**CLIP** model consisting of two components: 1) the **Quantity** module first retrospects more openworld knowledge as the candidate language inputs; 2) the **Relevance** module carefully estimates vision and language cues and infers the location and time. Experiments show our QR-CLIP's effectiveness, and it outperforms the previous SOTA on each task by an average of about 10% and 130% relative lift in terms of location and time reasoning. This study lays a technical foundation for location and time reasoning and suggests that effectively introducing open-world knowledge is one of the panaceas for the tasks.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        307.49999999999994,
                        877.5,
                        1125,
                        877.5,
                        1125,
                        1910,
                        307.49999999999994,
                        1910
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "arxiv",
            "poly": [
                70,
                902.5,
                152.5,
                902.5,
                152.5,
                2310,
                70,
                2310
            ],
            "ignore": false,
            "order": 1,
            "anno_id": 1,
            "text": "arXiv:2302.00952v1 [cs.CV] 2 Feb 2023",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        70,
                        902.5,
                        152.5,
                        902.5,
                        152.5,
                        2310,
                        70,
                        2310
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "section",
            "poly": [
                232.5,
                2012.5000000000002,
                550,
                2012.5000000000002,
                550,
                2052.5,
                232.5,
                2052.5
            ],
            "ignore": false,
            "order": 6,
            "anno_id": 1,
            "text": " # 1. Introduction",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        232.5,
                        2012.5000000000002,
                        550,
                        2012.5000000000002,
                        550,
                        2052.5,
                        232.5,
                        2052.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                227.5,
                2097.5,
                1212.5,
                2097.5,
                1212.5,
                2482.5,
                227.5,
                2482.5
            ],
            "ignore": false,
            "order": 7,
            "anno_id": 1,
            "text": " Many deep computer vision models have outstanding perception abilities and can solve regular tasks by extracting simple visual contexts ( $i.e.,$  color, texture, and objects), following the principle: \"what you see is what you get\". However, they cannot engage with a scene in the same insightful ways humans can (Crowder & Friess, 2012). It seems difficult for them to think deeper (or *learning to think*) based on their observations (Schmidhuber, 2015).",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        227.5,
                        2097.5,
                        1212.5,
                        2097.5,
                        1212.5,
                        2482.5,
                        227.5,
                        2482.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                225,
                2522.5,
                1215,
                2522.5,
                1215,
                2667.5,
                225,
                2667.5
            ],
            "ignore": false,
            "order": 8,
            "anno_id": 1,
            "text": " There are two reasons to explain the abovementioned problem: 1) until recently, many fundamental computer vision problems remained unsolved, so the community focused",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        225,
                        2522.5,
                        1215,
                        2522.5,
                        1215,
                        2667.5,
                        225,
                        2667.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1285,
                2230,
                2262.5,
                2230,
                2262.5,
                2927.5,
                1285,
                2927.5
            ],
            "ignore": false,
            "order": 13,
            "anno_id": 1,
            "text": " Compared with traditional image models (He et al., 2016; Liu et al., 2021), CLIP initially contains a certain amount of OWK. However, this knowledge is more encapsulated within the model. It can only be used implicitly through incontext learning (Min et al., 2022), preventing OWK from playing a larger role in our tasks. To solve this problem, we design a model named **QR-CLIP** with the capability to infer the location-and-time-related meta-information about the image. It is inspired by Horn's QR theory (Horn, 1984): Q-principle (quantity) requires maximization of the information content, which means \"ask speaker to present as *much information as possible*\". In contrast, the R-principle (relevance) requires minimization of form, which means \"should focus more on the relevant content\".",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1285,
                        2230,
                        2262.5,
                        2230,
                        2262.5,
                        2927.5,
                        1285,
                        2927.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1285,
                1510.0000000000002,
                2265,
                1510.0000000000002,
                2265,
                2192.5,
                1285,
                2192.5
            ],
            "ignore": false,
            "order": 12,
            "anno_id": 1,
            "text": " These days, industries afford millions of dollars to train foundation models (Reed et al.), and advanced parallel techniques (Rasley et al., 2020; Ott et al., 2019) enable the model to scale up the amounts of parameters and data. Some companies like OpenAI and DeepMind have developed a series of models, including GPT-3 (Brown et al., 2020), CLIP (Radford et al., 2021), and ChatGPT (Ouyang et al., 2022), etc. Most of those foundation models learn on their own from large amounts of online data made by people in a self-supervised fashion. This gives the models a certain amount of \"open-world knowledge\" (OWK). This motivates us to use CLIP (Radford et al., 2021) as our basic architecture to solve the proposed task since it shows effective performance in a number of multimodal tasks.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1285,
                        1510.0000000000002,
                        2265,
                        1510.0000000000002,
                        2265,
                        2192.5,
                        1285,
                        2192.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1277.5,
                1037.5,
                2265,
                1037.5,
                2265,
                1477.5,
                1277.5,
                1477.5
            ],
            "ignore": false,
            "order": 11,
            "anno_id": 1,
            "text": " This paper aims to delve into the location and time reasoning behind the images (Fu et al., 2022). The procedure can be summarized as: input an image; the goal is to have the model guess where and when the image was taken. It is pretty different from the others ( $e.g.$ , basic classification (Wang et al., 2022), summarization (Fabbri et al., 2019), or retrieval tasks (Conforti et al., 2020)) since it requires the model explore more in-depth information and truly comprehend the event behind the images.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1277.5,
                        1037.5,
                        2265,
                        1037.5,
                        2265,
                        1477.5,
                        1277.5,
                        1477.5
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                1270,
                815,
                2255,
                815,
                2255,
                1005,
                1270,
                1005
            ],
            "ignore": false,
            "order": 10,
            "anno_id": 1,
            "text": " more on basic vision learning (Srivastava et al., 2015; He et al., 2016); and 2) previous models were unable to absorb more human knowledge due to limited hardware resources and data (Bommasani et al., 2021).",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        1270,
                        815,
                        2255,
                        815,
                        2255,
                        1005,
                        1270,
                        1005
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        },
        {
            "category_type": "plain_text",
            "poly": [
                230,
                2697.5,
                1212.5,
                2697.5,
                1212.5,
                2905,
                230,
                2905
            ],
            "ignore": false,
            "order": 9,
            "anno_id": 1,
            "text": " Equal Contributions: †Mingchen Zhuge designed and advised the project since March 2022. 1State Key Laboratory of Virtual Reality Technology and System, BUAA 2AI Initiative, KAUST 3Northwestern Polytechnical University 4CV Lab, ETH Zürich. Correspondence to: Zhong Zhou zzhou@buaa.edu.cn.",
            "line_with_spans": [
                {
                    "category_type": "text_span",
                    "poly": [
                        230,
                        2697.5,
                        1212.5,
                        2697.5,
                        1212.5,
                        2905,
                        230,
                        2905
                    ],
                    "text": ""
                }
            ],
            "attribute": {
                "text_language": "",
                "text_background": "",
                "text_rotate": ""
            }
        }
    ],
    "extra": {
        "relation": []
    },
    "page_info": {
        "page_attribute": {},
        "page_no": 656,
        "height": 3300,
        "width": 2550,
        "image_path": "64_1_png.jpg"
    }
}